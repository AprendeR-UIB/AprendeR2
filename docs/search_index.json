[
["index.html", "AprendeR: Parte II Presentación", " AprendeR: Parte II The AprendeR team 2019-04-20 Presentación Esto es una edición preliminar en línea de la 2a parte del libro “AprendeR”. El libro está escrito en R Markdown, usando RStudio como editor de texto y el paquete bookdown para convertir los ficheros markdown en un libro. Este trabajo se publica bajo licencia Atribución-No Comercial-SinDerivadas 4.0 "],
["chap-distr.html", "Lección 1 Distribuciones de probabilidad 1.1 Ejercicios", " Lección 1 Distribuciones de probabilidad R conoce los tipos de distribución de probabilidad más importantes, incluyendo las que mostramos en la tabla siguiente: \\[ \\begin{array}{lll} \\hline \\textbf{Distribución} &amp; {\\textbf{Nombre en R}} &amp; {\\textbf{Parámetros}}\\\\ \\hline \\mbox{Binomial} &amp;{\\texttt{binom}} &amp; \\mbox{tamaño de la muestra $n$, probabilidad $p$}\\\\ \\mbox{Geométrica} &amp; {\\texttt{geom}} &amp; \\mbox{$p$}\\\\ \\mbox{Hipergeométrica} &amp; {\\texttt{hyper}} &amp; \\mbox{tamaño de la población $N$, número poblacional}\\\\[-0.75ex] &amp; &amp; \\mbox{de éxitos $M$, tamaño de la muestra $n$}\\\\ \\mbox{Poisson} &amp; {\\texttt{pois}} &amp; \\mbox{esperanza $\\lambda$}\\\\ \\mbox{Uniforme} &amp; {\\texttt{unif}} &amp; \\mbox{mínimo, máximo}\\\\ \\mbox{Exponencial} &amp; {\\texttt{exp}} &amp; \\lambda\\\\ \\mbox{Normal} &amp; {\\texttt{norm}} &amp; \\mbox{media $\\mu$, desviación típica $\\sigma$}\\\\ \\mbox{Khi cuadrado} &amp; {\\texttt{chisq}} &amp; \\mbox{número de grados de libertad $df$}\\\\ \\mbox{t de Student} &amp; {\\texttt{t}} &amp; \\mbox{número de grados de libertad $df$}\\\\ \\mbox{F de Fisher} &amp; {\\texttt{f}} &amp; \\mbox{los dos números de grados de libertad} \\\\ \\hline \\end{array} \\] Para cada una de estas distribuciones, R sabe calcular cuatro funciones, que se obtienen añadiendo un prefijo al nombre de la distribución: La función de densidad, con el prefijo d. La función de distribución de probabilidad, con el prefijo p; esta función dispone además del parámetro lower.tail que igualado a FALSE calcula la función de distribución de cola superior: la probabilidad de que una variable aleatoria con esta distribución de probabilidad tome un valor estrictamente mayor que uno dado. Los cuantiles, con el prefijo q. Vectores de números aleatorios con esta distribución, con el prefijo r. La función correspondiente se aplica entonces al valor sobre el que queremos calcular la función y a los parámetros de la distribución (en este orden, y los parámetros en el orden en que los damos en la tabla anterior, cuando hay más de uno). Por ejemplo, sea \\(X\\) una variable aleatoria binomial \\(B(20,0.3)\\), es decir, de tamaño \\(n=20\\) y probabilidad \\(p=0.3\\), y sean \\(f_X\\) su función de densidad y \\(F_X\\) su función de distribución. Calculemos algunos valores de funciones asociadas a esta variable aleatoria. \\(f_X(5)=P(X=5)\\): dbinom(5,20,0.3) ## [1] 0.1788631 Comprobémoslo, recordando que si \\(X\\sim B(n,k)\\), entonces \\(P(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}\\): choose(20,5)*0.3^5*0.7^15 ## [1] 0.1788631 \\(f_X(8)=P(X=8)\\): dbinom(8,20,0.3) ## [1] 0.1143967 \\(F_X(5)=P(X{\\leqslant}5)\\): pbinom(5,20,0.3) ## [1] 0.4163708 Comprobémoslo, usando que \\(P(X{\\leqslant}5)=\\sum_{k=0}^5 P(X=k)\\): sum(dbinom(0:5,20,0.3)) ## [1] 0.4163708 \\(F_X(8)=P(X{\\leqslant}8)\\): pbinom(8,20,0.3) ## [1] 0.8866685 \\(P(X&gt;8)\\) pbinom(8,20,0.3,lower.tail=FALSE) ## [1] 0.1133315 En efecto: 1-pbinom(8,20,0.3) ## [1] 0.1133315 El cuantil de orden \\(0.5\\) de \\(X\\), o sea, su mediana: el valor \\(x\\) más pequeño tal que \\(P(X{\\leqslant}x){\\geqslant}0.5\\) qbinom(0.5,20,0.3) ## [1] 6 Comprobemos que \\(P(X{\\leqslant}6){\\geqslant}0.5\\) y en cambio \\(P(X{\\leqslant}5)&lt; 0.5\\): pbinom(6,20,0.3) ## [1] 0.6080098 pbinom(5,20,0.3) ## [1] 0.4163708 El cuantil de orden \\(0.25\\) de \\(X\\), es decir, su primer cuartil: qbinom(0.25,20,0.3) ## [1] 5 Un vector aleatorio de 10 valores generado con la variable aleatoria \\(X\\): rbinom(10,20,0.3) ## [1] 5 7 6 6 6 9 4 6 4 3 Dos vectores aleatorios más, de 10 valores cada uno, generados con la variable aleatoria \\(X\\): rbinom(10,20,0.3) ## [1] 2 8 7 7 7 6 2 8 8 1 rbinom(10,20,0.3) ## [1] 8 7 8 3 5 9 5 7 4 7 Del mismo modo, si estamos trabajando con una variable aleatoria \\(Y\\) de Poisson con parámetro \\(\\lambda=5\\): \\(P(Y=8)\\): dpois(8,5) ## [1] 0.06527804 \\(P(Y{\\leqslant}8)\\): ppois(8,5) ## [1] 0.9319064 El cuantil de orden 0.6 de \\(Y\\): qpois(0.6,5) ## [1] 5 Un vector aleatorio de 20 valores generado con la variable aleatoria \\(Y\\): rpois(20,5) ## [1] 4 5 9 6 3 6 1 8 2 6 4 7 6 7 2 6 6 3 3 2 Si no entramos ningún parámetro en las funciones asociadas a la distribución normal, R entiende que se trata de la normal estándar (con media \\(\\mu=0\\) y desviación típica \\(\\sigma=1\\)): por ejemplo, las dos instrucciones siguientes nos dan el valor \\(f_Z(0.3)\\) de la función densidad de una normal estándar \\(Z\\) aplicada a 0.3 (que no es igual a \\(P(Z=0.3)\\)): dnorm(0.3) ## [1] 0.3813878 dnorm(0.3,0,1) ## [1] 0.3813878 Las funciones densidad y distribución de una variable aleatoria se pueden dibujar con la función curve. Así, la función siguiente dibuja la gráfica de la densidad de una variable normal estándard de la Figura 1.1: curve(dnorm(x,0,1.5),-5,5,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;&quot;) Figura 1.1: Función densidad de una variable N(0,1). De manera similar, la función siguiente dibuja la gráfica de la función de distribución de una variable normal estándard de la Figura 1.2: curve(pnorm(x,0,1.5),-5,5,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;&quot;) Figura 1.2: Función distribución de una variable N(0,1). 1.1 Ejercicios Test (1) Sea \\(f\\) la función de densidad de una variable aleatoria normal con \\(\\mu=0.2\\) y \\(\\sigma=1.2\\). Dad el valor de \\(f(0.5)\\) redondeado a 4 cifras decimales. (2) Sea \\(X\\) una variable aleatoria normal con \\(\\mu=0.2\\) y \\(\\sigma=1.2\\). Dad el valor de \\(P(3{\\leqslant}X{\\leqslant}7)\\) redondeado a 4 cifras decimales. (3) Sea \\(X\\) una variable aleatoria \\(B(10,0.2)\\). Dad el valor de \\(P(3{\\leqslant}X{\\leqslant}7)\\) redondeado a 4 cifras decimales. (4) Dad una instrucción que calcule la mediana de una lista de 20 números aleatorios generados con distribución \\(B(10,0.2)\\). No deis el resultado, solo la instrucción. Respuestas al test (1) 0.3222 Nosotros lo hemos calculado con round(dnorm(0.5,0.2,1.2),4). (2) 0.0098 Nosotros lo hemos calculado con round(pnorm(7,0.2,1.2)-pnorm(3,0.2,1.2),4). (3) 0.3221 Nosotros lo hemos calculado con round(pbinom(7,10,0.2)-pbinom(2,10,0.2),4). También se obtiene el resultado correcto con round(sum(dbinom(3:7,10,0.2)),4). En cambio, con round(pbinom(7,10,0.2)-pbinom(3,10,0.2),4) no se obtiene el resultado correcto: da 0.1208. Pensad por qué aquí hay que restar pbinom(2,10,0.2) y en la pregunta anterior restábamos pnorm(3,0.2,1.2). (4) median(rbinom(20,10,0.2)) También sería correcto median(rbinom(20,size=10,prob=0.2)), pero no es necesario dar los nombres de los parámetros si entráis sus valores en el orden correcto, y por eso no los hemos explicado. "],
["chap-muestreo.html", "Lección 2 Conceptos básicos de muestreo 2.1 Tipos de muestreo 2.2 Muestreo aleatorio con R 2.3 Guía rápida 2.4 Ejercicios", " Lección 2 Conceptos básicos de muestreo En todo estudio estadístico hemos de distinguir entre población, que es un conjunto de sujetos con una o varias características que podemos medir y deseamos estudiar, y muestra, un subconjunto de una población. Por ejemplo, si quisiéramos estudiar alguna característica de los estudiantes de grado de la UIB, entenderíamos que estos forman la población de interés, y si entonces escogiéramos al azar 10 estudiantes de cada grado, obtendríamos una muestra de esta población. Pero también podríamos considerar los estudiantes de grado de la UIB como una muestra de la población de los estudiantes universitarios españoles: depende del estudio que queramos realizar. Recordad que, cuando disponemos de un conjunto de datos obtenidos midiendo una o varias características sobre los sujetos de una muestra, podemos llevar a cabo dos tipos de análisis estadístico: Exploratorio o descriptivo: su objetivo es resumir, representar y explicar los datos de la muestra. Para llevarlo a cabo, se usan técnicas de estadística descriptiva como las que hemos descrito en lecciones anteriores. Inferencial o confirmatorio: su objetivo es deducir (inferir), a partir de los datos de la muestra, información significativa sobre el total de la población. A menudo esta inferencia pasa por contrastar una hipótesis sobre alguna propiedad de la población. Las técnicas que se usan en los análisis inferenciales forman la estadística inferencial. Por ejemplo, supongamos que hemos tomado una muestra de estudiantes de la UIB y sabemos sus calificaciones en un semestre concreto y sus números de hermanos. En un estudio exploratorio simplemente describiríamos estos datos mediante estadísticos y gráficos, mientras que usaríamos técnicas de estadística inferencial para deducir información sobre la población de todos los estudiantes de la UIB a partir de esta muestra: ¿Cuál estimamos que ha sido la nota media de los estudiantes de la UIB en el semestre en cuestión? La distribución de los números de hermanos en estudiantes de la UIB, ¿es similar a la del conjunto de la población española? ¿Es verdad que los estudiantes de la UIB con más hermanos tienen tendencia a tener mejores notas? Un estudio inferencial suele desglosarse en los pasos siguientes: Establecer la característica que se desea estimar o la hipótesis que se desea contrastar. Determinar la información (los datos) que se necesita para hacerlo. Diseñar un experimento que permita recoger estos datos; este paso incluye: Decidir qué tipo de muestra se va a tomar y su tamaño. Elegir las técnicas adecuadas para realizar las inferencias deseadas a partir de la muestra que se tomará. Tomar una muestra y medir los datos deseados sobre los individuos que la forman. Aplicar las técnicas de inferencia elegidas con el software adecuado. Obtener conclusiones. Si las conclusiones son fiables y suficientes, redactar un informe; en caso contrario, volver a empezar. En la próxima sección nos centraremos en las técnicas de muestreo: los métodos generales para seleccionar muestras representativas de una población que tenemos a nuestra disposición en el tercer paso de la lista anterior. 2.1 Tipos de muestreo Existen muchos tipos de muestreo, cada uno de los cuales proporciona una muestra representativa de la población en algún sentido. A continuación describimos de forma breve algunas de estas técnicas. Muestreo aleatorio con y sin reposición Un muestreo aleatorio consiste en seleccionar una muestra de la población de manera que todas las muestras del mismo tamaño sean equiprobables; es decir, que si fijamos el número de individuos de la muestra, cualquier conjunto de ese número de individuos tenga la misma probabilidad de ser seleccionado. Hay dos tipos básicos de muestreo aleatorio que vale la pena distinguir. Para ilustrarlos, supongamos que disponemos de una urna con 100 bolas numeradas del 1 al 100, de la que queremos extraer una muestra de 15 bolas. La Figura 2.1 representa dicha urna. Figura 2.1: Una urna de 100 bolas Una manera de hacerlo sería repetir 15 veces el proceso de sacar una bola de la urna, anotar su número y devolverla a la urna. El tipo de muestra obtenida de esta manera recibe el nombre de muestra aleatoria con reposición, o simple (una m.a.s., para abreviar). Observad que con este procedimiento una misma bola puede aparecer varias veces en una muestra, y que todos los subconjuntos de 15 bolas “con posibles repeticiones” tienen la misma probabilidad de obtenerse. Un posible resultado serían las bolas azules de la Figura 2.2; la bola azul más oscuro ha sido escogida dos veces en la muestra. Figura 2.2: Una muestra aleatoria simple Otra manera de extraer nuestra muestra sería repetir 15 veces el proceso de sacar una bola de la urna pero ahora sin devolverla. Esto es equivalente a extraer de golpe 15 bolas de la urna. Estas muestras no tienen bolas repetidas, y cualquier selección de 15 bolas diferentes tiene la misma probabilidad de ser la obtenida. En este caso se habla de una muestra aleatoria sin reposición. Un posible resultado serían las bolas azules de la Figura 2.3. Figura 2.3: Una muestra aleatoria sin reposición Cuando el tamaño de la población es muy grande en relación a la muestra, la probabilidad de que haya repeticiones en una muestra aleatoria simple es muy pequeña. Esto nos permite entender en este caso que los muestreos aleatorios con y sin reposición son equivalentes en el sentido siguiente: puesto que si la población es muy, muy grande, un muestreo con reposición daría muy probablemente una muestra con todos sus elementos diferentes, si tomamos directamente la muestra sin reposición podemos aceptar que permitíamos repeticiones, pero que no se han dado, y que por tanto es simple. A modo de ejemplo, vamos a calcular la probabilidad de al menos una repetición en muestras aleatorias simples de diferentes tamaños de una población de 12,000 individuos (aproximadamente, el número de estudiantes de la UIB) y representar estas probabilidades en un gráfico. Recordad que la probabilidad de que los sujetos de una muestra aleatoria simple de tamaño \\(n\\) tomada de una población de \\(N\\) individuos no sean todos diferentes es \\[ 1-\\frac{N(N-1)(N-2)\\cdots (N-n+1)}{N^n}. \\] Esta probabilidad es la que calcula la función f(N,n) del bloque de código siguiente, y su gráfica es la de la Figura 2.4. La curva negra representa las probabilidades deseadas. Hemos añadido al gráfico una línea horizontal que marca la probabilidad 0.01 y que muestra que la probabilidad de alguna repetición en una m.a.s. de 16 o menos estudiantes de la UIB es inferior al 1%: en más de 99 de cada 100 veces que tomemos una m.a.s. de a lo sumo 16 estudiantes, nos saldrán todos diferentes f=function(N,n){1-prod((N:(N-n+1))/N)} prob=sapply(1:200,f,N=12000) plot(1:200,prob,type=&quot;l&quot;,lwd=2,xlab=&quot;n&quot;,ylab=&quot;probabilidad&quot;, main=&quot;&quot;,xaxp=c(0,200,20),yaxp=c(0,1,10)) abline(h=0.01,col=&quot;red&quot;) text(160,0.04,labels=&quot;probabilidad 0.01&quot;,col=&quot;red&quot;,cex=0.7) Figura 2.4: Probabilidad de repetición en una m.a.s. de n estudiantes de la UIB Así, por ejemplo, una muestra aleatoria de 10 estudiantes diferentes de la UIB podría haberse obtenido perfectamente tomando los estudiantes con reposición, porque la probabilidad de alguna repetición en una m.a.s. como esta es muy pequeña: 0.004. En cambio, es difícil de creer que una muestra aleatoria de 200 estudiantes diferentes de la UIB sea simple, porque la probabilidad de alguna repetición en una m.a.s. como esta es grande: 0.811. La mayoría de técnicas de estadística inferencial que se pueden usar para muestras aleatorias simples se pueden considerar igualmente válidas para muestras aleatorias sin reposición si el tamaño de la población es muy grande en relación al de la muestra (por dar una regla, digamos que, al menos, unas 1000 veces mayor). Si el tamaño de la población es relativamente pequeño por comparación a la muestra, algunas de estas técnicas se pueden salvar aplicando correcciones adecuadas para compensar la pequeñez de la población, y otras directamente pierden toda validez. En todo caso, conviene ser consciente de que si queremos tomar una muestra aleatoria con o sin reposición de una población, es necesario disponer de una lista completa de todos sus individuos para poder sortear a quién vamos a seleccionar. Esto no siempre es posible. ¿Alguien tiene la lista completa de, pongamos, todos los diabéticos de España? ¿Que incluya los que no saben que lo son? Por lo tanto, en la vida real no siempre podemos tomar muestras aleatorias en el sentido que hemos explicado. Muestreo sistemático Una manera muy sencilla de obtener una muestra de una población cuando disponemos de una lista ordenada de sus individuos es tomarlos a intervalos constantes: cada quinto individuo, cada décimo individuo. Podemos añadir una componente aleatoria escogiendo al azar el primer individuo que elegimos, y a partir del cual empezamos a contar. Así, por ejemplo, si de una clase de 100 estudiantes quisiéramos escoger una muestra de 10, podríamos elegir un estudiante al azar, y a partir de él, por orden alfabético, elegir el décimo estudiante, el vigésimo, el trigésimo, etc.; si al llegar al final de la lista de clase no hubiéramos completado la muestra, volveríamos al principio de la misma. A esta técnica se la llama muestreo sistemático, aleatorio si además el primer sujeto se escoge de manera aleatoria. Por ejemplo, la Figura 2.5 describe una muestra aleatoria sistemática de 15 bolas de nuestra urna de 100 bolas: hemos empezado a escoger por la bola roja oscura, que ha sido elegida al azar, y a partir de ella hemos tomado 1 de cada 7 bolas, volviendo al principio cuando hemos llegado al final de la lista de bolas Figura 2.5: Una muestra aleatoria sistemática Cuando no disponemos de una lista de toda la población pero sí que tenemos una manera de acceder de manera ordenada a sujetos de la misma (por ejemplo, enfermos que acuden a un hospital), podemos realizar un muestreo sistemático tomando los sujetos a intervalos constantes a medida que los encontramos y hasta completar el tamaño deseado de la muestra. Por ejemplo, para escoger una muestra de 10 estudiantes de la UIB, podríamos escoger cada décimo estudiante que entrase en un edificio del Campus por una puerta concreta hasta llegar a los 10. Cuando el orden de los individuos de la población en la lista es aleatorio, el muestreo sistemático aleatorio es equivalente al muestreo aleatorio sin reposición. Pero en general este no es el caso, y se pueden producir sesgos. Por poner un caso extremo, si una clase de 100 estudiantes estuviera formada por 50 parejas de hermanos y tomáramos una muestra sistemática de 50 estudiantes, eligiéndolos por orden alfabético de los apellidos uno sí, uno no, es seguro que no aparecería ninguna pareja de hermanos en la muestra (porque dos hermanos son siempre consecutivos en la lista, y en nuestra muestra no habría ningún par de sujetos consecutivos). En cambio, la probabilidad de que una muestra aleatoria sin reposición del mismo tamaño contuviera una pareja de hermanos es prácticamente 1; en concreto esta probabilidad sería \\[ \\frac{100\\times 98\\times 96\\times\\cdots\\times 2}{100\\times 99\\times 98\\times\\cdots\\times 51}=\\frac{2^{50}\\cdot 50!^2}{100!}=0.999999999999989. \\] Muestreo aleatorio estratificado Este tipo de muestreo se utiliza cuando la población está clasificada en estratos que son de interés para la propiedad estudiada. En este caso, se toma una muestra aleatoria de cada estrato y se unen en una muestra global. A este proceso se le llama muestreo aleatorio estratificado. Normalmente, se impone que la composición por estratos de la muestra global mantenga las proporciones de la población original; es decir, que el tamaño de la muestra de cada estrato represente el mismo porcentaje del total de la muestra que el estrato correspondiente en la población completa. Por ejemplo, los estratos podrían ser grupos de edad, y entonces la muestra de cada grupo de edad se tomaría proporcional a la fracción que representa dicho grupo de edad en la población total. O podrían ser los sexos anatómicos, y procuraríamos que nuestra muestra estuviera formada por un 50% de hombres y un 50% de mujeres. O, en las Islas Baleares, los estratos podrían ser las islas, de manera que el número de representantes de cada isla en la muestra fuera proporcional a su población relativa dentro del conjunto total de la comunidad autónoma. Por continuar con nuestra urna de 100 bolas, supongamos que contiene 40 bolas de un color y 60 de otro color según muestra la Figura 2.6. Figura 2.6: Nuestra urna ahora tiene 2 estratos Para tomar una muestra aleatoria estratificada de 15 bolas, considerando como estratos los dos colores, tomaríamos una muestra aleatoria de 6 bolas del primer color y una muestra aleatoria de 9 bolas del segundo color. De esta manera, los porcentajes de colores en la muestra serían los mismos que en la urna. La Figura 2.7 describe una muestra obtenida de esta manera. Figura 2.7: Una muestra aleatoria estratificada En todo caso, el muestreo por estratos solo es necesario si esperamos que las características de la propiedad poblacional que queremos estudiar varíen según el estrato. Por ejemplo, si queremos tomar una muestra para estimar la altura media de los españoles adultos y no creemos que la altura de un español adulto dependa de su provincia de origen, no hay ninguna necesidad de esforzarse en tomar una muestra de cada provincia de manera que todas las provincias estén representadas proporcionalmente en la muestra. Muestreo por conglomerados El proceso de obtener y estudiar una muestra aleatoria en algunos casos es caro o difícil, incluso aunque dispongamos de la lista completa de la población. Imaginemos que quisiéramos estudiar los hábitos de alimentación de los estudiantes de Primaria de Baleares. Para ello, previo permiso de la autoridad competente, tendríamos que seleccionar una muestra representativa de los escolares de Baleares. Seguramente podríamos disponer de su lista completa y por lo tanto podríamos tomar una muestra aleatoria, pero entonces acceder a las niñas y niños que la formasen seguramente significaría contactar con unos pocos alumnos de muchos centros de primaria, lo que volvería el proceso lento y costoso. Y eso si la Conselleria d’Educació nos facilitase la lista completa de alumnos. Una alternativa posible sería, en vez de extraer una muestra aleatoria de todos los estudiantes de Primaria, escoger primero al azar unas pocas aulas de primaria de colegios de las Baleares, a las que llamamos en este contexto conglomerados (clusters), y formar entonces nuestra muestra con todos los alumnos de estas aulas. Y es que es mucho más sencillo poseer la lista completa de estudiantes de unas pocas aulas que conseguir la lista completa de todos los estudiantes de todos los colegios, y mucho más barato ir a unos pocos colegios concretos que ir a todos los colegios de las Islas a entrevistar a unos pocos estudiantes en cada centro. Por poner otro ejemplo, efectuamos también un muestreo por conglomerados cuando para medir algunas características de los ejemplares de una planta en un bosque concreto, cuadriculamos la superficie del bosque, escogemos una muestra aleatoria de sectores de la cuadrícula (serían los conglomerados de este ejemplo) y estudiamos las plantas de interés contenidas en los sectores elegidas. Volviendo de nuevo a nuestra urna, supongamos que sus 100 bolas se agrupan en 20 conglomerados de 5 bolas cada uno según las franjas verticales de la Figura 2.8 (donde mantenemos la clasificación en dos colores para poder comparar el resultado del muestreo por conglomerados con el estratificado). Figura 2.8: Nuestra urna ahora tiene 2 estratos y 20 conglomerados Para obtener una muestra aleatoria por conglomerados de tamaño 15, escogeríamos al azar 3 conglomerados y la muestra estaría formada por sus bolas. La Figura 2.9 describe una muestra obtenida de esta manera: los conglomerados escogidos están marcados en azul. Figura 2.9: Una muestra aleatoria por conglomerados Observad la diferencia entre el muestreo estratificado y el muestreo por conglomerados: En una muestra estratificada se escoge una muestra aleatoria de cada estrato existente. En una muestra por conglomerados se escogen algunos conglomerados al azar y se incluye en la muestra todos sus elementos. Muestreos no aleatorios Cuando la selección de la muestra no es aleatoria, se habla de muestreo no aleatorio. En realidad es el tipo más frecuente de muestreo porque casi siempre nos tenemos que conformar con los sujetos disponibles. Por ejemplo, en la UIB, para estimar la opinión que de un profesor tienen los alumnos de una clase, se consulta solo a los estudiantes que voluntariamente rellenan la encuesta de opinión, que de ninguna manera forman una muestra aleatoria: el perfil del estudiante que contesta voluntariamente una encuesta de este tipo está muy definido y no viene determinado por el azar. En este caso se trataría de una muestra autoseleccionada. Otro tipo de muestras no aleatorias son las oportunistas. Este es el caso, por ejemplo, si para estimar la opinión que de un profesor tienen los alumnos de una asignatura se visita un día la clase y se pasa la encuesta a los estudiantes que ese día asistieron a clase. De nuevo, puede que los alumnos presentes no sean representativos del alumnado de la asignatura (pueden ser los más aplicados, o los que no tienen la gripe, o a los que la asignatura no les coincide con otra). Veamos otros ejemplos de muestreo oportunista. Supongamos que queremos estudiar una característica de los animales de una determinada especie en un hábitat, y la medimos en los animales que capturamos. Estos ejemplares no tienen por qué ser representativos de la población: a lo mejor son los menos espabilados. O imaginad que tenéis una bolsa con bolas de diferentes tamaños. Si las removéis bien, las pequeñas tenderán a ir a parar al fondo y las grandes a quedar en la parte superior. Por lo tanto, si tomáis una muestra de la capa superior (que será lo más cómodo), no será representativa del total de la bolsa. La Figura 2.10 describe una muestra oportunista de nuestra urna: sus 15 primeras bolas. Aunque toda muestra de un mismo tamaño tiene la misma probabilidad de obtenerse por medio de un muestreo aleatorio sin reposición, es difícil de creer que esta muestra sea aleatoria; basta que calculéis cuál es la probabilidad de que en una muestra aleatoria de 15 bolas de nuestra urna todas tengan el mismo color: \\[ \\frac{40\\times 39\\times \\cdots\\times 26+60\\times 59\\times \\cdots\\times 46}{100\\times 99\\times \\cdots\\times 86}=0.00021 \\] Figura 2.10: Una muestra oportunista Las técnicas de estadística inferencial no se pueden aplicar a muestras no aleatorias, pero normalmente son las únicas que podemos conseguir. En este caso, lo que se suele hacer es describir en detalle las características de la muestra para justificar que, pese a no ser aleatoria, es representativa de la población y podría haber sido aleatoria. Por ejemplo, la muestra oportunista anterior de nuestra urna no es de ninguna manera representativa de su contenido por lo que refiere al color de las bolas. Muestreo polietápico En el ejemplo de los estudiantes de Primaria, la muestra final de estudiantes ha estado formada por todos los individuos de las aulas elegidas. Otra opción podría haber sido, tras seleccionar la muestra aleatoria de conglomerados, tomar de alguna manera una muestra aleatoria de cada uno de ellos. Por ejemplo, algunos estudios poblacionales a nivel estatal se realizan solamente en algunas provincias escogidas aleatoriamente, en las que luego se encuesta una muestra aleatoria de habitantes. Este sería un ejemplo de muestreo polietápico, en el que la muestra no se obtiene en un solo paso, sino mediante diversas elecciones sucesivas. La Figura 2.11 muestra un ejemplo sencillo de muestreo polietápico de nuestra urna: hemos elegido al azar 5 conglomerados (marcados en azul) y de cada uno de ellos hemos elegido 3 bolas al azar sin reposición. Figura 2.11: Una muestra polietápica Otro ejemplo enrevesado (pero real) de muestreo polietápico sería, para elegir una muestra de adolescentes de una ciudad grande, escoger en primer lugar 4 secciones censales al azar; a continuación, escoger al azar 10 manzanas de cada una de estas secciones censales y una esquina de cada manzana; finalmente, recorrer cada manzana en sentido horario a partir de la esquina seleccionada y visitar un portal de cada tres, entrevistando todos los habitantes de 13 a 19 años en las casas o fincas visitadas. En este proceso, hemos realizado tres muestreos aleatorios sin reposición (de secciones censales, de manzanas y de esquinas) y un muestreo sistemático (los portales). Si además los adolescentes que estudiamos al final no son todos los que viven en los portales seleccionados sino solo los que encontramos en casa el día que los visitamos, este muestreo oportunista significaría un cuarto paso en la formación de la muestra. Existen otros tipos de muestreo, solo hemos explicado los más comunes. En cualquier caso, lo importante es recordar que el estudio estadístico que se realice a posteriori deberá ser diferente según el tipo de muestreo usado. Por ejemplo, no se pueden usar las mismas técnicas para analizar una muestra aleatoria simple que una muestra por conglomerados. 2.2 Muestreo aleatorio con R En este curso estudiaremos las propiedades de las diferentes técnicas de estimación solamente para el caso de muestreo aleatorio simple, es decir, al azar y con reposición, o al azar sin reposición si la población es muy, muy grande en comparación con la muestra. Recordemos que un método de selección al azar de muestras de tamaño \\(n\\) (es decir, formadas por \\(n\\) individuos) de una cierta población produce muestras aleatorias simples (m.a.s.) cuando todas las muestras posibles de \\(n\\) individuos (con posibles repeticiones) tienen la misma probabilidad de ser elegidas. El tener una m.a.s. de una población junto con un tamaño muestral adecuado \\(n\\) nos asegurará que la estimación que hagamos sea muy probablemente correcta. La manera más sencilla de llevar a cabo un muestreo aleatorio simple es numerar todos los individuos de una población y sortearlos eligiendo números de uno en uno como si se tratase de una lotería, por ejemplo con algún generador de números aleatorios. Esto se puede llevar a cabo fácilmente con R. R dispone de un generador de muestras aleatorias de un vector. La función básica es sample(x, n, replace=...) donde: x es un vector o un número natural \\(x\\), en cuyo caso R entiende que representa el vector 1,2,…,\\(x\\); n es el tamaño de la muestra que deseamos extraer; el parámetro replace puede igualarse a TRUE, y será una muestra aleatoria con reposición, es decir, simple, o a FALSE, y será una muestra aleatoria sin reposición. Este último es su valor por defecto, por lo que no es necesario especificarlo si se quiere obtener una muestra sin reposición. Los dos primeros parámetros han de entrarse en este orden o igualados a los parámetros x y size, respectivamente. Así, por ejemplo, para obtener una m.a.s. de 15 números entre 1 y 100, podemos entrar: sample(100,15,replace=TRUE) ## [1] 47 42 86 16 53 41 53 41 44 20 56 66 72 55 17 Naturalmente, y como ya nos encontramos en la Lección 1 cuando generábamos vectores aleatorios con una distribución dada, cada ejecución de sample con los mismos parámetros puede dar lugar a muestras diferentes, y todas ellas tienen la misma probabilidad de aparecer: sample(100,15,replace=TRUE) ## [1] 83 8 16 1 33 24 25 96 13 29 73 74 14 99 24 sample(100,15,replace=TRUE) ## [1] 74 63 2 54 24 54 89 35 64 51 67 68 11 97 63 sample(100,15,replace=TRUE) ## [1] 58 43 68 6 20 6 55 77 44 43 88 6 96 29 53 Veamos cómo extraer una m.a.s de una tabla de datos. Recordemos el data frame iris, que recoge medidas de pétalos y sépalos de 150 flores de tres especies de iris. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Si queremos extraer una m.a.s. de 15 ejemplares (filas) de esta tabla de datos, podemos generar con sample una m.a.s. de índices de filas de la tabla (recordad que dim aplicado a un dataframe nos da un vector con sus dimensiones, es decir, sus números de filas y de columnas, en este orden; por lo tanto, dim(iris)[1] es el número de filas de iris): x=sample(dim(iris)[1],15,replace=TRUE) y a continuación crear un data frame que contenga solo estas filas: muestra_iris=iris[x,] muestra_iris ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 10 4.9 3.1 1.5 0.1 setosa ## 127 6.2 2.8 4.8 1.8 virginica ## 62 5.9 3.0 4.2 1.5 versicolor ## 73 6.3 2.5 4.9 1.5 versicolor ## 99 5.1 2.5 3.0 1.1 versicolor ## 140 6.9 3.1 5.4 2.1 virginica ## 12 4.8 3.4 1.6 0.2 setosa ## 26 5.0 3.0 1.6 0.2 setosa ## 139 6.0 3.0 4.8 1.8 virginica ## 30 4.7 3.2 1.6 0.2 setosa ## 119 7.7 2.6 6.9 2.3 virginica ## 50 5.0 3.3 1.4 0.2 setosa ## 97 5.7 2.9 4.2 1.3 versicolor ## 93 5.8 2.6 4.0 1.2 versicolor ## 111 6.5 3.2 5.1 2.0 virginica Si solo quisiéramos una muestra aleatoria de longitudes de pétalos, podríamos aplicar directamente la función sample al vector correspondiente: muestra_long_pet=sample(iris$Petal.Length,15,replace=TRUE) muestra_long_pet ## [1] 1.6 5.5 6.1 4.4 4.5 6.7 4.4 5.7 5.9 4.0 4.4 4.0 5.5 3.5 6.1 El hecho de que funciones como sample o los generadores de vectores aleatorios con una cierta distribución de probabilidad fijada, como rnorm o rbinom, produzcan… pues eso, vectores aleatorios, puede tener inconvenientes a la hora de reproducir una simulación. R permite “fijar” el resultado de una función aleatoria con la instrucción set.seed. Sin entrar en detalles sobre cómo funcionan, los diferentes algoritmos que usa R para generar números aleatorios usan una semilla de aleatoriedad, que se modifica después de la ejecución del algoritmo, y por eso cada vez dan un resultado distinto. Pero, para una semilla fija, el algoritmo da el mismo resultado siempre. Lo que hace la función set.seed es igualar esta semilla al valor que le entramos. Si tras aplicar esta función a un número concreto ejecutamos una instrucción que genere un vector aleatorio de una longitud fija con una distribución fija, el resultado será siempre el mismo. Veamos un ejemplo de su efecto, generando muestras aleatorias simples de 10 longitudes de pétalos de flores iris con diferentes semillas de aleatoriedad: sample(iris$Petal.Length,10,replace=TRUE) ## [1] 5.1 5.7 4.1 1.5 1.4 4.3 1.4 4.1 1.7 4.0 set.seed(20) sample(iris$Petal.Length,10,replace=TRUE) ## [1] 6.4 5.3 1.3 3.5 5.7 5.2 1.1 1.5 1.4 4.5 set.seed(20) sample(iris$Petal.Length,10,replace=TRUE) ## [1] 6.4 5.3 1.3 3.5 5.7 5.2 1.1 1.5 1.4 4.5 sample(iris$Petal.Length,10,replace=TRUE) ## [1] 6.3 5.0 1.4 5.3 1.4 4.1 1.5 1.3 1.6 6.7 set.seed(10) sample(iris$Petal.Length,10,replace=TRUE) ## [1] 4.8 1.6 3.6 5.6 1.4 1.4 1.3 1.3 4.0 3.6 set.seed(10) sample(iris$Petal.Length,10,replace=TRUE) ## [1] 4.8 1.6 3.6 5.6 1.4 1.4 1.3 1.3 4.0 3.6 Ejecutado inmediatamente después de set.seed(20), sample(iris$Petal.Length,10,replace=TRUE) siempre da lo mismo. Y ejecutado después de set.seed(10), sample(iris$Petal.Length,10,replace=TRUE) vuelve a dar siempre lo mismo, pero diferente de con set.seed(20). La función set.seed no solo fija el resultado de la primera instrucción tras ella que genere un vector aleatorio, sino que, como fija la semilla de aleatoriedad y las funciones posteriores la modificarán de manera determinista, también fija los resultados de todas las instrucciones siguientes que generen vectores aleatorios. set.seed(100) sample(10,3) ## [1] 4 3 5 sample(10,3) ## [1] 1 5 4 sample(10,3) ## [1] 9 4 5 set.seed(100) sample(10,3) ## [1] 4 3 5 sample(10,3) ## [1] 1 5 4 sample(10,3) ## [1] 9 4 5 Si queréis volver a “reiniciar” la semilla de la aleatoriedad tras haber usado un set.seed, podéis usar set.seed(NULL). set.seed(100) sample(10,3) ## [1] 4 3 5 set.seed(NULL) sample(10,3) ## [1] 8 3 10 set.seed(100) sample(10,3) ## [1] 4 3 5 set.seed(NULL) sample(10,3) ## [1] 4 2 3 A veces querremos tomar diversas muestras aleatorias de una misma población y calcular algo sobre ellas. Para hacerlo podemos usar la función replicate. La sintaxis básica es replicate(n, instrucción) donde n es el número de repeticiones de la instrucción. Por ejemplo, para tomar 10 muestras aleatorias simples de 15 longitudes de pétalos de flores iris, podemos hacer: muestras=replicate(10, sample(iris$Petal.Length,15,replace=TRUE)) muestras ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1.5 1.5 3.9 4.8 5.9 4.4 6.4 1.4 4.5 4.5 ## [2,] 1.5 4.4 5.1 1.5 4.1 1.3 6.6 1.4 1.7 6.0 ## [3,] 5.1 1.4 1.7 4.9 5.0 5.1 4.5 6.3 1.6 4.7 ## [4,] 1.0 4.9 5.8 1.6 4.7 4.0 5.5 1.4 1.2 1.2 ## [5,] 1.6 5.1 1.2 4.9 4.7 6.0 6.9 3.9 5.1 1.5 ## [6,] 1.3 4.7 5.8 1.6 5.1 1.3 4.5 4.9 5.0 5.3 ## [7,] 4.7 4.7 4.5 1.9 4.4 1.9 1.5 1.9 6.1 1.5 ## [8,] 4.4 5.0 5.6 1.6 1.5 6.4 1.5 5.6 5.3 5.8 ## [9,] 4.1 5.5 4.2 1.4 1.6 5.6 6.1 4.5 5.7 1.4 ## [10,] 5.1 3.9 4.4 5.1 5.1 5.1 4.8 4.8 1.3 4.3 ## [11,] 5.8 5.4 3.3 5.1 3.9 3.5 1.4 6.1 4.8 1.5 ## [12,] 1.5 6.3 6.1 1.6 3.3 5.1 5.5 6.6 4.5 1.4 ## [13,] 6.0 6.9 4.6 4.9 4.0 1.6 1.5 6.1 1.5 4.9 ## [14,] 1.3 5.7 6.7 6.1 1.4 4.0 1.5 5.9 5.0 4.9 ## [15,] 5.7 5.1 4.4 4.9 1.5 4.7 1.3 5.1 4.2 1.7 Observad que R ha organizado los 10 vectores generados con el replicate como columnas de una matriz. Si solo nos hubiera interesado calcular las medias, redondeadas a 2 cifras decimales, de 10 muestras aleatorias simples de 15 longitudes de pétalos de flores iris, podríamos haber hecho medias=replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2)) medias ## [1] 4.24 3.55 3.36 3.61 4.04 3.89 3.67 4.17 3.48 3.49 En este caso, como el resultado de la instrucción que iteramos es un solo número, los resultados del replicate forman un vector. ¿Y si quisiéramos la media y la desviación típica muestral de 10 muestras de estas? No podemos usar sin más dos replicate, como en replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2)) ## [1] 3.35 4.64 4.21 3.91 3.59 2.57 4.26 3.88 4.17 3.87 replicate(10,round(sd(sample(iris$Petal.Length,15,replace=TRUE)),2)) ## [1] 1.67 2.00 1.94 1.73 1.90 1.67 1.65 2.12 1.60 1.86 porque es muy probable que el conjunto de muestras de las que hemos calculado la media en el primer replicate sea diferente del conjunto de muestras de las que hemos calculado la desviación típica en el segundo replicate. Lo más adecuado es definir una función que calcule un vector con estos dos valores, y luego usarla dentro de un único replicate. info=function(x){round(c(mean(x),sd(x)),2)} info_lp=replicate(10,info(sample(iris$Petal.Length,15,replace=TRUE))) info_lp ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 3.55 4.14 3.09 3.67 3.41 3.54 2.75 3.13 4.23 4.09 ## [2,] 1.96 1.55 1.60 1.63 1.64 1.91 1.65 1.59 1.58 1.69 En este último caso, R ha organizado la información obtenida como columnas de una matriz: la primera fila son las medias y la segunda las desviaciones típicas. Naturalmente, la función set.seed permite “fijar” el resultado de un replicate que incluya la generación de números aleatorios: set.seed(1000) replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2)) ## [1] 3.63 3.18 4.05 4.59 4.13 2.51 3.61 3.31 3.63 3.96 set.seed(1000) replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2)) ## [1] 3.63 3.18 4.05 4.59 4.13 2.51 3.61 3.31 3.63 3.96 Un último comentario sobre la función sample. Aunque aquí la vamos a usar principalmente para tomar muestras aleatorias en las que todos los sujetos de la población tengan la misma probabilidad de ser escogidos, también podemos emplearla para obtener muestras en las que diferentes sujetos puedan tener probabilidades diferentes de salir. Estas probabilidades se especifican con el parámetro prob igualado a un vector de probabilidades (o de pesos proporcionales a probabilidades) de la misma longitud que el vector x al cual apliquemos sample. De esta manera, la primera entrada de prob representa la probabilidad del primer elemento de x, la segunda entrada de prob representa la probabilidad del segundo elemento de x, etc. Por ejemplo, si queremos tomar una muestra aleatoria de tamaño 10 del vector \\((1,2,3)\\) de manera que cada elemento de este vector tenga probabilidad de ser escogido proporcional a su valor (es decir, el 2 tiene el doble de probabilidades de aparecer en la muestra que el 1, y el 3, el triple), podemos usar: sample(1:3,10,prob=1:3) ## Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when &#39;replace = FALSE&#39; ¡Ups! Para tomar una muestra de 10 elementos de una población de 3 sujetos, habrá que permitir repeticiones sample(1:3,10,replace=TRUE,prob=1:3) ## [1] 3 2 2 2 1 3 2 2 2 1 Para terminar esta lección, damos una función sencilla para efectuar muestreos sistemáticos aleatorios. El objetivo es, dado un vector de longitud \\(N\\), obtener una muestra de tamaño \\(n\\). Lo que haremos será tomar el cociente por exceso \\(k=\\lceil N/n\\rceil\\) de \\(N\\) entre \\(n\\) para determinar el período con el que tenemos que tomar los elementos de manera que todos los elementos puedan ser escogidos. A continuación elegimos al azar un elemento del vector con sample y a partir de él generamos una progresión aritmética de \\(n\\) elementos y paso \\(k\\), volviendo al inicio del vector si llegamos al final sin haber completado la muestra (lo que especificamos tomando los valores de la progresión aritmética módulo \\(N\\)). sist.sample=function(N,n){ k=ceiling(N/n) x0=sample(N,1) seq(x0,length.out=n,by=k)%%N } Por ejemplo, una muestra sistemática de 10 flores iris se podría obtener de la manera siguiente: x=sist.sample(dim(iris)[1],10) #Los índices de la muestra sistemática muestra_sist_iris=iris[x,] #La muestra de la tabla iris muestra_sist_iris ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 132 7.9 3.8 6.4 2.0 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 12 4.8 3.4 1.6 0.2 setosa ## 27 5.0 3.4 1.6 0.4 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 57 6.3 3.3 4.7 1.6 versicolor ## 72 6.1 2.8 4.0 1.3 versicolor ## 87 6.7 3.1 4.7 1.5 versicolor ## 102 5.8 2.7 5.1 1.9 virginica ## 117 6.5 3.0 5.5 1.8 virginica Como 150/10=15, podemos observar que los índices avanzan de 15 en 15 a partir del que ha sido escogido al azar en primer lugar. 2.3 Guía rápida sample(x, n) genera una muestra aleatoria de tamaño n del vector x. Si x es un número natural \\(x\\), representa el vector 1,2,…,\\(x\\). Dispone de los dos parámetros siguientes: replace, que igualado a TRUE produce muestras con reposición e igualado a FALSE (su valor por defecto) produce muestras sin reposición. prob, que permite especificar las probabilidades de aparición de los diferentes elementos de x (por defecto, son todas la misma). set.seed permite fijar la semilla de aleatoriedad. replicate(n,expresión) evalúa n veces la expresión, y organiza los resultados como las columnas de una matriz (o un vector, si el resultado de cada expresión es unidimensional). 2.4 Ejercicios Test (1) Queremos escoger 100 estudiantes de grado de la UIB para preguntarles cuántas horas semanales estudian. Como creemos que el tipo de estudio cursado influye en este dato, clasificamos los estudiantes según el centro (facultad o escuela) en el que están matriculados, y tomaremos una muestra al azar de cada centro, por sorteo a partir de la lista de todos los matriculados en ese centro y de manera que el tamaño de la muestra de cada centro sea proporcional al número de matriculados en el mismo. ¿De qué tipo de muestreo se tratará? Muestreo aleatorio simple Muestreo aleatorio estratificado Muestreo aleatorio sin reposición Muestreo aleatorio por conglomerados Muestreo aleatorio sistemático Ninguno de los anteriores (2) Con una sola instrucción, calculad la media de una muestra aleatoria sin reposición de 15 elementos escogidos de un vector numérico llamado \\(X\\). (3) Con una sola instrucción, extraed un subdataframe del dataframe iris formado por una muestra aleatoria sin reposición de 40 filas, y llamadlo muestra. Y antes de contestar, comprobad que funciona. (4) Con una sola instrucción, calculad un vector formado por las medias de 100 muestras aleatorias sin reposición de 20 elementos cada una escogidos de un vector numérico llamado \\(X\\) y llamadlo medias. Respuestas al test (1) b (2) mean(sample(X,15)) (También sería correcto sum(sample(X,15))/15. Y en ambos casos también sería correcto añadiendo dentro de la función sample el parámetro replace=FALSE, que hemos omitido porque es el valor por defecto de replace.) (3) muestra=iris[sample(dim(iris)[1],40),] (También sería correcto consultar antes el número de filas con str o tail, ver que son 150, y responder muestra=iris[sample(150,40),]. Hay otras respuestas correctas, no las damos para no liaros. Además, y como antes, también sería correcto añadir replace=FALSE.) (4) medias=replicate(100,mean(sample(X,20))) (¿Ya os hemos dicho que también sería correcto con replace=FALSE?) "],
["chap-estimacion.html", "Lección 3 Estimación puntual 3.1 Estimación máximo verosímil 3.2 Guía rápida 3.3 Ejercicios", " Lección 3 Estimación puntual En un estudio inferencial, una vez tomada la muestra y obtenidos los datos sobre sus miembros, el siguiente paso es inferir, es decir, deducir información sobre la población a partir de estos datos. Dicha información se puede deducir de dos formas: Suponiendo que conocemos el modelo al que se ajusta la población: es decir, suponiendo que conocemos el tipo de distribución de la variable aleatoria que modela la característica de la población en la que estamos interesados, pero desconocemos uno o varios parámetros de los que depende dicha distribución (observad que si lo sabemos todo sobre esta distribución, ya no hace falta tomar muestras para inferir algo sobre ella). Así, podemos saber (o suponer) que las longitudes de los ejemplares adultos de una cierta especie se distribuyen según una variable aleatoria normal, pero desconocer sus parámetros \\(\\mu\\) (media) y \\(\\sigma\\) (desviación típica), y usar este conocimiento para inferir información sobre dichas longitudes a partir de las de una muestra: por ejemplo, para estimar con un cierto margen de error su longitud media. Si estamos en este caso, hablaremos de estimación paramétrica. Suponiendo que desconocemos qué tipo de distribución tiene la variable aleatoria que modela la característica que nos interesa (aunque a veces necesitaremos saber algo de esta distribución; por ejemplo, si es simétrica o no). En este caso, hablaremos de estimación no paramétrica. En ambos casos, existen tres vías para obtener información sobre los parámetros de la distribución (conocida o desconocida) de la variable aleatoria que nos interesa: Estimación puntual. Se trata de obtener expresiones matemáticas, llamadas estimadores puntuales, que aplicadas a los valores de una muestra nos dan una aproximación (el término exacto es una estimación) del valor de dicho parámetro para la población. A modo de ejemplo, la media aritmética de los datos \\(x_1,\\ldots,x_n\\) de una muestra aleatoria, \\[ \\overline{x}=\\frac{x_1+\\cdots +x_n}{n}, \\] es un estimador del valor medio (valor esperado, esperanza) de la variable aleatoria de la que hemos extraído la muestra. Estimación por intervalos de confianza. Se trata de obtener intervalos que contengan con probabilidad alta el parámetro objeto de estudio. Trataremos este tema en la Lección 4. Contraste de hipótesis. Grosso modo, se establecen dos hipótesis opuestas sobre el parámetro o, más en general, sobre la distribución de la variable aleatoria, y se contrastan para intentar decidir cuál es la verdadera. Los estudiaremos en próximas lecciones. En esta lección hablaremos de la estimación puntual. Para empezar, es obvio que no toda fórmula matemática sirve para estimar de manera sensata el valor de un parámetro. Por ejemplo, si queréis estimar la media de las alturas de los habitantes de una población y disponéis de una muestra aleatoria de las mismas, no tomáis la raíz cuadrada de la altura máxima en la muestra como estimación de la altura media de la población, ¿verdad? Lo que habéis hecho toda la vida, y seguiréis haciendo en este curso, ha sido calcular la media de las alturas en la muestra y dar ese valor como estimación de la altura media poblacional. Y es lo correcto, porque la media muestral es siempre un estimador insesgado de la media poblacional y muy a menudo es además su estimador máximo verosímil, Veamos qué significan estas propiedades. Insesgado: Los valores de un estimador sobre muestras aleatorias de una población forman una variable aleatoria con una distribución de probabilidad propia, llamada genéricamente muestral. Decimos entonces que un estimador es insesgado cuando su valor esperado coincide con el valor del parámetro poblacional que se quiere estimar. Por ejemplo, si se toman muestras aleatorias con o sin reposición, la media muestral es siempre un estimador insesgado del valor medio poblacional: su valor esperado es el valor medio poblacional. Máximo verosímil: Cada muestra aleatoria de una población tiene una probabilidad de obtenerse que no solo depende de la muestra, sino también de la distribución de probabilidad de la variable aleatoria poblacional. Si la distribución poblacional es de un tipo concreto (Bernoulli, normal, …), esta probabilidad depende de sus parámetros. Decimos entonces que un estimador es máximo verosímil cuando el resultado que da sobre cada muestra aleatoria es el valor del parámetro poblacional que maximiza la probabilidad de obtenerla. Por ejemplo, si lanzamos una moneda al aire \\(n\\) veces y calculamos la proporción de veces que obtenemos cara, esa proporción muestral \\(\\widehat{p}\\) es el estimador máximo verosímil de la probabilidad \\(p\\) de obtener cara con esa moneda. Esto quiere decir que, de entre todas las distribuciones binomiales \\(B(n,p)\\) que pueden modelar el número de caras que obtenemos al lanzar \\(n\\) veces nuestra moneda, aquella que asigna mayor probabilidad al número de caras que hemos obtenido es la que tiene como parámetro \\(p\\) la frecuencia relativa de caras \\(\\widehat{p}\\) que hemos observado. Para algunas distribuciones, el método de estimación por máxima verosimilitud de sus parámetros da lugar a fórmulas cerradas más o menos sencillas, pero en otros casos nos tenemos que conformar con un valor aproximado obtenido mediante algún método numérico. 3.1 Estimación máximo verosímil A continuación recordamos una lista de los estimadores máximo verosímiles de los parámetros de las distribuciones más comunes a partir de una muestra aleatoria simple: Para la familia Bernoulli, el estimador máximo verosímil del parámetro \\(p\\) es la proporción muestral de éxitos \\(\\widehat{p}\\). Este estimador es además insesgado. Para la familia Poisson, el estimador máximo verosímil del parámetro \\(\\lambda\\) es la media muestral \\(\\overline{X}\\). Este estimador es de nuevo insesgado. Para la familia geométrica, el estimador máximo verosímil del parámetro \\(p\\) es \\({1}/{\\overline{X}}\\). Este estimador es sesgado. Para la familia exponencial, el estimador máximo verosímil del parámetro \\(\\lambda\\) también es \\({1}/{\\overline{X}}\\). Este estimador también es sesgado. Para la familia normal, los estimadores máximo verosímiles de la media \\(\\mu\\), la desviación típica \\(\\sigma\\) y la varianza \\(\\sigma^2\\) son, respectivamente, la media muestral \\(\\overline{X}\\), la desviación típica “verdadera” \\(S_X\\) y la varianza “verdadera” \\(S_X^2\\). Además, \\(\\overline{X}\\) es un estimador insesgado de \\(\\mu\\). La varianza verdadera \\(S_X^2\\) no es un estimador insesgado de \\(\\sigma^2\\), pero sí que lo es la varianza muestral \\(\\widetilde{S}^2\\). Y ninguna de las dos desviaciones típicas, ni la “verdadera” \\(S_X\\) ni la muestral \\(\\widetilde{S}_X\\), es un estimador insesgado de \\(\\sigma\\); si necesitáis un estimador insesgado de la desviación típica de una variable aleatoria normal a partir de una muestra aleatoria simple, lo podéis encontrar en la correspondiente entrada de la Wikipedia. No obstante, el beneficio de usar este estimador insesgado no suele compensar lo complicado de su cálculo. Cuando se estima algún parámetro de una distribución a partir de una muestra, es conveniente aportar el error típico, o estándar, como medida de la finura de la estimación. Recordemos que el error típico de un estimador es la desviación típica de su distribución muestral, y que el error típico de una estimación a partir de una muestra es la estimación del error típico del estimador usando dicha muestra. Veamos un ejemplo sencillo. Supongamos que tenemos una muestra aleatoria simple de tamaño \\(n\\) de una variable \\(X\\) que sigue una distribución Bernoulli de probabilidad poblacional \\(p\\) desconocida que queremos estimar. Por ejemplo, puede ser que tengamos una moneda posiblemente trucada, la hayamos lanzado 100 veces al aire y hayamos anotado los resultados (1, cara, 0, cruz), y a partir de este experimento queramos estimar la probabilidad de sacar cara con esta moneda. O que hayamos anotado para 100 individuos de una población elegidos al azar si tienen o no una determinada enfermedad (1 significa que sí, 0 que no) y a partir de esta muestra deseemos estimar la prevalencia de la enfermedad en la población, es decir, la proporción real de enfermos, que coincide con la probabilidad de que un individuo elegido al azar tenga dicha enfermedad. Tomemos, para fijar ideas, la siguiente muestra de tamaño 100: x=c(0,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,1,0,1,0,0,0, 0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0, 1,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0) En este caso, podemos estimar \\(p\\) mediante la proporción muestral de éxitos \\(\\widehat{p}\\), que coincide con la media muestral. El error típico de este estimador es \\(\\sqrt{p(1-p)/n}\\), y el error típico de una estimación concreta es \\(\\sqrt{\\widehat{p}(1-\\widehat{p})/n}\\). Por lo tanto, a mano podemos estimar \\(p\\) y calcular el error típico de dicha estimación de la manera siguiente: n=length(x) #Tamaño de la muestra estim.p=mean(x) #Proporción muestral estim.p ## [1] 0.22 error.tip.p=sqrt(estim.p*(1-estim.p)/n) #Error típico de la estimación error.tip.p ## [1] 0.04142463 De esta manera, estimamos que \\(p\\)=0.22 con un error típico de 0.04. Con R podemos estimar un parámetro de una distribución por el método de máxima verosimilitud a partir de una muestra y además obtener el error típico de dicha estimación usando la función fitdistr del paquete MASS. Esta función calcula los estimadores máximo verosímiles de los parámetros de la mayoría de las familias de distribuciones disponibles en R. Su sintaxis básica es fitdistr(x, densfun=..., start=...) donde x es la muestra, un vector numérico. El valor de densfun ha de ser el nombre de la familia de distribuciones; se tiene que entrar entre comillas y puede tomar, entre otros, los valores siguientes: &quot;chi-squared&quot;, &quot;exponential&quot;, &quot;f&quot;, &quot;geometric&quot;, &quot;lognormal&quot;, &quot;normal&quot; y &quot;poisson&quot;. La lista de distribuciones a las que se puede aplicar, que podéis consultar en la Ayuda de la función, no incluye la Bernoulli ni la binomial. Si fitdistr no dispone de una fórmula cerrada para el estimador máximo verosímil de algún parámetro, usa un algoritmo numérico para aproximarlo que requiere de un valor inicial para arrancar. Este valor (o valores) se puede especificar igualando el parámetro start a una list con cada parámetro a estimar igualado a un valor inicial. Para algunas distribuciones, como la &quot;t&quot;, fitdistr sabe tomar valores iniciales razonables, y no es necesario especificar el parámetro start. Pero para otras distribuciones, como por ejemplo la &quot;chi-squared&quot;, es obligatorio especificarlo. Para las distribuciones que disponen de fórmula cerrada, como la &quot;normal&quot; o la &quot;poisson&quot;, se tiene que omitir el parámetro start. Como no podemos usar fitdistr para estimar el parámetro \\(p\\) de una Bernoulli (los autores del paquete debieron de considerar que era más fácil estimarlo directamente), vamos a usarla en otro ejemplo. Consideremos la siguiente muestra y de 100 valores generados con distribución de Poisson de parámetro \\(\\lambda=10\\): set.seed(100) y=rpois(100,10) set.seed(NULL) y ## [1] 8 10 9 12 10 11 8 12 7 11 11 12 7 10 9 8 11 7 6 12 10 12 7 ## [24] 7 10 6 7 15 9 9 7 8 15 11 12 5 14 4 7 14 8 14 10 4 9 8 ## [47] 11 11 10 12 7 14 7 9 10 3 10 7 9 21 14 6 13 3 10 6 3 13 9 ## [70] 12 8 11 10 11 11 8 6 17 7 8 10 12 15 12 13 10 9 12 8 11 12 4 ## [93] 10 8 5 8 8 10 8 11 Vamos a estimar el parámetro \\(\\lambda\\) de una distribución Poisson que haya generado este vector: library(MASS) fitdistr(y, densfun=&quot;poisson&quot;) ## lambda ## 9.5600000 ## (0.3091925) El resultado dice que el valor estimado de \\(\\lambda\\) es 9.56, con un error típico estimado de 0.31. Veámoslo directamente: el estimador máximo verosímil de \\(\\lambda\\) es la media aritmética \\(\\overline{X}\\) y el error típico de este estimador es \\(\\sqrt{\\lambda}/\\sqrt{n}\\) (recordad que la desviación típica de una Poisson de parámetro \\(\\lambda\\) es \\(\\sqrt{\\lambda}\\) y que el error típico de la media muestral es la desviación típica poblacional dividida por la raíz cuadrada del tamaño de la muestra), por lo que el error típico de una estimación es \\(\\sqrt{\\overline{X}}/\\sqrt{n}\\). mean(y) ## [1] 9.56 sqrt(mean(y)/length(y)) ## [1] 0.3091925 También podemos estimar la media y la desviación típica de una variable normal que hubiera producido esta muestra. fitdistr(y, densfun=&quot;normal&quot;) ## mean sd ## 9.5600000 3.0832450 ## (0.3083245) (0.2180183) Observad que la estimación de la desviación típica que nos da fitdistr es la desviación típica “verdadera” (que es su estimador máximo verosímil) y no la muestral: sd(y) ## [1] 3.098778 sqrt((length(y)-1)/length(y))*sd(y) ## [1] 3.083245 Vamos a estimar ahora el número de grados de libertad de una t de Student que hubiera producido esta muestra. fitdistr(y, densfun=&quot;t&quot;) ## m s df ## 9.5085516 2.7517475 9.9229027 ## (0.2997949) (0.3072053) (8.4890355) ¡Vaya!, aparte del número de grados de libertad, df, han aparecido parámetros que no esperábamos. Los parámetros m y s son los parámetros de posición, \\(\\mu\\), y de escala, \\(\\sigma\\), respectivamente, que definen una familia más general de distribuciones t de Student (si os interesa, consultad esta entrada de la Wikipedia). Las que usamos en este curso tienen \\(\\mu=0\\) y \\(\\sigma=1\\). ¿Cómo podríamos estimar los grados de libertad de una t de Student de las nuestras? Especificando dentro de fitdistr los valores de los parámetros que queremos que tomen un valor concreto: en este caso, añadiendo m=0 y s=1. fitdistr(y, densfun=&quot;t&quot;, m=0, s=1) ## Error in fitdistr(y, densfun = &quot;t&quot;, m = 0, s = 1): &#39;start&#39; must be a named list Ahora R nos pide que demos un valor inicial al número de grados de libertad, df, para poder arrancar el algoritmo numérico que usará. Vamos a inicializarlo a 1, y de paso veremos cómo se usa este parámetro: fitdistr(y, densfun=&quot;t&quot;, m=0, s=1, start=list(df=1)) ## Warning in stats::optim(x = c(8L, 10L, 9L, 12L, 10L, 11L, 8L, 12L, 7L, 11L, : one-dimensional optimization by Nelder-Mead is unreliable: ## use &quot;Brent&quot; or optimize() directly ## Warning in dt((x - m)/s, df, log = TRUE): NaNs produced ## df ## 0.37265625 ## (0.04277075) Obtenemos un número estimado de grados de libertad de la t de Student de aproximadamente 0.37 grados de libertad (sí, los grados de libertad de una t de Student pueden ser un número real positivo cualquiera). Por otro lado, R nos avisa de que el resultado es poco de fiar, pero tampoco nos importa mucho, porque el objetivo era mostrar un ejemplo de cómo fijar valores de parámetros, igualándolos a dichos valores, y cómo especificar el parámetro start, como una list donde asignamos a cada parámetro un valor inicial. El resultado de fitdistr es una list, y por lo tanto el valor de cada estimador y su error típico se pueden obtener con los sufijos adecuados. En concreto, los valores estimados forman la componente estimate y los errores típicos la componente sd. Para obtenerlos directamente, basta usar los sufijos $estimate y $sd, respectivamente: fitdistr(y,&quot;poisson&quot;)$estimate #Estimación de lambda ## lambda ## 9.56 fitdistr(y,&quot;poisson&quot;)$sd #Error típico ## lambda ## 0.3091925 fitdistr(y,&quot;normal&quot;)$estimate #Estimaciones ## mean sd ## 9.560000 3.083245 fitdistr(y,&quot;normal&quot;)$estimate[1] #Estimación de mu ## mean ## 9.56 fitdistr(y,&quot;normal&quot;)$estimate[2] #Estimación de sigma ## sd ## 3.083245 3.2 Guía rápida fitdistr del paquete MASS, sirve para calcular los estimadores máximo verosímiles de los parámetros de una distribución a partir de una muestra. El resultado es una list que incluye los objetos estimate (los valores estimados) y sd (los errores típicos de las estimaciones). Sus parámetros principales son: densfun: el nombre de la familia de distribuciones, entre comillas. start: permite fijar el valor inicial del algoritmo numérico para calcular el estimador, si la función lo requiere. 3.3 Ejercicios Test (1) Las distribuciones de Weibull tienen dos parámetros: forma, shape, y escala, scale. Supongamos que los datos siguientes siguen una distribución de Weibull: 2.46, 2.28, 1.7, 0.62, 0.87, 2.81, 2.35, 2.08, 2.11, 1.72. Calculad el estimador máximo verosímil del parámetro de escala de esta distribución, redondeado a 3 cifras decimales. Tenéis que dar el resultado (sin ceros innecesarios a la derecha), no cómo lo habéis calculado. (2) Generad, con semilla de aleatoriedad igual a 42, una secuencia aleatoria de 100 valores con distribución geométrica Ge(0.6). A continuación estimad por máxima verosimilitud el parámetro \\(p\\) de una distribución geométrica que haya generado dicha muestra y dad como respuesta a esta pregunta el error típico de esta estimación redondeado a 3 cifras decimales. Tenéis que dar el resultado (sin ceros innecesarios a la derecha), no cómo lo habéis calculado. Respuestas al test (1) 2.116 Nosotros lo hemos calculado con x=c(2.46, 2.28, 1.7, 0.62, 0.87, 2.81, 2.35, 2.08, 2.11, 1.72) round(fitdistr(x,&quot;weibull&quot;)$estimate,3) (2) 0.038 Nosotros lo hemos calculado con set.seed(42) x=rgeom(100,0.6) round(fitdistr(x,&quot;geometric&quot;)$sd,3) "],
["chap-IC.html", "Lección 4 Intervalos de confianza 4.1 Intervalo de confianza para la media basado en la t de Student 4.2 Intervalos de confianza para la proporción poblacional 4.3 Intervalo de confianza para la varianza de una población normal 4.4 Bootstrap 4.5 Guía rápida 4.6 Ejercicios", " Lección 4 Intervalos de confianza En esta lección explicamos cómo calcular con R algunos intervalos de confianza básicos. Recordad que un intervalo de confianza del \\(q\\times 100\\%\\) (con \\(q\\) entre 0 y 1) para un parámetro poblacional (la media, la desviación típica, la probabilidad de éxito de una variable Bernoulli, …) es un intervalo obtenido aplicando a una muestra aleatoria simple una fórmula que garantiza (si se cumplen una serie de condiciones sobre la distribución de la variable aleatoria poblacional que en cada caso dependen del parámetro y de la fórmula) que el \\(q\\times 100\\%\\) de las veces que la aplicáramos a una muestra aleatoria simple de la misma población, el intervalo resultante contendría el parámetro poblacional que queremos estimar. Esto es lo que significa lo de “confianza del \\(q\\times 100\\%\\)”: que confiamos en que nuestra muestra pertenece al \\(q\\times 100\\%\\) de las muestras (aleatorias simples) en las que la fórmula acierta y da un intervalo que contiene el parámetro deseado. Algunas de las funciones que aparecen en esta lección volverán a salir en la próxima, ya que aunque calculan intervalos de confianza, su función principal es en realidad efectuar contrastes de hipótesis. 4.1 Intervalo de confianza para la media basado en la t de Student Supongamos que queremos estimar a partir de una m.a.s. la media \\(\\mu\\) de una población que sigue una distribución normal o tomando la muestra grande (por fijar una cota, de tamaño 40 o mayor). En esta situación, si \\(\\overline{X}\\), \\(\\widetilde{S}_{X}\\) y \\(n\\) son, respectivamente, la media muestral, la desviación típica muestral y el tamaño de la muestra, un intervalo de confianza del \\(q\\times 100\\%\\) para \\(\\mu\\) es \\[\\begin{equation} \\overline{X}\\pm t_{n-1,(1+q)/2} \\cdot \\frac{\\widetilde{S}_{X}}{\\sqrt{n}} \\tag{4.1} \\end{equation}\\] donde \\(t_{n-1,(1+q)/2}\\) es el cuantil de orden \\((1+q)/2\\) de una variable aleatoria con distribución t de Student con \\(n-1\\) grados de libertad. Fijaos en que \\(\\widetilde{S}_{X}/\\sqrt{n}\\) es el error típico de la estimación de la media. A la hora de calcular este intervalo de confianza, tenemos dos posibles situaciones. Una, típica de ejercicios, es cuando de la muestra sólo conocemos su media muestral \\(\\overline{X}\\), su desviación típica muestral \\(\\widetilde{S}_X\\) y su tamaño \\(n\\). Si los denotamos por x.b, sdm y n, respectivamente, y denotamos el nivel de confianza en tanto por uno \\(q\\) por q, podemos calcular los extremos de este intervalo de confianza con la expresión siguiente: x.b+(qt((1+q)/2,n-1)*sdm/sqrt(n))*c(-1,1) Ahora bien, “en la vida real” lo usual es disponer de un vector numérico X con los valores de la muestra. En este caso, podemos usar la función t.test de R, que, entre otra información, calcula estos intervalos de confianza para \\(\\mu\\). Si solo nos interesa el intervalo de confianza, podemos usar la sintaxis siguiente: t.test(X,conf.level=...)$conf.int donde tenemos que igualar el parámetro conf.level al nivel de confianza \\(q\\) en tanto por uno. Si \\(q=0.95\\), no hace falta entrarlo, porque es su valor por defecto. Ejemplo 4.1 Tenemos una muestra de pesos en gramos de 28 recién nacidos con luxación severa de cadera: pesos=c(2466,3941,2807,3118,3175,3515,3317,3742,3062,3033,2353,3515,3260,2892, 4423,3572,2750,3459,3374,3062,3205,2608,3118,2637,3438,2722,2863,3513) Vamos a suponer que nuestra muestra es aleatoria simple y que los pesos al nacer de los bebés con esta patología siguen una distribución normal. A partir de esta muestra, queremos calcular un intervalo de confianza del 95% para el peso medio de un recién nacido con luxación severa de cadera, y ver si contiene el peso medio de la población global de recién nacidos, que es de unos 3400 g. Como suponemos que la variable aleatoria poblacional es normal, para calcular un intervalo de confianza del 95% para su valor medio vamos a usar la fórmula basada en la distribución t de Student, y por lo tanto la función t.test: t.test(pesos)$conf.int ## [1] 2997.849 3355.008 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 El intervalo que obtenemos es [2997.8, 3355] y está completamente a la izquierda del peso medio global de 3400 g, por lo que tenemos evidencia (a un 95% de confianza) de que los niños con luxación severa de cadera pesan de media al nacer por debajo de la media global. La apostilla entre paréntesis “a un 95% de confianza” aquí significa que hemos basado esta conclusión en un intervalo obtenido con una fórmula que acierta con una probabilidad del 95%, en el sentido de que el 95% de las ocasiones que aplicamos esta fórmula a una m.a.s. de una variable aleatoria normal, produce un intervalo que contiene la media de esta variable. Observad que el resultado de t.test(pesos)$conf.int tiene un atributo, conf.level, que indica su nivel de confianza. En principio este atributo no molesta para nada en cálculos posteriores con los extremos de este intervalo de confianza, pero si os molesta, lo podéis quitar igualándolo a NULL. IC.lux=t.test(pesos)$conf.int attr(IC.lux,&quot;conf.level&quot;)=NULL IC.lux ## [1] 2997.849 3355.008 Veamos cómo podríamos haber obtenido este intervalo directamente con la fórmula (4.1): x=mean(pesos) sdm=sd(pesos) n=length(pesos) q=0.95 x+(qt((1+q)/2,n-1)*sdm/sqrt(n))*c(-1,1) ## [1] 2997.849 3355.008 Como podéis ver, coincide con el intervalo obtenido con la función t.test. Ejemplo 4.2 Vamos a comprobar con un experimento esto de la “confianza” de los intervalos de confianza, y en concreto de la fórmula (4.1). Vamos a generar al azar una Población de 10,000,000 “individuos” con distribución normal estándard. Vamos a tomar 200 muestras aleatorias simples de tamaño 50 de esta población y calcularemos el intervalo de confianza para la media poblacional usando dicha fórmula. Finalmente, contaremos cuántos de estos intervalos de confianza contienen la media de la Población. Fijaremos la semilla de aleatoriedad para que el experimento sea reproducible y podáis comprobar que no hacemos trampa. En otras simulaciones habríamos obtenido resultados mejores o peores, es lo que tienen las simulaciones aleatorias. set.seed(42) Poblacion=rnorm(10^7) #La población mu=mean(Poblacion) # La media poblacional M=replicate(200, sample(Poblacion,50,replace=TRUE)) # Las muestras dim(M) ## [1] 50 200 Tenemos una matriz M de 200 columnas y 50 filas, donde cada columna es una m.a.s. de nuestra población. Vamos a aplicar a cada una de estas muestras la función t.test para calcular un intervalo de confianza del 95% y luego contaremos los aciertos, es decir, cuántos de ellos contienen la media poblacional IC.t=function(X){t.test(X)$conf.int} ICs=apply(M,FUN=IC.t,MARGIN=2) Aciertos=length(which((mu&gt;=ICs[1,]) &amp; (mu&lt;=ICs[2,]))) Aciertos ## [1] 189 Hemos acertado 189 veces, es decir, un 94.5% de los intervalos obtenidos contienen la media poblacional. No hemos quedado muy lejos del 95% esperado. Para visualizar mejor los aciertos, vamos a dibujar los intervalos apilados en un gráfico, donde aparecerán en azul claro los que aciertan y en rojo los que no aciertan. plot(1,type=&quot;n&quot;,xlim=c(-0.8,0.8),ylim=c(0,200),xlab=&quot;Valores&quot;,ylab=&quot;Repeticiones&quot;,main=&quot;&quot;) seg.int=function(i){ color=&quot;light blue&quot;; if((mu&lt;ICs[1,i]) | (mu&gt;ICs[2,i])){color = &quot;red&quot;} segments(ICs[1,i],i,ICs[2,i],i,col=color,lwd=2) } sapply(1:200,FUN=seg.int) abline(v=mu,lwd=2) Figura 4.1: Aciertos y errores en 200 Intervalos de confianza al 95% Fijaos en que los errores no se distribuyen por igual a los dos lados, hay muchos más intervalos que dejan la media poblacional a su izquierda que a su derecha, mientras que, en teoría, tendríamos que esperar que en la mitad de los errores la media poblacional estuviera a la izquierda del intervalo calculado y en la otra mitad a la derecha. Cosas de la aleatoriedad. 4.2 Intervalos de confianza para la proporción poblacional En esta sección consideramos el caso en que la población objeto de estudio sigue una distribución Bernoulli y queremos estimar su probabilidad de éxito (o proporción poblacional) \\(p\\). Para ello, tomamos una muestra aleatoria simple de tamaño \\(n\\) y número de éxitos \\(x\\), y, por lo tanto, de proporción muestral de éxitos \\(\\widehat{p}_X=x/n\\). El método “exacto” de Clopper-Pearson para calcular un intervalo de confianza del \\(q\\times 100\\%\\) para \\(p\\) se basa en el hecho de que, en estas condiciones, el valor de \\(x\\) sigue una distribución binomial \\(B(n,p)\\). Este método se puede usar siempre, sin ninguna restricción sobre la muestra, y consiste básicamente en encontrar los valores \\(p_0\\) y \\(p_1\\) tales que \\[ \\sum_{k=x}^n\\binom{n}{k}p_0^k(1-p_0)^{n-k}=(1-q)/2,\\qquad \\displaystyle\\sum_{k=0}^x\\binom{n}{k}p_1^k(1-p_1)^{n-k}=(1-q)/2 \\] y dar el intervalo \\([p_0,p_1]\\). Para calcular este intervalo se puede usar la función binom.exact del paquete epitools. Su sintaxis es binom.exact(x,n,conf.level) donde x y n representan, respectivamente, el número de éxitos y el tamaño de la muestra, y conf.level es \\(q\\), el nivel de confianza en tanto por uno. El valor por defecto de conf.level es 0.95. Ejemplo 4.3 Supongamos que, de una muestra de 15 enfermos tratados con un cierto medicamento, solo 1 ha desarrollado taquicardia. Queremos conocer un intervalo de confianza del 95% para la proporción de enfermos tratados con este medicamento que presentan este efecto adverso. Tenemos una población Bernoulli, formada por los enfermos tratados con el medicamento en cuestión, donde los éxitos son los enfermos que desarrollan taquicardia. La fracción de éstos es la fracción poblacional \\(p\\) para la que queremos calcular el intervalo de confianza del 95%. Para ello cargamos el paquete epitools y usamos binom.exact: library(epitools) binom.exact(1,15) ## x n proportion lower upper conf.level ## 1 1 15 0.06666667 0.00168643 0.3194846 0.95 El resultado de la función binom.exact es un data frame; el intervalo de confianza deseado está formado por los números en las columnas lower (extremo inferior) y upper (extremo superior): binom.exact(1,15)$lower ## [1] 0.00168643 binom.exact(1,15)$upper ## [1] 0.3194846 Hemos obtenido el intervalo de confianza [0.002,0.319]: podemos afirmar con un nivel de confianza del 95% que el porcentaje de enfermos tratados con este medicamento que presentan este efecto adverso está entre el 0.2% y el 31.9%. Supongamos ahora que el tamaño \\(n\\) de la muestra aleatoria simple es grande; de nuevo, digamos que \\(n{\\geqslant}40\\). En esta situación, podemos usar el Método de Wilson para aproximar, a partir del Teorema Central del Límite, un intervalo de confianza del parámetro \\(p\\) al nivel de confianza \\(q\\times 100\\%\\), mediante la fórmula \\[ \\frac{\\widehat{p}_{X}+\\frac{z_{(1+q)/2}^2}{2n}\\pm z_{(1+q)/2}\\sqrt{\\frac{\\widehat{p}_{X}(1-\\widehat{p}_{X})}{n}+\\frac{z_{(1+q)/2}^2}{4n^2}}}{1+\\frac{z_{(1+q)/2}^2}{n}} \\] donde \\(z_{(1+q)/2}\\) es el cuantil de orden \\((1+q)/2\\) de una variable aleatoria normal estándar. Para calcular este intervalo se puede usar la función binom.wilson del paquete epitools. Su sintaxis es binom.wilson(x,n,conf.level) con los mismos parámetros que binom.exact. Ejemplo 4.4 Supongamos que tratamos 45 ratones con un agente químico, y 10 de ellos desarrollan un determinado cáncer de piel. Queremos calcular un intervalo de confianza al 90% para la proporción \\(p\\) de ratones que desarrollan este cáncer de piel al ser tratados con este agente químico. Como 45 es relativamente grande, usaremos el método de Wilson. Para comparar los resultados, usaremos también el método exacto. Fijaos que, en este ejemplo, \\(q=0.9\\). binom.wilson(10,45,0.9) ## x n proportion lower upper conf.level ## 1 10 45 0.2222222 0.1377238 0.3382281 0.9 binom.exact(10,45,0.9) ## x n proportion lower upper conf.level ## 1 10 45 0.2222222 0.1258186 0.3477285 0.9 Con el método de Wilson obtenemos el intervalo [0.138,0.338] y con el método de Clopper-Pearson, el intervalo [0.126,0.348], un poco más ancho: hay una diferencia en los extremos de alrededor de un punto porcentual. Supongamos finalmente que la muestra aleatoria simple es considerablemente más grande que la usada en el método de Wilson y que, además, la proporción muestral de éxitos \\(\\widehat{p}_{X}\\) está alejada de 0 y de 1. Una posible manera de formalizar estas condiciones es requerir que \\(n{\\geqslant}100\\) y que \\(n\\widehat{p}_{X}{\\geqslant}10\\) y \\(n(1-\\widehat{p}_{X}){\\geqslant}10\\); observad que estas dos últimas condiciones son equivalentes a que tanto el número de éxitos como el número de fracasos en la muestra sean como mínimo 10. En este caso, se puede usar la fórmula de Laplace, que simplifica la de Wilson (aunque, en realidad, la precede en más de 100 años): un intervalo de confianza del parámetro \\(p\\) al nivel de confianza \\(q\\times 100\\%\\) viene dado aproximadamente por la fórmula \\[ \\widehat{p}_{X}\\pm z_{(1+q)/2}\\sqrt{\\frac{\\widehat{p}_{X} (1-\\widehat{p}_{X})}{n}} \\] Esta fórmula está implementada en la función binom.approx del paquete epitools, de uso similar al de las dos funciones anteriores. Ejemplo 4.5 En una muestra aleatoria de 500 familias con niños en edad escolar de una determinada ciudad se ha observado que 340 introducen fruta de forma diaria en la dieta de sus hijos. A partir de este dato, queremos encontrar un intervalo de confianza del 95% para la proporción real de familias de esta ciudad con niños en edad escolar que incorporan fruta fresca de forma diaria en la dieta de sus hijos. Tenemos una población Bernoulli donde los éxitos son las familias que aportan fruta de forma diaria a la dieta de sus hijos, y la fracción de estas familias en el total de la población es la proporción poblacional \\(p\\) para la que queremos calcular el intervalo de confianza. Como \\(n\\) es muy grande y los números de éxitos y fracasos también lo son, podemos emplear el método de Laplace. binom.approx(340,500) ## x n proportion lower upper conf.level ## 1 340 500 0.68 0.6391123 0.7208877 0.95 Por lo tanto, según la fórmula de Laplace, un intervalo de confianza al 95% para la proporción poblacional es [0.639,0.721]. ¿Qué hubiéramos obtenido con los otros dos métodos? binom.wilson(340,500) ## x n proportion lower upper conf.level ## 1 340 500 0.68 0.637873 0.7193822 0.95 binom.exact(340,500) ## x n proportion lower upper conf.level ## 1 340 500 0.68 0.6371369 0.7207188 0.95 Como podéis ver, los resultados son muy parecidos, con diferencias de unas pocas milésimas. 4.3 Intervalo de confianza para la varianza de una población normal Supongamos ahora que queremos estimar la varianza \\(\\sigma^2\\), o la desviación típica \\(\\sigma\\), de una población que sigue una distribución normal. Tomamos una muestra aleatoria simple de tamaño \\(n\\), y sea \\(\\widetilde{S}_{X}\\) su desviación típica muestral. En esta situación, un intervalo de confianza del \\(q\\times 100\\%\\) para la varianza \\(\\sigma^2\\) es \\[ \\left[ \\frac{(n-1)\\widetilde{S}_{X}^2}{\\chi_{n-1,(1+q)/2}^2},\\ \\frac{(n-1)\\widetilde{S}_{X}^2}{\\chi_{n-1,(1-q)/2}^2}\\right], \\] donde \\(\\chi_{n-1,(1-q)/2}^2\\) y \\(\\chi_{n-1,(1+q)/2}^2\\) son, respectivamente, los cuantiles de orden \\((1-q)/2\\) y \\((1+q)/2\\) de una variable aleatoria que sigue una distribución \\(\\chi^2\\) con \\(n-1\\) grados de libertad. Si conocéis la varianza muestral \\(\\widetilde{S}_{X}^2\\), que denotaremos por varm, podéis calcular este intervalo de confianza para la varianza con la fórmula siguiente (donde qindica el nivel de confianza en tanto por uno \\(q\\)): c((n-1)*varm/qchisq((1+q)/2,n-1),(n-1)*varm/qchisq((1-q)/2,n-1)) Si, en cambio, disponéis de la muestra, podéis calcular este intervalo de confianza con la función varTest del paquete EnvStats. La sintaxis es similar a la usada con t.test: varTest(X,conf.level)$conf.int donde X es el vector que contiene la muestra y conf.level el nivel de confianza, que por defecto es igual a 0.95. Ejemplo 4.6 Un índice de calidad de un reactivo químico es el tiempo que tarda en actuar. Se supone que la distribución de este tiempo de actuación del reactivo es aproximadamente normal. Se realizaron 30 pruebas independientes, que forman una muestra aleatoria simple, en las que se midió el tiempo de actuación del reactivo. Los tiempos obtenidos fueron reactivo = c(12,13,13,14,14,14,15,15,16,17,17,18,18,19,19,25,25,26,27,30,33,34,35, 40,40,51,51,58,59,83) Queremos usar estos datos para calcular un intervalo de confianza del 95% para la desviación típica de este tiempo de actuación. El siguiente código calcula un intervalo de confianza al 95% para la varianza a partir del vector reactivo: library(EnvStats) varTest(reactivo)$conf.int ## LCL UCL ## 191.2627 544.9572 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Este intervalo de confianza es para la varianza. Como la desviación típica es la raíz cuadrada de la varianza, para obtener un intervalo de confianza al 95% para la desviación típica, tenemos que tomar la raíz cuadrada de este intervalo para la varianza: sqrt(varTest(reactivo)$conf.int) ## LCL UCL ## 13.82977 23.34432 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Por lo tanto un intervalo de confianza del 95% para la desviación típica poblacional es [13.83, 23.34]. De nuevo, si os molesta, podéis eliminar el atributo conf.level igualándolo a NULL. 4.4 Bootstrap Cuando no tiene sentido usar un método paramétrico como los explicados en las secciones anteriores para calcular un intervalo de confianza porque no se satisfacen las condiciones teóricas que garantizan que el intervalo obtenido contiene el 95% de las veces el parámetro poblacional deseado, podemos recurrir a un método no paramétrico. El más utilizado es el bootstrap, que básicamente consiste en: Remuestrear la muestra: tomar muchas muestras aleatorias simples de la muestra de la que disponemos, cada una de ellas del mismo tamaño que la muestra original (pero simples, es decir, con reposición). Calcular el estimador sobre cada una de estas submuestras. Organizar los resultados en un vector. Usar este vector para calcular un intervalo de confianza. La manera más sencilla de llevar a cabo el cálculo final del intervalo de confianza es el llamado método de los percentiles, en el que se toman como extremos del intervalo de confianza del \\(q\\times 100\\%\\) los cuantiles de orden \\((1-q)/2\\) y \\((1+q)/2\\) del vector de estimadores, pero hay mucho otros métodos; encontraréis algunos en esta entrada de la Wikipedia. Ejemplo 4.7 Volvamos a la muestra de pesos del Ejemplo 4.1, pero supongamos ahora que la variable aleatoria poblacional de la que la hemos extraído no es normal (o que no queremos suponer que lo sea). Vamos a usar el método bootstrap de los percentiles para calcular un intervalo de confianza del 95% para el peso medio poblacional. Para ello, vamos a general 1000 muestras aleatorias simples de la muestra, todas ellas del mismo tamaño que la muestra, calcularemos la media de cada una de estas muestras simples, construiremos un vector con estas medias muestrales, y daremos como extremos del intervalo de confianza los cuantiles de orden 0.025 y 0.975 del vector así obtenido. set.seed(42) n=length(pesos) X=replicate(1000,mean(sample(pesos,n,replace=TRUE))) IC.boot=c(quantile(X,0.025),quantile(X,0.975)) round(IC.boot,1) ## 2.5% 97.5% ## 3000.0 3348.2 El intervalo obtenido en este caso es [3000, 3348.2]; como se trata de un método basado en una simulación aleatoria, seguramente con otra semilla de aleatoriedad daría un intervalo diferente. Para comparar, recordad que el intervalo de confianza obtenido con la fórmula basada en la t de Student ha sido [2997.8, 3355]. El paquete boot dispone de la función boot para llevar a cabo simulaciones bootstrap. Aplicando luego la función boot.ci al resultado de la función boot obtenemos diversos intervalos de confianza basados en el enfoque bootstrap. La sintaxis básica de la función boot es boot(X,estadístico,R) donde: X es el vector que forma la muestra de la que disponemos R es el número de muestras que queremos extraer de la muestra original El estadístico es la función que calcula el estadístico deseado de la submuestra, y tiene que tener dos parámetros: el primero representa la muestra original X y el segundo representa el vector de índices de una m.a.s. de X. Por ejemplo, si vamos a usar la función boot para efectuar una simulación bootstrap de medias muestrales, podemos tomar como estadístico la función: media.boot=function(X,índices){mean(X[índices])} Por otro lado, el nivel de confianza se especifica en la función boot.ci mediante el parámetro conf (no conf.level, como hasta ahora), cuyo valor por defecto es, eso sí, el de siempre: 0.95. A modo de ejemplo, vamos a usar las funciones del paquete boot para calcular un intervalo de confianza del 95% para la media de la variable aleatoria que ha producido el vector pesos. library(boot) set.seed(42) simulacion=boot(pesos,media.boot,1000) El resultado simulacion de esta última instrucción es una list que incluye, en su componente t, el vector de 1000 medias muestrales obtenido mediante la simulación; sus 10 primeros valores son: simulacion$t[1:10] ## [1] 3344.57 3107.43 3162.89 3038.86 3094.64 3202.71 3151.25 3020.39 ## [9] 3072.75 3284.75 Calculemos ahora el intervalo de confianza deseado: boot.ci(simulacion) ## Warning in boot.ci(simulacion): bootstrap variances needed for studentized ## intervals ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = simulacion) ## ## Intervals : ## Level Normal Basic ## 95% (3022, 3345 ) (3021, 3336 ) ## ## Level Percentile BCa ## 95% (3017, 3332 ) (3028, 3359 ) ## Calculations and Intervals on Original Scale Obtenemos cuatro intervalos de confianza para \\(\\mu\\), calculados con cuatro métodos a partir de la simulación realizada (y un aviso de que no ha podido calcular un quinto intervalo). Cada uno de estos intervalos es un objeto de una list y por lo tanto se puede obtener con el sufijo adecuado, que podréis deducir del resultado de aplicar str al resultado de una función boot.ci. El intervalo Percentile es el calculado con el método de los percentiles que hemos explicado antes, y se obtiene con el sufijo $percent[4:5] (no ha dado lo mismo que antes, pese a usar la misma semilla de aleatoriedad, porque el procedimiento interno usado por la función boot para remuestrear el vector pesos ha sido diferente). No vamos a entrar en detalle sobre los métodos que usa para calcular el resto de intervalos, en realidad todos tienen ventajas e inconvenientes. Ejemplo 4.8 ¿Realmente funciona el enfoque bootstrap? Vamos a retomar el experimento realizado en el Ejemplo 4.2, donde construimos una matriz M cuyas columnas son 200 muestras aleatorias simples de tamaño 50 de una población que sigue una distribución normal estándard. En dicho ejemplo calculamos para cada una de estas muestras el intervalo de confianza del 95% para la media poblacional usando la fórmula (4.1), que es la recomendada por la teoría en este caso. De los 200 intervalos calculados, 189 contuvieron la media poblacional, lo que representa un 94.5% de aciertos. Ahora vamos a calcular para cada una de estas muestras un intervalo de confianza del 95% por el método bootstrap de los percentiles y compararemos las tasas de aciertos. Aunque se puede comprobar fácilmente que no es el caso, para mayor seguridad vamos a volver a generar en las mismas condiciones las 200 muestras de la población, no sea que a lo largo de la lección hayamos modificado inadvertidamente el contenido de la matriz M (y así de paso fijamos la semilla de aleatoriedad). set.seed(42) Poblacion=rnorm(10^7) #La población mu=mean(Poblacion) # La media poblacional M=replicate(200, sample(Poblacion,50,replace=TRUE)) # Las muestras IC.b=function(X){boot.ci(boot(X,media.boot,1000))$percent[4:5]} ICs.bootstrap=apply(M,FUN=IC.b,MARGIN=2) Aciertos.bootstrap=length(which((mu&gt;=ICs.bootstrap[1,]) &amp; (mu&lt;=ICs.bootstrap[2,]))) Aciertos.bootstrap ## [1] 186 Con el bootstrap, hemos acertado en 186 ocasiones, lo que supone un 93% de aciertos. 4.5 Guía rápida t.test(X, conf.level=...)$conf.int calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la media poblacional usando la fórmula basada en la t de Student aplicada a la muestra X. binom.exact(x,n,conf.level=...) del paquete epitools, calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la proporción poblacional aplicando el método de Clopper-Pearson a una muestra de tamaño n con x éxitos. binom.wilson(x,n,conf.level=...) del paquete epitools, calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la proporción poblacional aplicando el método de Wilson a una muestra de tamaño n con x éxitos. binom.approx(x,n,conf.level=...) del paquete epitools, calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la proporción poblacional aplicando la fórmula de Laplace a una muestra de tamaño n con x éxitos. varTest(X,conf.level=...)$conf.int del paquete EnvStats, calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la varianza poblacional usando la fórmula basada en la ji cuadrado aplicada a la muestra X. boot(X,E,R) del paquete boot, lleva a cabo una simulación bootstrap, tomando R submuestras del vector X y calculando sobre ellas el estadístico representado por la función E. boot.ci del paquete boot, aplicado al resultado de una función boot, calcula diversos intervalos de confianza a partir del resultado de la simulación efectuada con boot. El nivel de confianza se especifica con el parámetro conf. 4.6 Ejercicios Modelo de test (1) Tomad la muestra de todas las longitudes de pétalos de flores Iris setosa contenida en la tabla de datos iris y usadla para calcular un intervalo de confianza del 95% para el valor medio de las longitudes de pétalos de esta especie de flores usando la fórmula basada en la t de Student. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 2 cifras decimales (sin ceros innecesarios a la derecha). (2) Tenemos una población de media \\(\\mu\\) desconocida. Tomamos una muestra aleatoria simple de tamaño 80 y obtenemos una media muestral de 6.2 y una desviación típica muestral de 1.2. Usad estos datos y la fórmula del intervalo de confianza para la media basado en la t de Student para calcular un intervalo de confianza al 95% para \\(\\mu\\). Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 2 cifras decimales (sin ceros innecesarios a la derecha). (3) Tenemos una población Bernoulli de proporción poblacional \\(p\\) desconocida. Tomamos una muestra aleatoria simple de 80 individuos y obtenemos una proporción muestral de 35% de éxitos. Calculad un intervalo de confianza para \\(p\\) a un nivel de confianza del 95% usando el método de Wilson. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 3 cifras decimales (sin ceros innecesarios a la derecha). (4) Tomad la muestra de todas las longitudes de pétalos de flores Iris setosa contenida en la tabla de datos iris y usadla para calcular un intervalo de confianza del 95% para la varianza de las longitudes de pétalos de esta especie de flores. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 3 cifras decimales (sin ceros innecesarios a la derecha). Respuestas al test (1) 1.41,1.51 Nosotros lo hemos calculado con round(t.test(iris[iris$Species==&quot;setosa&quot;,&quot;Petal.Length&quot;])$conf.int,2) ## [1] 1.41 1.51 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 (2) 5.933,6.467 Nosotros lo hemos calculado con x=6.2 n=80 sdm=1.2 conf.level=0.95 round(x+(qt((1+conf.level)/2,n-1)*sdm/sqrt(n))*c(-1,1),3) ## [1] 5.933 6.467 (3) 0.255,0.459 Nosotros lo hemos calculado con round(binom.wilson(0.35*80,80),3) ## x n proportion lower upper conf.level ## 1 28 80 0.35 0.255 0.459 0.95 (4) 0.021,0.047 Nosotros lo hemos calculado con round(varTest(iris[iris$Species==&quot;setosa&quot;,&quot;Petal.Length&quot;])$conf.int,3) ## LCL UCL ## 0.021 0.047 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 "],
["chap-contrastes.html", "Lección 5 Contrastes de hipótesis 5.1 Contrastes para medias 5.2 Contrastes para varianzas 5.3 Contrastes para proporciones 5.4 Cálculo de la potencia de un contraste 5.5 Guía rápida 5.6 Ejercicios", " Lección 5 Contrastes de hipótesis En esta lección explicamos algunas instrucciones de R que permiten llevar a cabo contrastes de hipótesis sobre parámetros poblacionales. Antes de empezar, repasemos el vocabulario básico relacionado con los contrastes de hipótesis: Hipótesis alternativa, \\(H_1\\): Aquella de la que buscamos evidencia en nuestro estudio. Hipótesis nula, \\(H_0\\): La hipótesis que estamos dispuestos a aceptar si no encontramos evidencia suficiente de la hipótesis alternativa. Suele plantearse en términos de “no hay diferencia”. Contraste bilateral: Contraste en el que la hipótesis alternativa viene definida por un \\(\\neq\\): que un parámetro sea diferente de un valor dado, que un parámetro sobre una población sea diferente del mismo parámetro sobre otra población, … Contraste unilateral: Contraste en el que la hipótesis alternativa viene definida por un \\(&gt;\\) o un \\(&lt;\\): que un parámetro sea mayor que un valor dado, que un parámetro sobre una población sea menor que el mismo parámetro sobre otra población, … Error de tipo I, o Falso positivo: Concluir que la hipótesis alternativa es verdadera cuando en realidad es falsa. Nivel de significación, \\(\\alpha\\): La probabilidad de cometer un error de tipo I. Nivel de confianza, \\(1-\\alpha\\): La probabilidad de no cometer un error de tipo I. Error de tipo II, o Falso negativo: Concluir que la hipótesis alternativa es falsa cuando en realidad es verdadera. Potencia, \\(1-\\beta\\): La probabilidad de no cometer un error de tipo II. Estadístico de contraste: El valor que se calcula a partir de la muestra obtenida en el estudio y que se usará para tomar la decisión en el contraste planteado. p-valor: La probabilidad de que, si la hipótesis nula es verdadera, el estadístico de contraste tome un valor tan o más extremo en el sentido de la hipótesis alternativa que el obtenido en el estudio Intervalo de confianza al nivel de confianza \\(1-\\alpha\\): Un intervalo en el que el parámetro poblacional que contrastamos tiene probabilidad \\(1-\\alpha\\) de pertenecer en el sentido de los intervalos de confianza de la Lección 4: es decir, porque se ha obtenido con un procedimiento que produce intervalos que, en el \\((1-\\alpha)\\times 100\\%\\) de las ocasiones que lo aplicamos a muestras aleatorias simples, contiene el parámetro poblacional. Está formado por los valores del parámetro poblacional que, si fueran los que contrastáramos en nuestro contraste, producirían un p-valor como mínimo \\(\\alpha\\). Los intervalos de confianza de los contrastes bilaterales coinciden con los definidos en la Lección 4. Regla de rechazo: Rechazamos la hipótesis nula en favor de la alternativa con un nivel de significación \\(\\alpha\\) dado cuando se da alguna de las dos condiciones siguientes (que son equivalentes, es decir, se dan las dos o ninguna): El p-valor és menor que el nivel de significación. El valor contrastado del parámetro poblacional no pertenece al intervalo de confianza del nivel \\(1-\\alpha\\). Ejemplo 5.1 Tenemos una moneda y creemos que está trucada a favor de Cara, es decir, que al lanzarla al aire produce más caras que cruces. Para intentar decidir si esto es cierto o no, lanzamos la moneda al aire 20 veces y obtenemos 15 caras. Llamemos \\(p\\) a la probabilidad de obtener cara al lanzar al aire esta moneda. Entonces: La hipótesis alternativa, de la que buscamos evidencia, es que la moneda está trucada a favor de cara: \\(H_1: p&gt;0.5\\). La hipótesis nula, que aceptaremos por defecto si no encontramos evidencia de que la alternativa sea verdadera, es que la moneda no está trucada: \\(H_0: p=0.5\\). Cometeríamos un error de tipo I si la moneda fuera honrada y nosotros concluyéramos que está trucada. Cometeríamos un error de tipo II si la moneda estuviera trucada y nosotros concluyéramos que no lo está. El estadístico de contraste en este experimento es simplemente el número de caras en un serie de lanzamientos de la moneda. El p-valor es la probabilidad de obtener 15 o más caras al lanzar al aire 20 veces la moneda si fuera verdad que \\(p=0.5\\). Como en este caso el número de caras seguiría una distribución binomial \\(B(20,0.5)\\), podemos calcular fácilmente esta probabilidad: 1-pbinom(14,20,0.5) ## [1] 0.0206947 Por lo tanto, el p-valor es 0.021. El intervalo de confianza del 95% de este contraste está formado por los valores de \\(p\\) para los que la probabilidad de obtener 15 o más caras al lanzar al aire 20 veces la moneda es mayor o igual que el 5%, y es [0.5444, 1]. Y bueno, tras todo este vocabulario, ¿cuál sería la conclusión? El p-valor obtenido significa que si la moneda no estuviera trucada, la probabilidad de obtener el número de caras que hemos obtenido o más es muy pequeña, lo que hace difícil de creer que la moneda no esté trucada. En particular, si trabajamos con un nivel de significación del 5%, como el p-valor es más pequeño que 0.05, rechazamos la hipótesis nula. Equivalentemente, como el intervalo de confianza del 95% para la \\(p\\) está completamente a la derecha del valor que contrastamos, 0.5, con este nivel de confianza hemos de concluir que \\(p&gt;0.5\\). En resumen, aceptando una probabilidad de error de tipo I (de concluir que una moneda honrada está trucada) del 5%, rechazamos la hipótesis nula e inferimos que la moneda está trucada a favor de cara. Figura 5.1: “Hipótesis Nula” (http://imgs.xkcd.com/comics/null_hypothesis.png (CC-BY-NC 2.5)) 5.1 Contrastes para medias El test t El test t para contrastar una o dos medias basado en la t de Student está implementado en la función t.test. Este test usa diferentes estadísticos según que el contraste sea de una media o de dos; en este último caso, según se usen muestras emparejadas o independientes; y en este último caso, según las poblaciones tengan varianzas iguales o diferentes. Aunque este test solo es exacto (en el sentido de que da la conclusión con el nivel de significación requerido) cuando las poblaciones involucradas siguen distribuciones normales, el Teorema Central del Límite garantiza que también da resultados aproximadamente correctos cuando las muestras son grandes, aunque las poblaciones no sean normales, por lo que en esta situación también se recomienda su uso. Así pues, aunque en la práctica el test t se use como test “de talla única” para contrastar una o dos medias en cualquier situación, hay que tener claro que su resultado es fiable tan solo: cuando las variables poblacionales involucradas son (aproximadamente) normales, o cuando todas las muestras usadas son grandes. Al final de esta sección explicamos las funciones asociadas a algunos contrastes no paramétricos que pueden usarse cuando estas condiciones no se cumplen. La sintaxis básica de la función t.test es t.test(x, y, mu=..., alternative=..., conf.level=..., paired=..., var.equal=...) donde: x es el vector de datos que forma la muestra que analizamos. y es un vector opcional; si lo entramos, R entiende que estamos realizando un contraste de dos medias, con hipótesis nula la igualdad de estas medias. Podemos sustituir los vectores x e y por una fórmula variable1~variable2 que indique que separamos la variable numérica variable1 en dos vectores definidos por los niveles de un factor variable2 de dos niveles (o de otra variable asimilable a un factor de dos niveles, como por ejemplo una variable numérica que solo tome dos valores diferentes). Con esta construcción, R tomará estos vectores en el orden natural de los niveles de variable2: x será el vector correspondiente al primer nivel e y el correspondiente al segundo. Hay que tener esto en cuenta a la hora de especificar la hipótesis alternativa si es unilateral. Si las dos variables de la fórmula son columnas de un dataframe, se puede usar el parámetro data=... para indicarlo. Solamente tenemos que especificar el parámetro mu si hemos entrado una sola muestra, y en este caso lo hemos de igualar al valor \\(\\mu_0\\) que queremos contrastar, de manera que la hipótesis nula será \\(H_0: \\mu=\\mu_0\\). El parámetro alternative puede tomar tres valores: &quot;two.sided&quot;, para contrastes bilaterales, y &quot;less&quot; y &quot;greater&quot;, para contrastes unilaterales. En esta función, y en todas las que explicamos en esta lección, su valor por defecto, que no hace falta especificar, es &quot;two.sided&quot;. El significado de estos valores depende del tipo de test que efectuemos: Si el test es de una sola muestra, &quot;two.sided&quot; representa la hipótesis alternativa \\(H_1: \\mu\\neq \\mu_0\\), &quot;less&quot; corresponde a \\(H_1: \\mu&lt; \\mu_0\\), y &quot;greater&quot; corresponde a \\(H_1: \\mu&gt; \\mu_0\\). Si hemos entrado dos muestras y llamamos \\(\\mu_x\\) y \\(\\mu_y\\) a las medias de las poblaciones de las que hemos extraído las muestras \\(x\\) e \\(y\\), respectivamente, entonces &quot;two.sided&quot; representa la hipótesis alternativa \\(H_1: \\mu_x \\neq \\mu_y\\); &quot;less&quot; indica que la hipótesis alternativa es \\(H_1: \\mu_x&lt; \\mu_y\\); y &quot;greater&quot;, que la hipótesis alternativa es \\(H_1: \\mu_x&gt; \\mu_y\\). El valor del parámetro conf.level es el nivel de confianza \\(1-\\alpha\\). En esta función, y en todas las que explicamos en esta lección, su valor por defecto, que no es necesario especificar, es 0.95, que corresponde a un nivel de confianza del 95%, es decir, a un nivel de significación \\(\\alpha=0.05\\). El parámetro paired solo lo tenemos que especificar si llevamos a cabo un contraste de dos medias. En este caso, con paired=TRUE indicamos que las muestras son emparejadas, y con paired=FALSE (que es su valor por defecto) que son independientes. Si se trata de muestras emparejadas, los vectores x e y tienen que tener la misma longitud, naturalmente. El parámetro var.equal solo lo tenemos que especificar si llevamos a cabo un contraste de dos medias usando muestras independientes, y en este caso sirve para indicar si queremos considerar las dos varianzas poblacionales iguales (igualándolo a TRUE) o diferentes (igualándolo a FALSE, que es su valor por defecto). La función t.test tiene otro parámetro que queremos destacar, que es común a la mayoría de las funciones de estadística inferencial y análisis de datos. Se trata del parámetro na.action, que sirve para especificar qué queremos hacer con los valores NA. Su valor por defecto es na.omit, que elimina las entradas NA de los vectores (o los pares que contengan algún NA, en el caso de muestras emparejadas). Por ahora, esta opción por defecto es la adecuada, por lo que no hace falta usar este parámetro, pero conviene saber que hay alternativas. Las más útiles son: na.fail, que hace que la ejecución pare si hay algún NA en los vectores, y na.pass, que no hace nada con los NA y permite que las operaciones internas de la función sigan su curso y los manejen como les corresponda. Veamos varios ejemplos de uso de esta función. Ejemplo 5.2 Consideremos el siguiente vector de longitud 25: x=c(2.2,2.66,2.74,3.41,2.46,2.96,3.34,2.16,2.46,2.71,2.04,3.74,3.24,3.92,2.38,2.82,2.2, 2.42,2.82,2.84,4.22,3.64,1.77,3.44,1.53) Supongamos que esta muestra ha sido extraída de una población normal. Postulamos que el valor medio \\(\\mu\\) de la población no es 2. Para confirmarlo, vamos realizar el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu=2\\\\ H_{1}:\\mu\\neq 2 \\end{array}\\right. \\] con nivel de significación \\(\\alpha=0.05\\): t.test(x, mu=2, alternative=&quot;two.sided&quot;, conf.level=0.95) ## ## One Sample t-test ## ## data: x ## t = 5.912, df = 24, p-value = 4.23e-06 ## alternative hypothesis: true mean is not equal to 2 ## 95 percent confidence interval: ## 2.52384 3.08576 ## sample estimates: ## mean of x ## 2.8048 (Como los parámetros alternative=&quot;two.sided&quot; y conf.level=0.95 eran los que toma R por defecto, en realidad no hacía falta especificarlos.) Observad la información que obtenemos con esta instrucción: Información sobre la muestra \\(x\\): su media muestral (mean of x) \\(\\overline{x}\\), que vale 2.8048. La hipótesis alternativa (alternative hypothesis), en este caso true mean is not equal to 2: la media verdadera, o poblacional, \\(\\mu\\) es diferente de 2. El valor t que toma el estadístico de contraste, \\(T=\\frac{\\overline{X}-\\mu_0}{\\widetilde{S}_X/\\sqrt{n}}\\), sobre la muestra, en este caso 5.912, y los grados de libertad df (degrees of freedom) de su distribución t de Student cuando la hipótesis nula es verdadera, df =24. El p-valor (p-value) de nuestro test, en este caso p-value = 4.232e-06, es decir, \\(4.232\\times 10^{-6}\\). Un intervalo de confianza del \\((1-\\alpha)\\times 100\\%\\) (en nuestro caso, 95 percent confidence interval) para la \\(\\mu\\): en este ejemplo, [2.523844, 3.085756]. Lo único que no nos dice directamente es si tenemos que rechazar o no la hipótesis nula, pero esto lo deducimos del p-valor: como es más pequeño que el nivel de significación (de hecho, es muy pequeño), podemos rechazar la hipótesis nula, \\(\\mu=2\\), en favor de la alternativa, \\(\\mu \\neq 2\\). Es decir, hay evidencia estadísticamente significativa de que \\(\\mu \\neq 2\\). Otra manera de decidir si rechazamos o no la hipótesis nula es mirar si el valor que contrastamos pertenece al intervalo de confianza del contraste. Puesto que \\(2 \\notin [2.523844, 3.085756]\\), podemos rechazar la hipótesis nula en favor de la alternativa y concluir que \\(\\mu\\neq 2\\). Hagamos ahora el test cambiando la hipótesis alternativa por \\(H_{1}:\\mu&lt; 3\\), es decir, \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu=3\\\\ H_{1}:\\mu&lt; 3 \\end{array}\\right. \\] y tomando como nivel de significación \\(\\alpha=0.1\\): t.test(x, mu=3, alternative=&quot;less&quot;, conf.level=0.9) ## ## One Sample t-test ## ## data: x ## t = -1.434, df = 24, p-value = 0.0822 ## alternative hypothesis: true mean is less than 3 ## 90 percent confidence interval: ## -Inf 2.9842 ## sample estimates: ## mean of x ## 2.8048 En este caso, el p-valor es 0.082, por lo que podemos rechazar la hipótesis nula con un nivel de significación del 10% y concluir, con este nivel de significación (es decir, asumiendo esta probabilidad de equivocarnos), que \\(\\mu&lt;3\\); pero fijaos en que con un nivel de significación del 5% no podríamos rechazar la hipótesis nula. El intervalo de confianza del 90% es ahora \\((-\\infty,2.984]\\) (Inf representa \\(\\infty\\)). Que no contenga el 3 (aunque por muy poco) también indica que podemos rechazar la hipótesis nula \\(\\mu=3\\) en favor de la alternativa \\(\\mu&lt; 3\\) con este nivel de confianza. El p-valor y el intervalo de confianza se pueden obtener directamente, añadiendo a la instrucción t.test los sufijos $p.value o $conf.int, respectivamente. Esperamos que recordéis que en la lección anterior ya usábamos la construcción t.test(...)$conf.int para calcular un intervalo de confianza para la media usando la fórmula basada en la t de Student. Es lo correcto, puesto que el intervalo de confianza de un test t bilateral es el que explicamos entonces y no depende para nada de la hipótesis nula (por eso no especificábamos la \\(\\mu\\), y dejábamos que la función tomase su valor por defecto, 0). Pero ahora podemos calcular dos intervalos de confianza más, correspondientes a los dos tipos de contrastes unilaterales. En ellos toda la “probabilidad de equivocarnos” se concentra a un lado del intervalo de confianza, en lugar de repartirse por igual a ambos lados. t.test(x, mu=2)$p.value ## [1] 4.23159e-06 t.test(x, mu=2)$conf.int ## [1] 2.52384 3.08576 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 t.test(x, mu=2)$conf.int[1] ## [1] 2.52384 t.test(x, mu=2)$conf.int[2] ## [1] 3.08576 t.test(x, mu=3, alternative=&quot;less&quot;, conf.level=0.9)$conf.int ## [1] -Inf 2.9842 ## attr(,&quot;conf.level&quot;) ## [1] 0.9 Podéis consultar los sufijos necesarios para obtener las otras componentes del resultado en la Ayuda de la función. Ejemplo 5.3 Queremos contrastar si el valor medio del nivel de colesterol en una población es de 220 mg/dl o no, a un nivel de significación del 5%. Es decir, si llamamos \\(\\mu\\) a la media de la variable aleatoria “Nivel de colesterol de un individuo de esta población, en mg/dl”, queremos realizar el contraste bilateral \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu=220\\\\ H_{1}:\\mu \\neq 220 \\end{array}\\right. \\] Para ello, hemos tomado una muestra del nivel de colesterol en plasma de 9 individuos de la población. Los datos obtenidos, en mg/dl, son los siguientes: colesterol=c(203,229,215,220,223,233,208,228,209) Suponemos que el nivel de colesterol en plasma sigue una ley normal y que por lo tanto nos podemos fiar del resultado de un test t: t.test(colesterol, mu=220) ## ## One Sample t-test ## ## data: colesterol ## t = -0.3801, df = 8, p-value = 0.714 ## alternative hypothesis: true mean is not equal to 220 ## 95 percent confidence interval: ## 210.577 226.756 ## sample estimates: ## mean of x ## 218.667 El p-valor es 0.714, muy grande y en particular superior a 0.05, por lo tanto no podemos rechazar la hipótesis nula de que el valor medio sea 220 mg/dl. Además, el intervalo de confianza del 95% del contraste es [210.58, 226.76], y contiene holgadamente el valor 220. Más adelante en esta misma sección discutiremos qué podemos hacer si el nivel de colesterol en plasma no sigue una ley aproximadamente normal, en cuyo caso el resultado de este test t no sirve para nada. Ejemplo 5.4 Recordad el dataframe iris, que recoge datos de las flores de 50 ejemplares de cada una de tres especies de iris. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Queremos estudiar si la longitud media \\(\\mu_v\\) de los sépalos de las Iris virginica es mayor que la longitud media \\(\\mu_s\\) de los sépalos de las Iris setosa usando las muestras contenidas en esta tabla de datos. Para ello realizamos el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu_s=\\mu_v\\\\ H_{1}:\\mu_s&lt; \\mu_v \\end{array}\\right. \\] En este caso, se trata de un contraste de dos muestras independientes, y como las muestras son grandes, podemos usar con garantías un test t. Ahora bien, recordad que el test t concreto que hay que usar depende de si las varianzas de las dos variables cuyas medias comparamos son iguales o diferentes. Como no sabemos nada de las varianzas de las longitudes de los sépalos de estas dos especies, y no nos supone apenas esfuerzo realizar los tests, llevaremos a cabo el contraste en los dos casos: varianzas iguales y varianzas diferentes.1 Más adelante, en el Ejemplo 5.11, explicamos cómo contrastar si estas dos varianzas son iguales o diferentes. S=iris[iris$Species==&quot;setosa&quot;,]$Sepal.Length V=iris[iris$Species==&quot;virginica&quot;,]$Sepal.Length El test suponiendo que las dos varianzas son iguales: t.test(S, V, alternative=&quot;less&quot;, var.equal=TRUE) ## ## Two Sample t-test ## ## data: S and V ## t = -15.39, df = 98, p-value &lt;2e-16 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -1.41126 ## sample estimates: ## mean of x mean of y ## 5.006 6.588 El test suponiendo que las dos varianzas son diferentes: t.test(S, V, alternative=&quot;less&quot;, var.equal=FALSE) ## ## Welch Two Sample t-test ## ## data: S and V ## t = -15.39, df = 76.52, p-value &lt;2e-16 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -1.4108 ## sample estimates: ## mean of x mean of y ## 5.006 6.588 En los dos casos el p-valor es prácticamente 0 y por lo tanto podemos rechazar la hipótesis nula en favor de la alternativa: tenemos evidencia estadísticamente muy significativa de que, en promedio, las flores de la especie setosa tienen sépalos más cortos que las de la especie virginica. El intervalo de confianza del 95% para la diferencia de medias \\(\\mu_s-\\mu_v\\) en este contraste es en ambos casos \\((-\\infty, -1.41]\\) y no contiene el 0, que sería el valor de esta diferencia si la hipótesis nula \\(\\mu_s=\\mu_v\\) fuera verdad. Ejemplo 5.5 En un experimento clásico de la primera década del siglo XX, Student quiso comparar el efecto somnífero de dos compuestos químicos, la hiosciamina y la hioscina. La hipótesis a contrastar era que la hioscina es más efectiva que la hiosciamina. Para ello, tomó 10 sujetos y midió su promedio de horas de sueño durante períodos de entre 3 y 9 días bajo tres tratamientos: en condiciones normales, tomando antes de acostarse 0.6 mg de hiosciamina y tomando antes de acostarse 0.6 mg de hioscina. A continuación, calculó para cada sujeto y cada compuesto la diferencia “promedio de horas de sueño tomando el compuesto menos promedio de horas de sueño en condiciones normales”. Las diferencias que obtuvo fueron las siguientes (las podéis encontrar en el dataframe sleep de la instalación básica de R, aunque no lo vamos a usar): Sujeto Hiosciamina Hioscina 1 0.7 1.9 2 -1.6 0.8 3 -0.2 1.1 4 -1.2 0.1 5 -0.1 -0.1 6 3.4 4.4 7 3.7 5.5 8 0.8 1.6 9 0.0 4.6 10 2.0 3.4 Una manera de comparar el efecto en las horas de sueño de estos compuestos es comparando las medias de estas diferencias de promedios de horas de sueño: una diferencia media mayor significa que, de media, el compuesto “ha añadido” más horas de sueño al promedio normal. Digamos \\(\\mu_1\\) a la media de las diferencias individuales del promedio de horas de sueño tomando hiosciamina menos el promedio en condiciones normales y \\(\\mu_2\\) a la media de las diferencias individuales del promedio de horas de sueño tomando hioscina menos el promedio en condiciones normales. Tomaremos como hipótesis nula \\(H_0: \\mu_1= \\mu_2\\) (ambos compuestos tienen el mismo efecto medio sobre las horas de sueño de los individuos) e hipótesis alternativa \\(H_1: \\mu_1&lt;\\mu_2\\) (la hioscina aumenta más las horas de sueño que la hiosciamina). Si podemos rechazar la hipótesis nula en favor de la alternativa, concluiremos que la hioscina tiene un mayor efecto somnífero que la hiosciamina. Observad que se trata de un contraste de dos muestras emparejadas, porque los datos refieren a los mismos 10 pacientes. Vamos a suponer que las diferencias medias en horas de sueño en ambos casos siguen leyes normales (Student así lo hizo) y que, por lo tanto, el resultado de un test t es fiable. Hiosciamina=c(0.7,-1.6,-0.2,-1.2,-0.1,3.4,3.7,0.8,0,2.0) Hioscina=c(1.9,0.8,1.1,0.1,-0.1,4.4,5.5,1.6,4.6,3.4) t.test(Hiosciamina, Hioscina, alternative=&quot;less&quot;, paired=TRUE) ## ## Paired t-test ## ## data: Hiosciamina and Hioscina ## t = -4.062, df = 9, p-value = 0.00142 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -0.866995 ## sample estimates: ## mean of the differences ## -1.58 El p-valor es 0.001, mucho menor que 0.05, y por lo tanto podemos rechazar la hipótesis nula con un nivel de significación del 5%. Observad también que el intervalo de confianza del 95% para la diferencia de medias \\(\\mu_1-\\mu_2\\) es \\((-\\infty,-0.867]\\) y está totalmente a la izquierda del 0. La conclusión es, pues, que efectivamente la hioscina tiene un mayor efecto somnífero que la hiosciamina y que con un 95% de confianza podemos afirmar que añade, de media, como mínimo 52 minutos (-0.867 horas) diarios más de sueño. Ejemplo 5.6 Veamos un ejemplo de aplicación de t.test a una fórmula. Queremos contrastar si es cierto que fumar durante el embarazo está asociado a un peso menor del recién nacido. Si llamamos \\(\\mu_n\\) y \\(\\mu_f\\) al peso medio de un recién nacido de madre no fumadora y fumadora, respectivamente, el contraste que queremos realizar es \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu_n=\\mu_f\\\\ H_{1}:\\mu_n&gt; \\mu_f \\end{array}\\right. \\] Vamos a usar los datos incluidos en la tabla birthwt incluida en el paquete MASS, que recoge información sobre una muestra de madres y sus hijos. library(MASS) str(birthwt) ## &#39;data.frame&#39;: 189 obs. of 10 variables: ## $ low : int 0 0 0 0 0 0 0 0 0 0 ... ## $ age : int 19 33 20 21 18 21 22 17 29 26 ... ## $ lwt : int 182 155 105 108 107 124 118 103 123 113 ... ## $ race : int 2 3 1 1 1 3 1 3 1 1 ... ## $ smoke: int 0 0 1 1 1 0 0 0 1 1 ... ## $ ptl : int 0 0 0 0 0 0 0 0 0 0 ... ## $ ht : int 0 0 0 0 0 0 0 0 0 0 ... ## $ ui : int 1 0 0 1 1 0 0 0 0 0 ... ## $ ftv : int 0 3 1 2 0 0 1 1 1 0 ... ## $ bwt : int 2523 2551 2557 2594 2600 2622 2637 2637 2663 2665 ... En la Ayuda de birthwt nos enteramos de que la variable smoke indica si la madre ha fumado durante el embarazo (1) o no (0), y que la variable bwt da el peso del recién nacido en gramos. Lo primero que haremos será mirar si las muestras de madres fumadoras y no fumadoras contenidas en esta tabla son lo suficientemente grandes como para que el resultado del test t sea fiable. table(birthwt$smoke) ## ## 0 1 ## 115 74 Vemos que sí, que ambas son suficientemente grandes. Para entrar en la instrucción t.test los vectores de pesos de hijos de fumadoras y no fumadoras, usaremos la fórmula bwt~smoke especificando que data=birthwt. Fijaos en que los valores de smoke son 0 y 1, y que R los considera ordenados en este orden (basta ver el resultado de la función table anterior). Por consiguiente, bwt~smoke representa, en este orden, el vector de pesos de recién nacidos de madres no fumadoras (smoke=0) y el vector de pesos de recién nacidos de madres fumadoras (smoke=1). Como la hipótesis alternativa es \\(\\mu_n&gt;\\mu_f\\), deberemos especificar en la función t.test que alternative=&quot;greater&quot;. Es un contraste de muestras independientes, y por lo tanto el procedimiento para llevarlo a cabo depende de la igualdad o no de las varianzas de los pesos de los recién nacidos en los dos grupos de madres. Como en el Ejemplo 5.4, vamos a llevar a cabo el test t suponiendo que estas varianzas son iguales y que son diferentes, y cruzaremos los dedos para que la conclusión sea la misma. Otra posibilidad es contrastar antes la igualdad de las varianzas. t.test(bwt~smoke, data=birthwt, alternative=&quot;greater&quot;, paired=FALSE, var.equal=TRUE) ## ## Two Sample t-test ## ## data: bwt by smoke ## t = 2.653, df = 187, p-value = 0.00433 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 106.953 Inf ## sample estimates: ## mean in group 0 mean in group 1 ## 3055.70 2771.92 t.test(bwt~smoke, data=birthwt, alternative=&quot;greater&quot;, paired=FALSE, var.equal=FALSE) ## ## Welch Two Sample t-test ## ## data: bwt by smoke ## t = 2.73, df = 170.1, p-value = 0.0035 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 111.855 Inf ## sample estimates: ## mean in group 0 mean in group 1 ## 3055.70 2771.92 En ambos casos hemos obtenido un p-valor un orden de magnitud inferior a 0.05, lo que nos permite concluir que, en efecto, los hijos de las madres no fumadoras pesan más al nacer que los de las fumadoras. En vez de especificar los vectores de pesos con bwt~smoke,data=birthwt, hubiéramos podido usar birthwt$bwt~birthwt$smoke. Por ejemplo: t.test(birthwt$bwt~birthwt$smoke, alternative=&quot;greater&quot;, paired=FALSE, var.equal=TRUE) ## ## Two Sample t-test ## ## data: birthwt$bwt by birthwt$smoke ## t = 2.653, df = 187, p-value = 0.00433 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 106.953 Inf ## sample estimates: ## mean in group 0 mean in group 1 ## 3055.70 2771.92 Tests no paramétricos Cuando comparamos dos medias, o una media con un valor, usando un test t sobre muestras pequeñas, suponemos que las variables poblacionales que han producido las muestras son normales. En la Lección 6 estudiaremos los contrastes que nos permiten aceptar o rechazar que una muestra provenga de una variable aleatoria con una distribución concreta, pero en estos momentos ya tendría que ser claro que nos podemos encontrar con conjuntos de datos para los cuales el supuesto de normalidad de la variable poblacional no esté justificado: por ejemplo, porque sean datos cuantitativos discretos o porque la variable sea claramente muy asimétrica (por citar una, la duración del embarazo, con una clara cola a la izquierda: hay un número no despreciable de partos prematuros, pero ningún embarazo dura 11 meses). En las situaciones en las que no estamos seguros de que las variables poblacionales satisfagan aproximadamente las hipótesis de los teoremas que nos garantizan la fiabilidad de las conclusiones de un contraste, por ejemplo de un test t, una salida razonable es usar un test no paramétrico alternativo. En el caso de los contrastes de medias, los tests no paramétricos para comparar medias en realidad lo que comparan son las medianas. Los más populares son los siguientes: El test de signos, que permite contrastar si la mediana de una variable aleatoria cualquiera (incluso ordinal) es un valor dado \\(M_0\\) estudiando la distribución de los signos de las diferencias entre este valor y los de una muestra (si la mediana fuera \\(M_0\\), los números de diferencias positivas y negativas en muestras aleatorias seguirían distribuciones binomiales con \\(p=0.5\\)). En R está implementado en la función SIGN.test del paquete BSDA. Su sintaxis es similar a la de t.test para una muestra, cambiando el parámetro mu, que en t.test sirve para especificar el valor de la media que contrastamos, por md, que en SIGN.test sirve para especificar el valor de la mediana que contrastamos. Esta función también se puede aplicar a dos muestras emparejadas: en este caso, la hipótesis nula del contraste que realiza es que “la mediana de las diferencias de las dos variables es 0”. El test de Wilcoxon para comparar la media de una variable continua simétrica con un valor dado o las medias de dos variables continuas cuya diferencia sea simétrica por medio de muestras emparejadas. Más en general, se puede usar para comparar la mediana de una variable continua con un valor dado o para comparar la mediana de la diferencia de dos variables continuas (medidas sobre muestras emparejadas) con 0. Observad que cuando las variables en juego son simétricas, las medianas coinciden con las medias y el contraste de medianas es también un contraste de medias. En R está implementado en la función wilcox.test y su sintaxis es la misma que la de t.test para una muestra o para dos muestras emparejadas (en este último caso, hay que especificar paired=TRUE). El test de Mann-Whitney para comparar las medianas de dos variables aleatorias por medio de muestras independientes. En R también está implementado en la función wilcox.test y su sintaxis es la misma que la de t.test para dos muestras independientes (especificando paired=FALSE), salvo que aquí no hay que especificar si las varianzas son iguales o diferentes, puesto que esto no se usa en este test. Ejemplo 5.7 Si los niveles de colesterol no siguen una distribución normal, el test t realizado en el Ejemplo 5.3 no sirve para nada. Una posibilidad es entonces no contrastar si el nivel medio de colesterol es 220, sino si el nivel mediano es 220. Para ello vamos realizar un test de signos. Los parámetros alternative=&quot;two.sided&quot; y conf.level=0.95 son los que usa la función SIGN.test por defecto, así que no haría falta especificarlos; los incluimos para que los veáis. library(BSDA) SIGN.test(colesterol, md=220, alternative=&quot;two.sided&quot;, conf.level=0.95) ## ## One-sample Sign-Test ## ## data: colesterol ## s = 4, p-value = 1 ## alternative hypothesis: true median is not equal to 220 ## 95 percent confidence interval: ## 208.078 228.922 ## sample estimates: ## median of x ## 220 ## ## Achieved and Interpolated Confidence Intervals: ## ## Conf.Level L.E.pt U.E.pt ## Lower Achieved CI 0.8203 209.000 228.000 ## Interpolated CI 0.9500 208.078 228.922 ## Upper Achieved CI 0.9609 208.000 229.000 Observad que la salida de la función es muy similar a la de t.test (salvo por los últimos intervalos de confianza, que no vamos a explicar). El p-valor ha dado directamente 1 y el intervalo de confianza al 95% para la mediana ha dado [208.1, 228.9]: por lo tanto, no podemos rechazar que la mediana del nivel de colesterol en la población de la que hemos extraído la muestra sea 220. También podríamos usar el test de Wilcoxon para realizar este contraste de una mediana: wilcox.test(colesterol, mu=220, alternative=&quot;two.sided&quot;,conf.level=0.95) ## Warning in wilcox.test.default(colesterol, mu = 220, alternative = ## &quot;two.sided&quot;, : cannot compute exact p-value with zeroes ## ## Wilcoxon signed rank test with continuity correction ## ## data: colesterol ## V = 15, p-value = 0.726 ## alternative hypothesis: true location is not equal to 220 El p-valor es 0.726, la conclusión es la misma. El mensaje de advertencia nos avisa de que la muestra ha contenido valores iguales al valor de la mediana contrastado, por lo que el p-valor obtenido no es exacto. Solo os tenéis que preocupar de un mensaje como este si el p-valor fuera muy cercano al nivel de significación deseado, que no es el caso. Ejemplo 5.8 Si las diferencias en promedios de horas de sueño no siguen distribuciones normales, el test t realizado en el Ejemplo 5.5 no sirve para nada. En este caso, vamos a usar un test de Wilcoxon para muestras emparejadas. Este test en realidad contrastará la hipótesis nula de que si para cada individuo calculamos la diferencia entre el aumento promedio de horas de sueño cuando toma hiosciamina y el aumento promedio tomando hioscina, la mediana de la variable aleatoria que define estas diferencias es 0, y como hipótesis alternativa que esta mediana es menor que 0 (y que por lo tanto más de la mitad de las veces es negativa, es decir, que a más de la mitad de la población la hioscina le añade más tiempo promedio de sueño que la hiosciamina). Si las variables “aumento de horas de sueño” en juego son simétricas, estas medianas coinciden con las correspondientes medias y llevamos a cabo el contraste del Ejemplo 5.5. Si no son simétricas, igualmente estamos contrastando si la hioscina es más efectiva que la hiosciamina, solo que planteándolo de otra manera. wilcox.test(Hiosciamina, Hioscina, alternative=&quot;less&quot;, paired=TRUE) ## Warning in wilcox.test.default(Hiosciamina, Hioscina, alternative = ## &quot;less&quot;, : cannot compute exact p-value with ties ## Warning in wilcox.test.default(Hiosciamina, Hioscina, alternative = ## &quot;less&quot;, : cannot compute exact p-value with zeroes ## ## Wilcoxon signed rank test with continuity correction ## ## data: Hiosciamina and Hioscina ## V = 0, p-value = 0.00455 ## alternative hypothesis: true location shift is less than 0 En este caso R nos avisa de nuevo de que el p-valor no es exacto, pero esto no afecta a la conclusión dado que el p-valor es muy pequeño: rechazamos la hipótesis nula en favor de la alternativa y también concluimos con este test no paramétrico que la hioscina tiene un mayor efecto somnífero que la hiosciamina. Ejemplo 5.9 Nos preguntamos si los hijos de madres de 20 años pesan lo mismo al nacer que los de madres de 30 años, o no. Querríamos responder esta pregunta planteándola como un contraste bilateral de los pesos medios de los hijos de madres de 20 y 30 años y usando la muestra recogida en la tabla de datos birthwt del paquete MASS, que contiene la variable age con la edad de las madres. hijos.20=birthwt[birthwt$age==20,&quot;bwt&quot;] hijos.30=birthwt[birthwt$age==30,&quot;bwt&quot;] c(length(hijos.20),length(hijos.30)) ## [1] 18 7 Las muestras no son lo suficientemente grandes como para usar un test t si no estamos seguros de que las variables poblacionales sean normales. Como las muestras son independientes, vamos a usar un test de Mann-Whitney para comparar los pesos medianos. Es decir, en vez de traducir “los hijos de madres de 20 años pesan lo mismo al nacer que los de madres de 30 años” en términos de igualdad de pesos medios, lo traducimos en términos de igualdad de pesos medianos. (En realidad, la hipótesis nula de este test es que “Es igual de probable que un hijo de madre de 20 años pese más que un hijo de madre de 30 años que al revés”, pero no vamos a entrar en este nivel de precisión. Es otra manera de decir que no hay tendencia a que unos pesen más que los otros.) wilcox.test(hijos.20, hijos.30, alternative=&quot;two.sided&quot;,paired=FALSE) ## Warning in wilcox.test.default(hijos.20, hijos.30, alternative = ## &quot;two.sided&quot;, : cannot compute exact p-value with ties ## ## Wilcoxon rank sum test with continuity correction ## ## data: hijos.20 and hijos.30 ## W = 43.5, p-value = 0.25 ## alternative hypothesis: true location shift is not equal to 0 El p-valor es 0.25, por lo que no podemos rechazar que las medianas de los pesos al nacer de los hijos de madres de 20 años y de 30 sean iguales. 5.2 Contrastes para varianzas El test \\(\\chi^2\\) para comparar la varianza \\(\\sigma^2\\) (o la desviación típica \\(\\sigma\\)) de una población normal con un valor dado \\(\\sigma_0^2\\) (o \\(\\sigma_0\\)) usa el estadístico \\[ \\frac{(n-1)\\widetilde{S}_X^2}{\\sigma_0^2} \\] que, si la hipótesis nula \\(\\sigma^2=\\sigma_0^2\\) es verdadera, sigue una distribución \\(\\chi^2_{n-1}\\), de ahí su nombre. Dicho test está convenientemente implementado en la función sigma.test del paquete TeachingDemos. Su sintaxis es la misma que la de la función t.test para una muestra, substituyendo el parámetro mu de t.test por el parámetro sigma (para especificar el valor de la desviación típica que contrastamos, \\(\\sigma_0\\)) o sigmasq (por “sigma al cuadrado”, para especificar el valor de la varianza que contrastamos, \\(\\sigma_0^2\\)). Como siempre, los valores por defecto de alternative y conf.level son &quot;two.sided&quot; y 0.95, respectivamente. La salida de la función es también similar a la de t.test. Veamos un ejemplo. Ejemplo 5.10 Se ha realizado un experimento para estudiar el tiempo \\(X\\) (en minutos) que tarda un lagarto del desierto en llegar a los 45o partiendo de su temperatura normal mientras está a la sombra. Los tiempos obtenidos (en minutos) en una muestra aleatoria de lagartos fueron los siguientes: TL45=c(10.1,12.5,12.2,10.2,12.8,12.1,11.2,11.4,10.7,14.9,13.9,13.3) Supongamos que estos tiempos siguen una ley normal. ¿Aporta este experimento evidencia de que la desviación típica \\(\\sigma\\) de \\(X\\) es inferior a 1.5 minutos? Para responder esta pregunta, hemos de realizar el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:\\sigma= 1.5 \\\\ H_{1}:\\sigma&lt; 1.5 \\end{array}\\right. \\] Para ello, usaremos la función sigma.test aplicada a esta muestra y a sigma=1.5: library(TeachingDemos) sigma.test(TL45, sigma=1.5, alternative=&quot;less&quot;) ## ## One sample Chi-squared test for variance ## ## data: TL45 ## X-squared = 10.69, df = 11, p-value = 0.53 ## alternative hypothesis: true variance is less than 2.25 ## 95 percent confidence interval: ## 0.00000 5.25686 ## sample estimates: ## var of TL45 ## 2.18629 El p-valor que obtenemos es 0.53, muy grande, por lo que no tenemos evidencia que nos permita concluir que \\(\\sigma&lt;1.5\\). ¡Atención! El intervalo de confianza que da la función sigma.test es siempre para la varianza, aunque le entréis el valor de la desviación típica. Así que si queréis un intervalo de confianza para la desviación típica, tenéis que tomar la raíz cuadrada del que os da sigma.test: sqrt(sigma.test(TL45, sigma=1.5, alternative=&quot;less&quot;)$conf.int) ## [1] 0.00000 2.29279 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 El test \\(\\chi^2\\) no se usa mucho en la práctica. En parte, porque realmente es poco interesante ya que suele ser difícil conjeturar la desviación típica a contrastar, y en parte porque su validez depende fuertemente de la hipótesis de que la variable aleatoria poblacional sea normal. En cambio, el contraste de las desviaciones típicas de dos poblaciones sí que es muy utilizado. Por ejemplo, en un contraste de dos medias usando un test t sobre dos muestras independientes, nos puede interesar conocer a priori si las varianzas poblacionales son iguales o diferentes, en lugar de realizar el test bajo ambas suposiciones. Si no las conocemos, ¿cómo podemos saber cuál es el caso? Si las dos variables poblacionales son normales, podemos contrastar la igualdad de las varianzas con el test F, basado en el estadístico \\[ \\frac{\\widetilde{S}_{X_1}^2} {\\widetilde{S}_{X_2}^2} \\] que, si las dos poblaciones son normales y tienen la misma varianza, sigue una distribución F de Fisher-Snedecor. Por desgracia, este test es también muy sensible a la no normalidad de las poblaciones objeto de estudio: a la que una de ellas se aleja un poco de la normalidad, el test deja de dar resultados fiables.2 La función para efectuar este test es var.test y su sintaxis básica es la misma que la de t.test para dos muestras: var.test(x, y, alternative=..., conf.level=...) donde x e y son los dos vectores de datos, que se pueden especificar mediante una fórmula como en el caso de t.test, y el parámetro alternative puede tomar los tres mismos valores que en los tests anteriores: su valor por defecto es, como siempre, &quot;two.sided&quot;, que es el que nos permite contrastar si las varianzas son iguales o diferentes. Ejemplo 5.11 Suponiendo que las longitudes de los sépalos de las flores de las diferentes especies de iris siguen leyes normales, ¿hubiéramos podido considerar a priori iguales las varianzas de las dos muestras en el Ejemplo 5.4? Veamos: S=iris[iris$Species==&quot;setosa&quot;,]$Sepal.Length V=iris[iris$Species==&quot;virginica&quot;,]$Sepal.Length var.test(S,V) ## ## F test to compare two variances ## ## data: S and V ## F = 0.3073, num df = 49, denom df = 49, p-value = 6.37e-05 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.174378 0.541496 ## sample estimates: ## ratio of variances ## 0.307286 El p-valor es \\(6.4\\times 10^{-5}\\), muy pequeño. Por lo tanto, podemos rechazar la hipótesis nula de que las dos varianzas son iguales, en favor de la hipótesis alternativa de que las dos varianzas son diferentes. Así, pues, bastaba realizar solo el t.test con var.equal=FALSE. Puede ser conveniente remarcar aquí que el intervalo de confianza obtenido con var.test es para el cociente de varianzas poblacionales \\(\\sigma^2_x/\\sigma^2_y\\), no para su diferencia. Por lo tanto, para contrastar si las varianzas son iguales o diferentes, hay que mirar si el 1 pertenece o no al intervalo obtenido. En este ejemplo, el intervalo de confianza al 95% ha sido [0.174, 0.541] y no contiene el 1, lo que confirma la evidencia de que las varianzas son diferentes. Ejemplo 5.12 Queremos contrastar si los gatos adultos macho pesan más que los gatos adultos hembra. Para ello usaremos los datos recogidos en el dataframe cats del paquete MASS, que contiene información sobre el peso de una muestra de gatos adultos, separados por su sexo. str(cats) ## &#39;data.frame&#39;: 144 obs. of 3 variables: ## $ Sex: Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Bwt: num 2 2 2 2.1 2.1 2.1 2.1 2.1 2.1 2.1 ... ## $ Hwt: num 7 7.4 9.5 7.2 7.3 7.6 8.1 8.2 8.3 8.5 ... table(cats$Sex) ## ## F M ## 47 97 Consultando la Ayuda de cats nos enteramos de que la variable Bwt contiene el peso de cada gato en kg, y la variable Sex contiene el sexo de cada gato: F para hembra (female) y M para macho (male). Como vemos en la tabla de frecuencias, los números de ejemplares de cada sexo son grandes. Así pues, si llamamos \\(\\mu_m\\) al peso medio de un gato macho adulto y \\(\\mu_h\\) al peso medio de un gato hembra adulto, el contraste que vamos a realizar es \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu_m=\\mu_h\\\\ H_{1}:\\mu_m&gt;\\mu_h \\end{array}\\right. \\] y para ello antes vamos a contrastar si las varianzas de ambas poblaciones son iguales o diferentes, para luego poder aplicar la función t.test con el valor de var.equal adecuado. Vamos a suponer que los pesos en ambos sexos siguen leyes normales. Para que el contraste de las varianzas sea fiable es necesario que esta suposición sea cierta; para el de los pesos medios, no, ya que ambas muestras son grandes. El contraste de la igualdad de varianzas es el siguiente: var.test(Bwt~Sex, data=cats) ## ## F test to compare two variances ## ## data: Bwt by Sex ## F = 0.3435, num df = 46, denom df = 96, p-value = 0.000116 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.212628 0.580348 ## sample estimates: ## ratio of variances ## 0.343501 El p-valor es \\(1.2\\times 10^{-4}\\), y por lo tanto podemos rechazar la hipótesis nula de que las varianzas son iguales y concluir que son diferentes. Así que en el test t las consideraremos diferentes. Recordemos ahora que la hipótesis alternativa que queremos contrastar es \\(H_{1}:\\mu_m&gt;\\mu_h\\). En el factor cats$Sex, la F (hembra) va antes que la M (macho), y, por tanto, si entramos los vectores de pesos mediante Bwt~Sex,data=cats, el primer vector corresponderá a las gatas y el segundo a los gatos. Así pues, la hipótesis alternativa que tenemos que especificar es que la media del primer vector es inferior a la del segundo vector: alternative=&quot;less&quot;. t.test(Bwt~Sex, data=cats, alternative=&quot;less&quot;,var.equal=FALSE) ## ## Welch Two Sample t-test ## ## data: Bwt by Sex ## t = -8.709, df = 136.8, p-value = 4.42e-15 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -0.437666 ## sample estimates: ## mean in group F mean in group M ## 2.35957 2.90000 Como el p-valor es prácticamente 0, podemos concluir que, efectivamente, de media, los gatos adultos pesan más que las gatas adultas. Hemos insistido en que el test F solo es válido si las dos poblaciones cuyas varianzas comparamos son normales. ¿Qué podemos hacer si dudamos de su normalidad? Usar un test no paramétrico que no presuponga esta hipótesis. Hay diversos tests no paramétricos para realizar contrastes bilaterales de dos varianzas. Aquí os recomendamos el test de Fligner-Killeen, implementado en la función fligner.test. Se aplica o bien a una list formada por las dos muestras, o bien a una fórmula que separe un vector numérico en dos muestras por medio de un factor de dos niveles. Ejemplo 5.13 Si queremos contrastar si las varianzas de las longitudes de los sépalos de las flores iris setosa y virginica son iguales o no sin presuponer que siguen leyes normales, podemos usar el test de Fligner-Killeen de la manera siguiente: fligner.test(list(S,V)) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: list(S, V) ## Fligner-Killeen:med chi-squared = 9.984, df = 1, p-value = 0.00158 El p-valor es 0.0016, por lo que podemos concluir que las varianzas son diferentes. Ejemplo 5.14 Si queremos contrastar si las varianzas de los pesos de los gatos y las gatas adultos son iguales o no sin presuponer que dichos pesos tienen distribuciones normales, podemos usar el test de Fligner-Killeen de la manera siguiente: fligner.test(Bwt~Sex, data=cats) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: Bwt by Sex ## Fligner-Killeen:med chi-squared = 16.91, df = 1, p-value = ## 3.92e-05 El p-valor es \\(4\\times 10^{-5}\\), por lo que podemos concluir que las varianzas son diferentes. 5.3 Contrastes para proporciones Cuando tenemos que efectuar un contraste sobre una probabilidad de éxito \\(p\\) de una variable Bernoulli, podemos emplear el test binomial exacto. Este test se basa en que, si la hipótesis nula \\(H_0: p=p_0\\) es verdadera, los números de éxitos en muestras aleatorias simples de tamaño \\(n\\) de la variable poblacional, que será de tipo \\(Be(p_0)\\), siguen una ley binomial \\(B(n,p_0)\\). Este test está implementado en la función binom.test, cuya sintaxis es binom.test(x, n, p=..., alternative=..., conf.level=...) donde x y n son números naturales: el número de éxitos y el tamaño de la muestra. p es la probabilidad de éxito que queremos contrastar. El significado de alternative y conf.level, y sus posibles valores, son los usuales. Fijaos en particular que binom.test no se aplica directamente al vector de una muestra, sino a su número de éxitos y a su longitud. Si la muestra es un vector binario X, el número de éxitos será sum(X) y la longitud length(X). Nota. El intervalo de confianza para la \\(p\\) que da binom.test en un contraste bilateral es el de Clopper-Pearson. Ejemplo 5.15 Recordemos el Ejemplo 5.1, donde, en una serie de 20 lanzamientos de una moneda, había obtenido 15 caras. ¿Podemos sospechar que la moneda está trucada a favor de cara? Como comentamos en ese ejemplo, si llamamos \\(p\\) a la probabilidad de obtener cara con esta moneda, el contraste que queremos realizar es \\[ \\left\\{\\begin{array}{l} H_{0}:p=0.5\\\\ H_{1}:p&gt; 0.5 \\end{array}\\right. \\] Usaremos la función binom.test: binom.test(15,20, p=0.5, alternative=&quot;greater&quot;) ## ## Exact binomial test ## ## data: 15 and 20 ## number of successes = 15, number of trials = 20, p-value = 0.0207 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.544418 1.000000 ## sample estimates: ## probability of success ## 0.75 El p-valor del test es 0.0207 y el intervalo de confianza que nos da este test para la \\(p\\) es [0.5444, 1]. Ambos valores coinciden con los dados en el ejemplo original Cuando la muestra es grande, pongamos de 40 o más sujetos, podemos usar también el test aproximado, basado en la aproximación de la distribución de la proporción muestral por medio de una normal dada por el Teorema Central del Límite. En R está implementado en la función prop.test, que además también sirve para contrastar dos proporciones por medio de muestras independientes grandes. Su sintaxis es prop.test(x, n, p =..., alternative=..., conf.level=...) donde: x puede ser dos cosas: Un número natural: en este caso, R entiende que es el número de éxitos en una muestra. Un vector de dos números naturales: en este caso, R entiende que es un contraste de dos proporciones y que estos son los números de éxitos en las muestras. Cuando trabajamos con una sola muestra, n es su tamaño. Cuando estamos trabajando con dos muestras, n es el vector de dos entradas de sus tamaños. Cuando trabajamos con una sola muestra, p es la proporción poblacional que contrastamos. En el caso de un contraste de dos muestras, no hay que especificarlo. El significado de alternative y conf.level, y sus posibles valores, son los usuales. Veamos algunos ejemplos más. Ejemplo 5.16 Queremos contrastar si la proporción de estudiantes zurdos en la UIB es diferente del 10%, el porcentaje estimado de zurdos en España. Es decir, si llamamos \\(p\\) a la proporción de estudiantes zurdos en la UIB, queremos realizar el contraste \\[ \\left\\{ \\begin{array}{l} H_0:p=0.1\\\\ H_1:p\\neq 0.1 \\end{array} \\right. \\] Para ello, tomamos una muestra de 50 estudiantes de la UIB encuestados al azar y resulta que 3 son zurdos. Vamos a suponer que forman una muestra aleatoria simple. Como la muestra es grande (\\(n=50\\)) usaremos la función prop.test. prop.test(3, 50, p=0.1) ## ## 1-sample proportions test with continuity correction ## ## data: 3 out of 50, null probability 0.1 ## X-squared = 0.5, df = 1, p-value = 0.48 ## alternative hypothesis: true p is not equal to 0.1 ## 95 percent confidence interval: ## 0.0156246 0.1754187 ## sample estimates: ## p ## 0.06 El p-valor obtenido en el test es 0.48, muy superior a 0.05. Por lo tanto, no podemos rechazar que un 10% de los estudiantes de la UIB sean zurdos. El intervalo de confianza del 95% para \\(p\\) que hemos obtenido es [0.016, 0.175]. La conclusión usando el test binomial hubiera sido la misma: binom.test(3, 50, p=0.1) ## ## Exact binomial test ## ## data: 3 and 50 ## number of successes = 3, number of trials = 50, p-value = 0.48 ## alternative hypothesis: true probability of success is not equal to 0.1 ## 95 percent confidence interval: ## 0.0125486 0.1654819 ## sample estimates: ## probability of success ## 0.06 Ya que estamos, comprobemos que el intervalo de confianza del 95% obtenido con binom.test es efectivamente el de Clopper-Pearson: library(epitools) binom.exact(3,50) ## x n proportion lower upper conf.level ## 1 3 50 0.06 0.0125486 0.165482 0.95 Nota. El intervalo de confianza que se obtiene con prop.test en un contraste bilateral es el de Wilson modificado mediante una corrección de continuidad, un ajuste que se recomienda realizar cuando una distribución discreta (en este caso, una binomial) se aproxima mediante una distribución continua. En concreto, la fórmula que utiliza prop.test es la que se explica en esta entrada de la Wikipedia. Podéis indicar que R no efectue esta corrección de continuidad con el parámetro correct=FALSE: prop.test(3, 50, p=0.1, correct=FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 3 out of 50, null probability 0.1 ## X-squared = 0.8889, df = 1, p-value = 0.346 ## alternative hypothesis: true p is not equal to 0.1 ## 95 percent confidence interval: ## 0.020615 0.162171 ## sample estimates: ## p ## 0.06 (Observad que sin el correct=FALSE, el encabezamiento del resultado del prop.test es 1-sample proportions test with continuity correction, mientras que con correct=FALSE es 1-sample proportions test without continuity correction). Comprobemos que el intervalo de confianza del 95% que hemos obtenido ahora sí que es el de Wilson: binom.wilson(3, 50) ## x n proportion lower upper conf.level ## 1 3 50 0.06 0.020615 0.162171 0.95 Ejemplo 5.17 Una empresa que fabrica trampas para cucarachas ha producido una nueva versión de su trampa más popular y afirma que la nueva trampa mata más cucarachas que la vieja. Hemos llevado a cabo un experimento para comprobarlo. Hemos situado dos trampas en dos habitaciones. En cada habitación hemos soltado 60 cucarachas. La versión vieja de la trampa ha matado 40 y la nueva, 48. ¿Es suficiente evidencia de que la nueva trampa es más efectiva que la vieja? Digamos \\(p_v\\) y \\(p_n\\) a las proporciones de cucarachas que matan la trampa vieja y la trampa nueva, respectivamente. La hipótesis nula será que las trampas de los dos tipos son igual de efectivas, \\(H_0:p_v=p_n\\), y la hipótesis alternativa que las trampas nuevas son más efectivas que las viejas, \\(H_1:p_v&lt;p_n\\). Los tamaños de las muestras nos permiten usar la función prop.test. prop.test(c(40,48),c(60,60),alternative=&quot;less&quot;) ## ## 2-sample test for equality of proportions with continuity ## correction ## ## data: c(40, 48) out of c(60, 60) ## X-squared = 2.088, df = 1, p-value = 0.0742 ## alternative hypothesis: less ## 95 percent confidence interval: ## -1.0000000 0.0146167 ## sample estimates: ## prop 1 prop 2 ## 0.666667 0.800000 El p-valor es 0.074, y el intervalo de confianza que nos da el test, [-1, 0.015], es para la diferencia de proporciones \\(p_v-p_n\\) y contiene el 0, aunque por poco. En resumen, a un nivel de significación de 0.05 no encontramos evidencia de que la trampa nueva sea mejor que la vieja, pero el resultado no es del todo concluyente y convendría llevar a cabo otro experimento con más cucarachas para aumentar la potencia (cf. Ejemplo 5.23 en la próxima sección). La función prop.test solo sirve para contrastar dos proporciones cuando las dos muestras son independientes y grandes. Un test que se puede usar siempre para contrastar dos proporciones usando muestras independientes es el test exacto de Fisher, que usa una distribución hipergeométrica. Supongamos que evaluamos una característica dicotómica (es decir, que solo puede tomar dos valores y por tanto define distribuciones de Bernoulli) sobre dos poblaciones y tomamos dos muestras independientes, una de cada población. Resumimos los resultados en una tabla como la que sigue: \\[ \\begin{array}{r|c} &amp; \\quad\\mbox{Población}\\quad \\\\ \\mbox{Característica} &amp;\\quad 1 \\qquad 2\\quad \\\\\\hline \\mbox{Sí} &amp;\\quad a \\qquad b\\quad \\\\ \\mbox{No} &amp;\\quad c \\qquad d\\quad \\end{array} \\] Llamemos \\(p_{1}\\) a la proporción de individuos con la característica bajo estudio en la población 1 y \\(p_{2}\\) a su proporción en la población 2. Queremos contrastar la hipótesis nula \\(H_{0}:p_1=p_2\\) contra alguna hipótesis alternativa. Por ejemplo, en el experimento de las trampas para cucarachas, las poblaciones vendrían definidas por el tipo de trampa, y la característica que tendríamos en cuenta sería si la cucaracha ha muerto o no, lo que nos daría la tabla siguiente: \\[ \\begin{array}{r|c} &amp; \\qquad\\mbox{Trampas}\\quad \\\\ &amp;\\quad \\mbox{Viejas}\\qquad \\mbox{Nuevas}\\\\\\hline \\mbox{Muertas} &amp;\\qquad 40 \\qquad\\qquad 48\\quad \\\\ \\mbox{Vivas} &amp;\\qquad 20 \\qquad\\qquad 12\\quad \\end{array} \\] El test exacto de Fisher está implementado en la función fisher.test. Su sintaxis es fisher.test(x, alternative=..., conf.level=...) donde x es la matriz \\(\\left(\\begin{array}{cc} a &amp; b\\\\ c &amp; d\\end{array}\\right)\\), en la que los números de éxitos van en la primera fila y los de fracasos en la segunda, y las poblaciones se ordenan por columnas. El significado de alternative y conf.level, y sus posibles valores, son los usuales. Así, en el ejemplo de las trampas para cucarachas, entraríamos: Datos=rbind(c(40,48),c(20,12)) Datos ## [,1] [,2] ## [1,] 40 48 ## [2,] 20 12 fisher.test(Datos, alternative=&quot;less&quot;) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: Datos ## p-value = 0.0739 ## alternative hypothesis: true odds ratio is less than 1 ## 95 percent confidence interval: ## 0.00000 1.08414 ## sample estimates: ## odds ratio ## 0.502909 y obtenemos de nuevo un p-valor cercano a 0.074. Hay que ir con cuidado con la interpretación del intervalo de confianza que da esta función: no es ni para la diferencia de las proporciones ni para su cociente, sino para su odds ratio: el cociente \\[ \\Big({\\frac{p_v}{1-p_v}}\\Big)\\Big/\\Big({\\frac{p_n}{1-p_n}}\\Big). \\] Recordad que si la probabilidad de un suceso \\(A\\) es \\(P(A)\\), sus odds son el cociente \\[ \\mbox{Odds}(A)=\\frac{P(A)}{1-P(A)} \\] que mide cuántas veces es más probable \\(A\\) que su contrario. Las odds son una función creciente de la probabilidad, y por lo tanto \\[ \\mbox{Odds}(A)&lt;\\mbox{Odds}(B)\\Longleftrightarrow P(A)&lt;P(B). \\] Esto permite comparar odds en vez de probabilidades, con la misma conclusión. Por ejemplo, en nuestro caso, como el intervalo de confianza para la odds ratio va de 0 a 1.084, en particular contiene el 1, por lo que no podemos rechazar que \\[ \\Big({\\frac{p_v}{1-p_v}}\\Big)\\Big/\\Big({\\frac{p_n}{1-p_n}}\\Big)=1, \\] es decir, no podemos rechazar que \\[ \\frac{p_v}{1-p_v}=\\frac{p_n}{1-p_n} \\] y esto es equivalente a \\(p_v=p_n\\). Si, por ejemplo, el intervalo de confianza hubiera ido de 0 a 0.8, entonces la conclusión a este nivel de confianza hubiera sido que \\[ \\Big({\\frac{p_v}{1-p_v}}\\Big)\\Big/\\Big({\\frac{p_n}{1-p_n}}\\Big)&lt;1 \\] es decir, que \\[ \\frac{p_v}{1-p_v}&lt;\\frac{p_n}{1-p_n} \\] y esto es equivalente a \\(p_v&lt;p_n\\). Ejemplo 5.18 Para determinar si el Síndrome de Muerte Súbita del Recién Nacido (SIDS, por sus siglas en inglés) tiene algún componente genético, se estudiaron parejas de gemelos y mellizos en las que se dio algún caso de SIDS. Sean \\(p_1\\) la proporción de casos con exactamente una muerte por SIDS entre las parejas de gemelos con algún caso de SIDS, y \\(p_2\\) la proporción de casos con exactamente una muerte por SIDS entre las parejas de mellizos con algún caso de SIDS. La hipótesis de trabajo es que si el SIDS tiene componente genético, será más probable que un gemelo de un muerto por SIDS también lo sufra que si solo es mellizo, y por lo tanto que en las parejas de gemelos ha de ser más raro que haya exactamente un caso de SIDS que en las parejas de mellizos. Es decir, que \\(p_1&lt;p_2\\). Así pues, queremos realizar el contraste \\[ \\left\\{\\begin{array}{l} H_0:p_1=p_2\\\\ H_1:p_1&lt; p_2 \\end{array}\\right. \\] En un estudio se obtuvieron los datos siguientes: \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Tipo de gemelos}\\ \\\\ \\mbox{Casos de SIDS} &amp;\\ \\mbox{Gemelos}\\qquad \\mbox{Mellizos}\\\\\\hline \\mbox{Uno} &amp; \\quad\\ 23 \\qquad\\quad\\quad\\quad \\ 35\\quad \\\\ \\mbox{Dos} &amp; \\quad\\ 1 \\quad\\quad\\qquad\\quad \\ \\hphantom{3} 2 \\quad \\end{array} \\] Vamos a realizar el contraste. Observad que damos la tabla de manera que \\(p_1\\) es la proporción de parejas con un solo caso de SIDS entre las de la población 1 (gemelos), y \\(p_{2}\\) es la proporción de parejas con un solo caso de SIDS entre las de la población 2 (mellizos). Por tanto hemos de aplicar fisher.test a esta matriz y \\(p_1&lt;p_2\\) corresponderá a alternative=&quot;less&quot;. Datos=rbind(c(23,35),c(1,2)) Datos ## [,1] [,2] ## [1,] 23 35 ## [2,] 1 2 fisher.test(Datos, alternative=&quot;less&quot;) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: Datos ## p-value = 0.784 ## alternative hypothesis: true odds ratio is less than 1 ## 95 percent confidence interval: ## 0.0000 39.7395 ## sample estimates: ## odds ratio ## 1.30859 El p-valor es 0.784, muy grande, por lo que no obtenemos evidencia de componente genético en el SIDS. Supongamos ahora que queremos comparar dos proporciones usando muestras emparejadas. Por ejemplo, supongamos que evaluamos dos características dicotómicas sobre una misma muestra de \\(n\\) sujetos. Resumimos los resultados obtenidos en la tabla siguiente: \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Característica 1}\\ \\\\ \\mbox{Característica 2} &amp;\\ \\ \\, \\mbox{Sí}\\qquad \\mbox{No}\\\\\\hline \\mbox{Sí} &amp; \\quad\\ \\ a \\qquad \\ \\ \\, b\\quad \\\\ \\mbox{No} &amp; \\quad\\ \\ c \\qquad \\ \\ \\, d\\quad \\end{array} \\] donde \\(a+b+c+d=n\\). Esta tabla quiere decir, naturalmente, que \\(a\\) sujetos de la muestra tuvieron la característica 1 y la característica 2, que \\(b\\) sujetos de la muestra tuvieron la característica 2 pero no tuvieron la característica 1, etc. Vamos a llamar \\(p_{1}\\) a la proporción poblacional de individuos con la característica 1, y \\(p_{2}\\) a la proporción poblacional de individuos con la característica 2. Queremos contrastar la hipótesis nula \\(H_{0}:p_1=p_2\\) contra alguna hipótesis alternativa. En este caso, no pueden usarse las funciones prop.test o fisher.test. Tenemos dos soluciones posibles. La primera nos permite realizar el contraste bilateral \\[ \\left\\{\\begin{array}{l} H_{0}:p_1=p_2\\\\ H_{1}:p_1\\neq p_2 \\end{array}\\right. \\] cuando \\(n\\) es grande (digamos que \\(n{\\geqslant}100\\)) y el número \\(b+c\\) de casos discordantes (en los que una característica da Sí y la otra da No) es razonablemente grande, pongamos \\({\\geqslant}20\\). En esta situación podemos usar el test de McNemar, que se lleva a cabo en R con la instrucción mcnemar.test. Su sintaxis básica es mcnemar.test(X) donde X es la matriz \\(\\left(\\begin{array}{cc} a &amp; b\\\\ c&amp; d \\end{array}\\right)\\) que corresponde a la tabla anterior. Ejemplo 5.19 Para comparar la efectividad de dos tratamientos del asma, se escogieron 200 pacientes con asma severo, y a cada uno se le trató durante un mes con el tratamiento A o el tratamiento B, decidiéndose cada tratamiento al azar; tras esta fase de tratamiento, se les dejó sin tratamiento durante un mes, y a continuación a cada uno se le trató durante un mes con el otro tratamiento (B si antes había recibido A, A si antes había recibido B). Se anotó si durante cada periodo de tratamiento cada enfermo visitó o no el servicio de urgencias por dificultades respiratorias. Los resultados del experimento se resumen en la tabla siguiente (“Sí” significa que sí que acudió a urgencias por dificultades respiratorias): \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Tratamiento A}\\ \\\\ \\mbox{Trat. B} &amp;\\quad \\ \\mbox{ Sí}\\qquad\\quad \\mbox{No}\\quad \\\\\\hline \\mbox{Sí} &amp; \\quad \\ 71 \\qquad\\quad 48\\quad \\\\ \\mbox{No} &amp; \\quad \\ 30 \\qquad\\quad 51\\quad \\end{array} \\] Queremos determinar si hay diferencia en la efectividad de los dos tratamientos. Para ello, entramos la tabla anterior en una matriz y le aplicamos la función mcnemar.test: Datos=matrix(c(71,48,30,51),nrow=2,byrow=TRUE) Datos ## [,1] [,2] ## [1,] 71 48 ## [2,] 30 51 mcnemar.test(Datos) ## ## McNemar&#39;s Chi-squared test with continuity correction ## ## data: Datos ## McNemar&#39;s chi-squared = 3.705, df = 1, p-value = 0.0542 El p-valor del test es 0.054, ligeramente superior a 0.05, por lo tanto no permite concluir a un nivel de significación del 5% que haya evidencia de que la efectividad de los dos tratamientos sea diferente, pero sería conveniente llevar a cabo un estudio más amplio. Otra posibilidad para realizar un contraste de dos proporciones usando muestras emparejadas, que no requiere de ninguna hipótesis sobre los tamaños de las muestras, es usar de manera adecuada la función binom.test. Para explicar este método, consideremos la tabla siguiente, donde ahora damos las probabilidades poblacionales de las cuatro combinaciones de resultados: \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Característica 1}\\ \\\\ \\mbox{Característica 2} &amp;\\quad \\ \\!\\mbox{Sí}\\qquad\\quad\\, \\mbox{No}\\quad \\\\\\hline \\mbox{Sí} &amp; \\quad \\ p_{11} \\qquad\\quad p_{01}\\quad \\\\ \\mbox{No} &amp; \\quad \\ p_{10} \\qquad\\quad p_{00}\\quad \\end{array} \\] De esta manera \\(p_1=p_{11}+p_{10}\\) y \\(p_2=p_{11}+p_{01}\\). Entonces, \\(p_1=p_2\\) es equivalente a \\(p_{10}=p_{01}\\) y cualquier hipótesis alternativa se traduce en la misma desigualdad, pero para \\(p_{10}\\) y \\(p_{01}\\): \\(p_1\\neq p_2\\) es equivalente a \\(p_{10}\\neq p_{01}\\); \\(p_1&lt; p_2\\) es equivalente a \\(p_{10}&lt; p_{01}\\); y \\(p_1&gt; p_2\\) es equivalente a \\(p_{10}&gt; p_{01}\\). Por lo tanto podemos traducir el contraste sobre \\(p_1\\) y \\(p_2\\) al mismo contraste sobre \\(p_{10}\\) y \\(p_{01}\\). La gracia ahora está en que si la hipótesis nula \\(p_{10}=p_{01}\\) es cierta, entonces, en el total de casos discordantes, el número de sujetos en los que la característica 1 da Sí y la característica 2 da No sigue una ley binomial con \\(p=0.5\\). Por lo tanto, podemos efectuar el contraste usando un test binomial exacto tomando como muestra los casos discordantes de nuestra muestra, de tamaño \\(b+c\\), como éxitos los sujetos que han dado Sí en la característica 1 y No en la característica 2, de tamaño \\(c\\), con proporción a contrastar \\(p=0.5\\) y con hipótesis alternativa la que corresponda. La ventaja de este test es que su validez no requiere de ninguna hipótesis sobre los tamaños de las muestras. El inconveniente es que el intervalo de confianza que nos dará será para \\(p_{10}/(p_{10}+p_{01})\\), y no permite obtener un intervalo de confianza para la diferencia o el cociente de las probabilidades \\(p_1\\) y \\(p_2\\) de interés. Ejemplo 5.20 Usemos el test binomial para llevar a cabo el contraste bilateral del Ejemplo 5.19. Habíamos obtenido 30+48=78 casos discordantes, de los que 48 eran casos en los que el tratamiento A había dado Sí y el tratamiento B había dado No. binom.test(48, 78, p=0.5) ## ## Exact binomial test ## ## data: 48 and 78 ## number of successes = 48, number of trials = 78, p-value = 0.0535 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.498331 0.723398 ## sample estimates: ## probability of success ## 0.615385 Obtenemos de nuevo un p-valor en la zona de penumbra, ligeramente superior a 0.05. Ejemplo 5.21 Para determinar si un test casero de VIH basado en un frotis bucal da más positivos (que seguramente serán falsos positivos) que el test de VIH de referencia, basado en una analítica de sangre que detecta la presencia del virus, se tomó una muestra aleatoria de 241 individuos en situación de riesgo, y a todos se les realizaron ambos tests. Los resultados se resumen en la tabla siguiente: \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Test estándar}\\ \\\\ \\mbox{Test casero} &amp;\\quad \\ \\mbox{ Positivo}\\qquad\\quad \\quad \\mbox{Negativo}\\quad \\\\\\hline \\mbox{Positivo} &amp; \\quad\\ \\ 72 \\qquad\\qquad\\qquad\\ 10 \\quad \\\\ \\mbox{Negativo} &amp; \\quad\\quad 2 \\qquad\\qquad\\quad\\quad \\ 157 \\quad \\end{array} \\] Si llamamos \\(p_{c}\\) a la probabilidad de que el test casero dé positivo y \\(p_{e}\\) a la probabilidad de que el test estándar dé positivo, queremos realizar el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:p_{e}=p_{c}\\\\ H_{1}:p_{e}&lt; p_{c} \\end{array}\\right. \\] El número de casos discordantes es pequeño (10+2=12) y además el test es unilateral, así que usaremos el test binomial. Como queremos realizar un contraste unilateral, hay que pensar en cómo traducir la hipótesis alternativa en términos de una hipótesis sobre la probabilidad \\(p=p_{10}/(p_{10}+p_{01})\\) de que un caso discordante tenga la característica 1 (la de las columnas). Veamos, \\(p_{e}&lt; p_{c}\\) significa que la característica de las columnas es menos probable que la de las filas, por tanto se ha de traducir en que la probabilidad de tener la característica de las columnas y no la de las filas es más pequeña que la probabilidad de tener la característica de las filas y no la de las columnas, es decir, en que \\(p&lt;0.5\\): hemos de usar alternative=less. binom.test(2, 12, alternative=&quot;less&quot;, p=0.5) ## ## Exact binomial test ## ## data: 2 and 12 ## number of successes = 2, number of trials = 12, p-value = 0.0193 ## alternative hypothesis: true probability of success is less than 0.5 ## 95 percent confidence interval: ## 0.000000 0.438105 ## sample estimates: ## probability of success ## 0.166667 Obtenemos evidencia significativa de que, efectivamente, el test casero da positivo con mayor frecuencia que el de referencia. 5.4 Cálculo de la potencia de un contraste Recordemos que la potencia de un contraste de hipótesis es la probabilidad de no cometer un error de tipo II, es decir, la probabilidad de aceptar la hipótesis alternativa si es verdadera. Usualmente, la probabilidad de cometer un error de tipo II se denota por \\(\\beta\\), y por lo tanto la potencia es \\(1-\\beta\\). La potencia de un contraste está relacionada con lo que se llama la magnitud del efecto (effect size). En un contraste, el efecto es la diferencia entre el valor estimado del parámetro a partir de la muestra usada y el valor que se da a dicho parámetro como hipótesis nula: por ejemplo, en el contraste de una media, la diferencia entre la media muestral \\(\\overline{x}\\) y el valor contrastado \\(\\mu_0\\); o, en el contraste de dos medias, la diferencia entre las dos medias muestrales. Se rechaza entonces la hipótesis nula si el efecto observado es tan grande que es muy improbable cuando la hipótesis nula es verdadera. Pero recordad que, en realidad, no se tiene en cuenta si el efecto observado ha sido grande o no por si mismo, solo si es estadísticamente significativo, es decir, si es improbable cuando la hipótesis nula es verdadera. Entonces, sin entrar en detalle, digamos que la magnitud del efecto es una medida estadística específica del tamaño del efecto observado respecto de su valor esperado si la hipótesis nula es verdadera. La fórmula para calcular la magnitud del efecto depende del contraste y del estadístico usado. Para cada tipo de test se han consensuado unos valores de la magnitud del efecto considerados como “pequeño”, “medio” y “grande”. Estos valores se obtienen con R con la función cohen.ES del paquete pwr. Su sintaxis básica es cohen.ES(test=..., size=...) donde: el parámetro test sirve para indicar el tipo de test: por ejemplo, test=&quot;t&quot; para un test t usando t.test, o test=&quot;p&quot; para un test aproximado de proporciones usando prop.test; el parámetro size sirve para indicar la magnitud esperada: &quot;small&quot;, &quot;medium&quot; o &quot;large&quot;. A modo de ejemplo, la siguiente instrucción nos da la magnitud de efecto que se considera pequeña en un test t: library(pwr) cohen.ES(test=&quot;t&quot;,size=&quot;small&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = t ## size = small ## effect.size = 0.2 De manera similar, para saber la magnitud de efecto que se considera media en un test aproximado de proporciones podemos usar instrucción siguiente: cohen.ES(test=&quot;p&quot;,size=&quot;medium&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = p ## size = medium ## effect.size = 0.5 Si se desea solo el valor de la magnitud del efecto, para poderlo entrar en otras funciones, se obtiene con el sufijo $effect.size: cohen.ES(test=&quot;p&quot;,size=&quot;medium&quot;)$effect.size ## [1] 0.5 Así pues, en un contraste de hipótesis intervienen cuatro cantidades fundamentales: el tamaño de la muestra, \\(n\\); el nivel de significación, \\(\\alpha\\); la potencia, \\(1-\\beta\\); y la magnitud del efecto. El tamaño de la muestra y el nivel de significación están bajo el control del investigador; sin embargo, la potencia del contraste y la magnitud del efecto afectan al contraste de forma más indirecta y su control escapa al investigador. Por ejemplo, si incrementamos el tamaño de la muestra, la potencia aumenta, pero el aumento preciso depende de la magnitud del efecto esperada. De hecho, las cuatro cantidades anteriores no son independientes, sino que, a partir de tres cualesquiera de ellas, se puede calcular la cuarta. Las funciones del paquete pwr permiten realizar estos cálculos para los contrastes de medias y proporciones. Las funciones de dicho paquete que por ahora nos interesan en este sentido son las siguientes: pwr.t.test, para utilizar en tests t de una media, de dos medias usando muestras emparejadas o de dos medias usando muestras independientes del mismo tamaño. pwr.t2n.test, para utilizar en tests t de dos medias usando muestras independientes de distinto tamaño. pwr.p.test, para utilizar en contrastes aproximados de una proporción. pwr.2p.test, para utilizar en contrastes aproximados de dos proporciones usando muestras independientes del mismo tamaño. pwr.2p2n.test, para utilizar en contrastes aproximados de dos proporciones usando muestras de distinto tamaño. Estas funciones tienen los parámetros básicos siguientes: n: el tamaño de la muestra (o de las muestras cuando son del mismo tamaño). n1 y n2: los tamaños de las dos muestras en pwr.2p2n.test y pwr.t2n.test. d (en las dos primeras) o h (en las tres últimas): la magnitud del efecto. sig.level: el nivel de significación. power: la potencia. type (en la primera): el tipo de muestras usado, siendo sus posibles valores &quot;one.sample&quot; (para contrastes de una muestra), &quot;two.sample&quot; (para contrastes de dos muestras independientes), o &quot;paired&quot; (para contrastes de dos muestras emparejadas). alternative: el tipo de hipótesis alternativa, con sus valores usuales. Si, en una cualquiera de estas funciones se especifican todos los parámetros n (o n1 y n2), d (o h), sig.level y power menos uno, la función da el valor del parámetro que falta. Veamos algunos ejemplos de uso. Ejemplo 5.22 Queremos calcular la potencia del contraste llevado a cabo en el Ejemplo 5.2. Se trataba de un contraste bilateral de una media usando un test t, por lo que utilizaremos la función pwr.t.test. Los parámetros que le entraremos son: n, el tamaño de la muestra; en este ejemplo, \\(n=25\\). d, la magnitud del efecto. Para tests t de una media e hipótesis nula \\(H_0: \\mu = \\mu_0\\), la magnitud del efecto se calcula con la fórmula \\[ d=\\frac{|\\overline{x}-\\mu_0|}{\\widetilde{s}_x}. \\] En nuestro ejemplo, \\(d=\\frac{|2.8048-2|}{0.68064}= 1.1824\\). sig.level, el nivel de significación; en este ejemplo, \\(\\alpha=0.05\\). Además como es un contraste bliateral de una media, especificaremos type=&quot;one.sample&quot; y alternative=&quot;two.sided&quot; (esto último en realidad no hace falta: como siempre, este es su valor por defecto). x=c(2.2,2.66,2.74,3.41,2.46,2.96,3.34,2.16,2.46,2.71,2.04,3.74,3.24, 3.92,2.38,2.82,2.2,2.42,2.82,2.84,4.22,3.64,1.77,3.44,1.53) mag.ef=abs(mean(x)-2)/sd(x) #Magnitud del efecto pwr.t.test(n=25, d=mag.ef, sig.level=0.05, type=&quot;one.sample&quot;, alternative=&quot;two.sided&quot;) ## ## One-sample t test power calculation ## ## n = 25 ## d = 1.18241 ## sig.level = 0.05 ## power = 0.999893 ## alternative = two.sided Obtenemos que la potencia del test es prácticamente 1. Si estuviéramos diseñando el experimento y quisiéramos calcular el tamaño mínimo de una muestra para tener un nivel de significación del 5% y potencia del 99%, suponiendo a priori que la magnitud del efecto esperado va a ser grande (y que por lo tanto detectar que la hipótesis alternativa es verdadera va a ser fácil), primero calcularíamos cuánto vale una magnitud del efecto grande: cohen.ES(test=&quot;t&quot;,size=&quot;large&quot;)$effect.size ## [1] 0.8 y a continuación la usaríamos en la función pwr.t.test: pwr.t.test(d=0.8, sig.level=0.05, power=0.99, type=&quot;one.sample&quot;) ## ## One-sample t test power calculation ## ## n = 30.7143 ## d = 0.8 ## sig.level = 0.05 ## power = 0.99 ## alternative = two.sided Bastarían 31 observaciones para tener la potencia deseada. Si en cambio esperáramos una magnitud del efecto pequeña: pwr.t.test(d=cohen.ES(test=&quot;t&quot;,size=&quot;small&quot;)$effect.size, sig.level=0.05, power=0.99, type=&quot;one.sample&quot;) ## ## One-sample t test power calculation ## ## n = 461.238 ## d = 0.2 ## sig.level = 0.05 ## power = 0.99 ## alternative = two.sided En este caso necesitaríamos 462 observaciones. Podemos obtener solo una de las componentes del resultado de una de estas funciones añadiéndole el sufijo adecuado. Por ejemplo, la potencia se obtiene con el sufijo $power y el valor de \\(n\\) con el sufijo $n: pwr.t.test(n=25, d=mag.ef, sig.level=0.05, type=&quot;one.sample&quot;, alternative=&quot;two.sided&quot;)$power ## [1] 0.999893 pwr.t.test(d=0.8, sig.level=0.05, power=0.99, type=&quot;one.sample&quot;)$n ## [1] 30.7143 Ejemplo 5.23 Vamos a calcular la potencia del contraste \\[ \\left\\{ \\begin{array}{l} H_0:p_v=p_n\\\\ H_1:p_v&lt;p_n \\end{array} \\right. \\] del Ejemplo 5.17. En este caso, usamos la función pwr.2p.test, ya que usamos dos muestras del mismo tamaño, y le entramos los parámetros siguientes: n, el tamaño de las muestras; en este ejemplo, \\(n=60\\). h, la magnitud del efecto. Para calcularla,3 usamos la función ES.h del mismo paquete pwr y que se aplica a las proporciones muestrales de éxitos: en este ejemplo, \\(\\widehat{p}_v=0.67\\) y \\(\\widehat{p}_n =0.8\\) y la magnitud del efecto vale: ES.h(0.67,0.8) ## [1] -0.296584 sig.level, el nivel de significación, 0.05. Como solo nos interesa la potencia, añadiremos al pwr.2p.test el sufijo $power: pwr.2p.test(h=ES.h(0.67,0.8), n=60, sig.level=0.05,alternative=&quot;less&quot;)$power ## [1] 0.491864 Hemos obtenido una potencia de, aproximadamente, un 49%. Si estuviéramos diseñando el experimento y quisiéramos calcular el tamaño de las muestras necesario para tener una potencia del 90% al nivel de significación del 5% y esperando una magnitud del efecto pequeña (porque esperamos una mejora con las nuevas trampas, pero solo pequeña), entraríamos: cohen.ES(test=&quot;p&quot;,size=&quot;small&quot;)$effect.size ## [1] 0.2 pwr.2p.test(h=-0.2, sig.level=0.05, power=0.9,alternative=&quot;less&quot;)$n ## [1] 428.192 Tendríamos que usar dos muestras de 429 cucarachas cada una. Observad que en pwr.2p.test hemos entrado en h la magnitud del efecto en negativo: esto es debido a que usamos alternative=&quot;less&quot; y por lo tanto esperamos que la primera proporción sea menor que la segunda. Ejemplo 5.24 En el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu_n=\\mu_f\\\\ H_{1}:\\mu_n&gt; \\mu_f \\end{array}\\right. \\] del Ejemplo 5.6, ¿qué tamaño de la muestra de mujeres fumadoras tendríamos que tomar si usáramos una muestra de 100 no fumadoras, quisiéramos una potencia del 90% y un nivel de significación del 5% y esperáramos una magnitud del efecto media? Como es un contraste de dos medias independientes y los tamaños de las muestras pueden ser diferentes, usaremos la función pwr.t2n.test. Entraremos como n1 el tamaño de la muestra de fumadoras y le pediremos que nos dé solo el valor de n2, el tamaño de la “otra” muestra: pwr.t2n.test(n1=100, d=cohen.ES(test=&quot;t&quot;,size=&quot;medium&quot;)$effect.size, sig.level=0.05, power=0.9, alternative=&quot;greater&quot;)$n2 ## [1] 52.8251 Bastaría estudiar 53 madres fumadoras. 5.5 Guía rápida Excepto en las que decimos lo contrario, todas las funciones para realizar contrastes que damos a continuación admiten los parámetros alternative, que sirve para especificar el tipo de contraste (unilateral en un sentido u otro o bilateral), y conf.level, que sirve para indicar el nivel de confianza \\(1-\\alpha\\). Sus valores por defecto son contraste bilateral y nivel de confianza 0.95. t.test realiza tests t para contrastar una o dos medias (tanto usando muestras independientes como emparejadas). Aparte de alternative y conf.level, sus parámetros principales son: mu para especificar el valor de la media que queremos contrastar en un test de una media. paired para indicar si en un contraste de dos medias usamos muestras independientes o emparejadas. var.equal para indicar en un contraste de dos medias usando muestras independientes si las varianzas poblacionales son iguales o diferentes. SIGN.test del paquete BSDA, realiza un test de signos para contrastar una mediana. Dispone del parámetro md para entrar la mediana a contrastar. wilcox.test, para realizar tests de Wilcoxon y de Mann-Whitney para contrastar una o dos medianas (tanto usando muestras independientes como emparejadas). Sus parámetros son los mismos que los de t.test (salvo var.equal, que en estos tests no tiene sentido). sigma.test, para realizar tests \\(\\chi^2\\) para contrastar una varianza (o una desviación típica). Dispone de los parámetros sigma y sigmasq para indicar, respectivamente, la desviación típica o la varianza a contrastar. var.test, para realizar tests F para contrastar dos varianzas (o dos desviaciones típicas). fligner.test, para realizar tests no paramétricos de Fligner-Killeen para contrastar dos varianzas (o dos desviaciones típicas). No dispone de los parámetros alternative (solo sirve para contrastes bilaterales) ni conf.level (no calcula intervalos de confianza). binom.test, para realizar tests binomiales exactos para contrastar una proporción. Dispone del parámetro p para indicar la proporción a contrastar. prop.test, para realizar tests aproximados para contrastar una proporción o dos proporciones de poblaciones usando muestras independientes. También dispone del parámetro p para indicar la proporción a contrastar en un contraste de una proporción. fisher.test, para realizar tests exactos de Fisher para contrastar dos proporciones usando muestras independientes. mcnemar.test, para realizar tests bilaterales de McNemar para contrastar dos proporciones usando muestras emparejadas. No dispone de los parámetros alternative ni conf.level. cohen.ES del paquete pwr, da los valores aceptados por convenio como “pequeño”, “mediano” y “grande” para diferentes tests. pwr.t.test del paquete pwr, relaciona el tamaño de la(s) muestra(s), el nivel de significación, la potencia y la magnitud del efecto (en el sentido de que si se entran tres de estos valores se obtiene el cuarto) en tests t de una media, de dos medias usando muestras emparejadas o de dos medias usando muestras independientes del mismo tamaño. Sus parámetros, son n: el tamaño de la muestra o de las muestras. sig.level: el nivel de significación. power: la potencia. d: la magnitud del efecto type: el tipo de muestras (una muestra, dos muestras emparejadas, dos muestras independientes). alternative: el tipo de hipótesis alternativa. pwr.t2n.testdel paquete pwr, relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en tests t de dos medias usando muestras independientes de distinto tamaño. Sus parámetros son n1 y n2: los tamaños de las dos muestras. sig.level, power, d y alternative como en pwr.t.test. pwr.p.test del paquete pwr, relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en contrastes aproximados de una proporción. Sus parámetros son n, sig.level, power y alternative como en pwr.t.test. h: la magnitud del efecto pwr.2p.test del paquete pwr, relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en contrastes aproximados de dos proporciones usando muestras independientes del mismo tamaño. Sus parámetros son los mismos que los de pwr.p.test. pwr.2p2n.test, del paquete pwr, relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en contrastes aproximados de dos proporciones usando muestras de distinto tamaño. Sus parámetros son n1 y n2: los tamaños de las dos muestras. sig.level, power, h y alternative como en pwr.p.test. 5.6 Ejercicios Modelo de test (1) Tenemos una m.a.s. de una población normal \\(X\\sim N(\\mu,\\sigma)\\) formada por los números 2,5,3,5,6,6,7,2. Usando la función t.test, calculad el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del contraste \\(H_0: \\mu=4\\) contra \\(H_1:\\mu \\neq 4\\) y decid (contestando SI, sin acento, o NO) si podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.05. Tenéis que dar las dos respuestas en este orden, separadas por un único espacio en blanco. (2) Tenemos dos muestras de poblaciones normales, \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\). Sean \\(x_1=(2,5,3,5,6,6,7,2)\\) y \\(x_2=(3,2,5,4,2,2,4,5,1,6,2)\\) muestras aleatorias simples de \\(X_1\\) y \\(X_2\\), respectivamente. Usando la función t.test, calculad el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del contraste \\(H_0: \\mu_1=\\mu_2\\) contra \\(H_1:\\mu_1&gt;\\mu_2\\) suponiendo que las varianzas son diferentes y decid (contestando SI, sin acento, o NO) si podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.1. Tenéis que dar las dos respuestas en este orden, separadas por un único espacio en blanco. (3) Tenemos dos muestras de poblaciones normales, \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\). Sean \\(x_1=(2,5,3,5,6,6,7,2)\\) y \\(x_2=(3,2,10,9,2,2,4,5,1,10,2)\\) muestras aleatorias simples de \\(X_1\\) y \\(X_2\\), respectivamente. Usando la función var.test, calculad los extremos inferior y superior de un intervalo de confianza del 95% para \\(\\sigma_1^2/\\sigma_2^2\\) (redondeados a 3 cifras decimales, sin ceros innecesarios a la derecha) y decid (contestando SI, sin acento, o NO) si en el contraste \\(H_0: \\sigma_1=\\sigma_2\\) contra \\(H_1:\\sigma_1 \\neq \\sigma_2\\) podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.05. Tenéis que dar las tres respuestas en este orden, separadas por un único espacio en blanco. (4) Tenemos dos variables aleatorias de Bernoulli de proporciones poblacionales \\(p_1\\) y \\(p_2\\), respectivamente. En una muestra de 100 observaciones de la primera hemos obtenido 20 éxitos, y en una muestra de 150 observaciones de la segunda, hemos obtenido 40 éxitos. Usando la función prop.test, calculad el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del contraste \\(H_0: p_1=p_2\\) contra \\(H_1:p_1&lt;p_2\\) y decid (contestando SI, sin acento, o NO) si podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.05. Tenéis que dar las dos respuestas en este orden, separadas por un único espacio en blanco. Respuestas al test (1) 0.487 NO Nosotros lo hemos resuelto con x=c(2,5,3,5,6,6,7,2) round(t.test(x,mu=4)$p.value,3) ## [1] 0.487 (2) 0.083 SI Nosotros lo hemos resuelto con x1=c(2,5,3,5,6,6,7,2) x2=c(3,2,5,4,2,2,4,5,1,6,2) round(t.test(x1,x2,alternative=&quot;greater&quot;)$p.value,3) ## [1] 0.083 (3) 0.078 1.465 NO Nosotros lo hemos resuelto con x1=c(2,5,3,5,6,6,7,2) x2=c(3,2,10,9,2,2,4,5,1,10,2) round(var.test(x1,x2)$conf.int,3) ## [1] 0.078 1.465 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 (4) 0.145 NO Nosotros lo hemos resuelto con round(prop.test(c(20,40),c(100,150),alternative=&quot;less&quot;)$p.value,3) ## [1] 0.145 Se sabe que si las dos muestras provienen de poblaciones normales y son del mismo tamaño, el test t tiende a dar la misma conclusión tanto si se supone que las dos varianzas son iguales como si se supone que son diferentes: véase C. A. Markowski y E. P. Markowski, “Conditions for the Effectiveness of a Preliminary Test of Variance,” The American Statistician 44 (1990), pp. 322-326. Pero no sabemos si estas longitudes siguen distribuciones normales o no, y que “tienda a dar” la misma conclusión no significa que en un ejemplo concreto con p-valores cercanos al nivel de significación no pueda dar conclusiones diferentes.↩ Véanse: E. S. Pearson, “The analysis of variance in cases of non-normal variation,” Biometrika 23 (1931), pp. 114-133; G. E. P. Box, “Non-normality and tests on variances,” Biometrika 40 (1953), pp. 318-335.↩ Por si a alguien le interesa, la fórmula para esta magnitud del efecto es \\[ h=2\\left(\\arcsin\\big(\\sqrt{\\widehat{p}_1}\\,\\big)-\\arcsin\\big(\\sqrt{\\widehat{p}_2}\\,\\big)\\right), \\] siendo \\(\\widehat{p}_1\\) y \\(\\widehat{p}_2\\) las proporciones muestrales de éxitos de las dos muestras.↩ "],
["chap-bondad.html", "Lección 6 Contrastes de bondad de ajuste 6.1 Pruebas gráficas: Q-Q-plots 6.2 El test \\(\\chi^2\\) de Pearson 6.3 El test \\(\\chi^2\\) para distribuciones continuas 6.4 El test de Kolgomorov-Smirnov 6.5 Tests de normalidad 6.6 Guía rápida 6.7 Ejercicios", " Lección 6 Contrastes de bondad de ajuste Una de las condiciones habituales que requerimos sobre una muestra, por ejemplo, al razonar sobre la distribución de sus estadísticos o al realizar contrastes de hipótesis, es que la población de la que la hemos extraído siga una determinada distribución. En la Lección ?? de la primera parte del curso comprobábamos gráficamente el ajuste de una muestra a una distribución normal mediante histogramas y dibujando las curvas de densidad muestral y de densidad de la normal. En esta lección presentamos algunas instrucciones que implementan contrastes de bondad de ajuste (goodness of fit), técnicas cuantitativas que permiten decidir si los datos de una muestra “se ajustan” a una determinada distribución de probabilidad, es decir, si la variable aleatoria que los ha generado sigue o no esta distribución de probabilidad. Los contrastes de bondad de ajuste tienen el mismo significado que los explicados en la Lección 5. Se contrasta una hipótesis nula \\(H_0\\): La variable aleatoria poblacional tiene distribución \\(X\\) contra la hipótesis alternativa \\(H_1\\): La variable aleatoria poblacional no tiene distribución \\(X\\) Como siempre, para llevar a cabo el contraste tomamos una muestra aleatoria de la población. Entonces, obtenemos evidencia significativa de que la población no tiene distribución \\(X\\) cuando es muy raro obtener nuestra muestra si la población tiene esta distribución. En este caso, rechazamos la hipótesis nula en favor de la alternativa. Si, en cambio, no es del todo inverosímil que la muestra se haya generado con la distribución \\(X\\), aceptamos la hipótesis nula “por defecto” y concluimos que la población sí que tiene esta distribución. Pero que aceptemos la hipótesis nula no nos da evidencia de que la población tenga distribución \\(X\\): simplemente nos dice que no encontramos motivos para rechazarlo. Naturalmente, a efectos prácticos, si aceptamos la hipótesis nula de que la población tiene la distribución \\(X\\), actuaremos como si creyéramos que efectivamente esta hipótesis es verdadera, por ejemplo a la hora de decidir que fórmulas usar para calcular un intervalo de confianza o para efectuar un contraste de hipótesis. Pero no estaremos seguros de que podamos emplear estas fórmulas sobre nuestra muestra, simplemente no tendremos motivos para dejar de creerlo. Los pasos habituales para contrastar la bondad del ajuste de una muestra a una distribución son los siguientes: Fijar la familia de distribuciones teóricas a la que queremos ajustar los datos. Esta familia estará parametrizada por uno o varios parámetros. Recordemos los ejemplos más comunes: Si la familia es la Bernoulli, el parámetro es \\(p\\): la probabilidad poblacional de éxito. Si la familia es la Poisson, el parámetro es \\(\\lambda\\): la esperanza. Si la familia es la binomial, los parámetros son \\(n\\) y \\(p\\): el tamaño de las muestras y la probabilidad de éxito, respectivamente. Si la familia es la normal, los parámetros son \\(\\mu\\) y \\(\\sigma\\): la esperanza y la desviación típica, respectivamente. Si la familia es la \\(\\chi^2\\), el parámetro es el número de grados de libertad. Si la familia es la t de Student, el parámetro es de nuevo el número de grados de libertad. Otras familias de distribuciones tienen parámetros de localización (location), escala (scale) o forma (shape), por lo que no nos ha de extrañar si R nos pide que asignemos parámetros con estos nombres. Si el diseño del experimento no fija sus valores, tendremos que estimar a partir de la muestra los valores de los parámetros que mejor se ajusten a nuestros datos. Ya hemos tratado la estimación de parámetros en la Lección 3. Determinar qué tipo de contraste vamos a utilizar. En esta lección veremos dos tipos básicos de contrastes generales: El test \\(\\chi^2\\) de Pearson. Este test es válido tanto para variables discretas como para continuas, pero solo se puede aplicar a conjuntos grandes de datos (por fijar una cota concreta, de 30 o más elementos). Además, si el espacio muestral, es decir, el conjunto de resultados posibles, es infinito, es necesario agrupar estos resultados en un número finito de clases. El test de Kolgomorov-Smirnov. Este test solo es válido para variables continuas, y compara la función de distribución acumulada muestral con la teórica. No requiere que la muestra sea grande, pero en cambio, en principio, no admite que los datos de la muestra se puedan repetir.4 Por desgracia, las repeticiones suelen ser habituales si la muestra es grande y la precisión de los datos es baja o la variabilidad de la población muestreada es pequeña. Aparte, determinados tipos de distribuciones tienen sus contrastes de bondad de ajustes específicos. Este es el caso especialmente de la normal, para la que explicaremos algunos tests que permiten contrastar si una muestra proviene de alguna distribución normal. Realizar el contraste y redactar las conclusiones. Es conveniente apoyar los resultados del contraste con gráficos. En esta lección explicaremos los gráficos cuantil-cuantil, o Q-Q-plots, que sirven para visualizar el ajuste de unos datos a una distribución conocida y son una buena alternativa a los histogramas con curvas de densidad. 6.1 Pruebas gráficas: Q-Q-plots Para comparar la distribución de una muestra con una distribución poblacional teórica se pueden realizar diversas pruebas gráficas. En la Lección ?? de la primera parte del curso usábamos para ello histogramas con densidades estimadas y teóricas. En esta sección explicamos otro tipo de gráficos que pueden usarse con el mismo fin, los gráficos cuantil-cuantil, o, para abreviar, Q-Q-plots. Estos gráficos comparan los cuantiles observados de la muestra con los cuantiles teóricos de la distribución teórica. Figura 6.1: Q-Q-plot básico de la muestra del Ejemplo 6.1 contra una t de Student con 4 grados de libertad. La Figura 6.1 muestra un Q-Q-plot. Cada punto corresponde a un cuantil: grosso modo, hay un punto para cada \\(k/n\\)-cuantil, siendo \\(n\\) la longitud de la muestra y \\(k=1,\\ldots,n\\). Para cada uno de estos cuantiles, el punto correspondiente tiene abscisa el cuantil de la distribución teórica (en este caso, una t de Student con 4 grados de libertad) y ordenada el cuantil de la muestra. Por lo tanto, si el ajuste es bueno, para cada \\(k/n\\), el cuantil muestral y el cuantil teórico han de ser parecidos, de manera que los puntos del gráfico (les llamaremos Q-Q-puntos, para abreviar) han de estar cerca de la diagonal \\(y=x\\), que hemos añadido al gráfico. En general, se considera que un Q-Q-plot muestra un buen ajuste cuando no se observa una tendencia marcada de desviación respecto de la diagonal. Sin embargo, a menudo los Q-Q-plots son difíciles de interpretar, y es conveniente combinarlos con algún contraste de bondad de ajuste. Hay varias maneras de producir Q-Q-plots con R. Aquí solo explicaremos una: la función qqPlot del paquete car. Su sintaxis básica es qqPlot(x, distribution=..., parámetros, id=FALSE, ...) donde: x es el vector con la muestra. El parámetro distribution se ha de igualar al nombre de la familia de distribuciones entre comillas, y puede tomar como valor cualquier familia de distribuciones de la que R sepa calcular la densidad y los cuantiles: esto incluye las distribuciones que hemos estudiado hasta el momento: &quot;norm&quot;, &quot;binom&quot;, &quot;poisson&quot;, &quot;t&quot;, etc. A continuación, se tienen que entrar los parámetros de la distribución, igualando su nombre habitual (mean para la media, sd para la desviación típica, df para los grados de libertad, etc.) a su valor. En algunos casos, si no se especifican los parámetros, qqPlot toma sus valores por defecto: por ejemplo, si queremos realizar un Q-Q-plot contra una normal y no especificamos los valores de la media y la desviación típica de la distribución teórica, qqPlot los toma iguales a 0 y 1, respectivamente. Por defecto, el gráfico obtenido con la función qqPlot identifica los dos Q-Q-puntos con ordenadas más extremas. Para omitirlos, usad el parámetro id=FALSE. Otros parámetros a tener en cuenta: qqPlot añade por defecto una rejilla al gráfico, que podéis eliminar con grid=FALSE. qqPlot añade por defecto una línea recta que une los Q-Q-puntos correspondientes al primer y tercer cuartil: se la llama recta cuartil-cuartil. Un buen ajuste de los Q-Q-puntos a esta recta significa que la muestra se ajusta a la distribución teórica, pero posiblemente con parámetros diferentes a los especificados. Os recomendamos mantenerla, pero si queréis eliminarla por ejemplo para substituirla por la diagonal \\(y=x\\), podéis usar el parámetro line=&quot;none&quot;. qqPlot también añade dos curvas discontinuas que abrazan una “región de confianza del 95%” para el Q-Q-plot. Sin entrar en detalles, esta región contendría todos los Q-Q-puntos en un 95% de las ocasiones que tomáramos una muestra de la distribución teórica del mismo tamaño que la nuestra. Por lo tanto, si todos los Q-Q-puntos caen dentro de esta franja, no hay evidencia para rechazar que la muestra provenga de la distribución teórica. Esta franja de confianza es muy útil para interpretar el Q-Q-plot, pero la podéis eliminar con envelope=FALSE. Se pueden usar los parámetros usuales de plot para poner nombres a los ejes, título, modificar el estilo de los puntos, etc., y otros parámetros específicos para modificar el aspecto del gráfico. Por ejemplo, col.lines sirve para especificar el color de las líneas que añade. Consultad la Ayuda de la función. Ejemplo 6.1 Consideremos la siguiente muestra: muestra=c(0.27,0.81,-0.73,-0.96,1.33,0.91,-1.70,0.24,-0.19,0.29,1.41,0.13,-0.06, -0.85,-0.59,-3.62,-1.02,2.36,0.34,-0.31,0.81,-0.88,0.27,0.52,1.05,0.20,0.76,0.25, -1.43,3.71,-0.78,0.39,-1.01,1.53,-0.72,1.22,0.56,-1.17,-0.65,-0.33,-0.07,0.31, -0.74,0.36,-1.72,-1.21,-0.05,-1.17,0.28,1.30,0.89,1.45,0.13,-1.12,3.13,-1.21, -0.90,-0.31,-1.05,0.89,-1.06,0.21,-0.50,-0.36,-0.29,-0.19,-1.71,0.09,0.21,0.55, -1.42,0.19,-0.62,2.46,-0.17,-0.63,0.77,0.94,0.55,0.35,-4.47,1.71,0.07,-0.57, -1.43,-0.85,1.06,0.82,0.19,-1.08,0.30,-0.87,0.77,1.23,-0.04,0.66,-0.87,-0.86, -1.06,0.10) Queremos comprobar gráficamente si sigue una distribución t de Student de 4 grados de libertad. Vamos a usar la función qqPlot con sus parámetros por defecto: library(car) qqPlot(muestra, distribution=&quot;t&quot;, df=4, id=FALSE) Figura 6.2: Q-Q-plot de la muestra del Ejemplo 6.1 contra una t de Student con 4 grados de libertad producido por defecto con qqPlot. Como todos los Q-Q-puntos están dentro de la región de confianza del 95%, podemos aceptar que la muestra proviene de una t de Student. El Q-Q-plot básico de la Figura 6.1 se ha obtenido con el código siguiente: qqPlot(muestra, distribution=&quot;t&quot;, df=4, envelope=FALSE, xlab=&quot;Cuantiles de t&quot;, ylab=&quot;Cuantiles de la muestra&quot;, line=&quot;none&quot;, pch=20, grid=FALSE, id=FALSE) abline(0,1, col=&quot;red&quot;, lwd=1.5) Veamos otro ejemplo. Ejemplo 6.2 Consideremos el data frame iris que contiene información sobre medidas relacionadas con las flores de una muestra de iris de tres especies. Vamos a producir un Q-Q-plot que ilustre si las longitudes de los sépalos de las plantas iris recogidas en esta tabla de datos siguen una distribución normal. A un Q-Q-plot que compara una muestra con una distribución normal se le suele llamar, para abreviar, un normal-plot. En primer lugar, estimamos los parámetros máximo verosímiles de la distribución normal que podría haber generado nuestra muestra: library(MASS) iris.sl=iris$Sepal.Length mu=fitdistr(iris.sl,&quot;normal&quot;)$estimate[1] sigma=fitdistr(iris.sl,&quot;normal&quot;)$estimate[2] round(c(mu,sigma),3) ## mean sd ## 5.843 0.825 y ahora generamos el Q-Q-plot usando estos parámetros qqPlot(iris.sl, distribution=&quot;norm&quot;, mean=mu, sd=sigma,xlab=&quot;Cuantiles de la normal&quot;,id=FALSE, ylab=&quot;Cuantiles de la muestra&quot;,main=&quot;&quot;) Figura 6.3: Normal-plot de longitudes de sépalos de flores iris. Vemos cómo los primeros puntos salen de la región de confianza del 95%. Esto significa que los datos están más desplazados hacia la izquierda de la media que lo que se esperaría en una muestra aleatoria de una normal. El boxplot de la Figura 6.4 muestra este desplazamiento. boxplot(iris.sl) Figura 6.4: Boxplot de longitudes de sépalos de flores iris. Interpretamos el Q-Q-plot anterior como evidencia de que estas longitudes no siguen una distribución normal. Más adelante usaremos tests de normalidad específicos para contrastar la normalidad de estos datos. 6.2 El test \\(\\chi^2\\) de Pearson El test \\(\\chi^2\\) de Pearson contrasta si una muestra ha sido generada o no con una cierta distribución, cuantificando si sus valores aparecen con una frecuencia cercana a la que sería de esperar si la muestra siguiera esa distribución. Esto se lleva a cabo por medio del estadístico de contraste \\[ X^2=\\sum_{i=1}^k\\frac{(\\mbox{frec. observada}_i-\\mbox{frec. esperada}_i)^2}{\\mbox{frec. esperada}_i} \\] donde k es el número de clases e i es el índice de las clases, de manera que “frec. observadai” y “frec. esperadai” denotan, respectivamente, la frecuencia observada de la clase i-ésima y su frecuencia esperada bajo la distribución que contrastamos. Si se satisfacen una serie de condiciones, este estadístico sigue aproximadamente una ley \\(\\chi^2\\) con un número de grados de libertad igual al número de clases menos uno y menos el número de parámetros de la distribución teórica que hayamos estimado. Las condiciones que se han de satisfacer son: La muestra ha de ser grande, digamos que de tamaño como mínimo 30; Si los posibles valores son infinitos, hay que agruparlos en un número finito k de clases que cubran todos los posibles valores (recordad que en la Lección ?? de la primera parte del curso ya explicamos cómo agrupar variables aleatorias continuas con la función cut); Las frecuencias esperadas de las clases en las que hemos agrupado el espacio muestral han de ser todas, o al menos una gran mayoría, mayores o iguales que 5. La instrucción básica en R para realizar un test \\(\\chi^2\\) es chisq.test. Su sintaxis básica es chisq.test(x, p=..., rescale.p=..., simulate.p.value=...) donde: x es el vector (o la tabla, calculada con table) de frecuencias absolutas observadas de las clases en la muestra. p es el vector de probabilidades teóricas de las clases para la distribución que queremos contrastar. Si no lo especificamos, se entiende que la probabilidad es la misma para todas las clases. Obviamente, estas probabilidades se tienen que especificar en el mismo orden que las frecuencias de x y, como son las probabilidades de todos los resultados posibles, en principio tienen que sumar 1; esta condición se puede relajar con el siguiente parámetro. rescale.p es un parámetro lógico que, si se iguala a TRUE, indica que los valores de p no son probabilidades, sino solo proporcionales a las probabilidades; esto hace que R tome como probabilidades teóricas los valores de p partidos por su suma, para que sumen 1. Por defecto vale FALSE, es decir, se supone que el vector que se entra como p son probabilidades y por lo tanto debe sumar 1, y si esto no pasa se genera un mensaje de error indicándolo. Igualarlo a TRUE puede ser útil, porque nos permite especificar las probabilidades mediante las frecuencias esperadas o mediante porcentajes. Pero también es peligroso, porque si nos hemos equivocado y hemos entrado un vector en p que no corresponda a una probabilidad, R no nos avisará. simulate.p.value es un parámetro lógico que indica a la función si debe optar por una simulación para el cálculo del p-valor del contraste. Por defecto vale FALSE, en cuyo caso este p-valor no se simula sino que se calcula mediante la distribución \\(\\chi^2\\) correspondiente. Si se especifica como TRUE, R realiza una serie de replicaciones aleatorias de la situación teórica: por defecto, 2000, pero su número se puede especificar mediante el parámetro B. Es decir, genera un conjunto de vectores aleatorios de frecuencias con la distribución que queremos contrastar, cada uno de suma total la de x. A continuación, calcula la proporción de estas repeticiones en las que el estadístico de contraste es mayor o igual que el obtenido para x, y éste es el p-valor que da. Cuando no se satisfacen las condiciones para que \\(X^2\\) siga aproximadamente una distribución \\(\\chi^2\\), estimar el p-valor mediante simulaciones es una buena alternativa. Veamos un primer ejemplo sencillo. Ejemplo 6.3 Tenemos un dado, y queremos contrastar si está equilibrado o trucado. Lo hemos lanzado 40 veces y hemos obtenido los resultados siguientes: Resultados Frecuencias 1 8 2 4 3 6 4 3 5 7 6 12 Si el dado está equilibrado, la probabilidad de cada resultado es 1/6 y por lo tanto la frecuencia esperada de cada resultado es 40/6=6.667. Como la muestra tiene más de 30 elementos y las frecuencias esperadas son todas mayores que 5, podemos realizar de manera segura un test \\(\\chi^2\\). Por lo tanto, entraremos estas frecuencias en un vector y le aplicaremos la función chisq.test. Como contrastamos si todas las clases tienen la misma probabilidad, no hace falta especificar el valor del parámetro p. freqs=c(8,4,6,3,7,12) chisq.test(freqs) ## ## Chi-squared test for given probabilities ## ## data: freqs ## X-squared = 7.7, df = 5, p-value = 0.174 Observemos la estructura del resultado de un chisq.test. Nos da el valor del estadístico \\(X^2\\) (X-squared), el p-valor del contraste (p-value), y los grados de libertad de la distribución \\(\\chi^2\\) que ha usado para calcularlo (df). En este caso, el p-valor es 0.174, y por lo tanto no podemos rechazar que el dado esté equilibrado. Queremos remarcar que, como R no sabe si hemos estimado o no parámetros, el número de grados de libertad que usa chisq.test es simplemente el número de clases menos 1. Si no hemos estimado parámetros para calcular las probabilidades teóricas, ya va bien, pero si lo hemos hecho y por lo tanto el número de grados de libertad no es el adecuado, tendremos que calcular el p-valor correcto a partir del valor del estadístico. Veremos varios ejemplos más adelante. El resultado de un chisq.test es una list, de la que podemos extraer directamente la información que deseemos con los sufijos adecuados. En concreto, podemos obtener el valor del estadístico \\(X^2\\) con el sufijo $statistic, los grados de libertad con el sufijo $parameter y el p-valor con el sufijo $p.value. chisq.test(freqs)$statistic ## X-squared ## 7.7 chisq.test(freqs)$parameter ## df ## 5 chisq.test(freqs)$p.value ## [1] 0.173563 Imaginemos ahora que, en vez de lanzar el dado 40 veces, lo lanzamos 20 veces, y obtenemos los resultados siguientes: Resultados Frecuencias 1 4 2 2 3 3 4 2 5 3 6 6 ¿Hay evidencia de que el dado esté trucado? Ahora la muestra no es grande y las frecuencias esperadas son todas 20/6=3.333, menores que 5. Por tanto, el p-valor del test \\(\\chi^2\\) que se obtiene usando una distribución \\(\\chi^2_5\\) no tiene por qué tener ningún significado. En una situación como ésta es cuando conviene usar el parámetro simulate.p.value. Vamos a pedir a R que simule 5000 veces el experimento de lanzar 20 veces un dado equilibrado, y que calcule como p-valor la proporción de simulaciones en las que el estadístico \\(X^2\\) haya dado un valor mayor o igual que el que se obtiene con nuestra muestra. freqs2=c(4,2,3,2,3,6) chisq.test(freqs2, simulate.p.value=TRUE, B=5000) ## ## Chi-squared test for given probabilities with simulated p-value ## (based on 5000 replicates) ## ## data: freqs2 ## X-squared = 3.4, df = NA, p-value = 0.7 Resulta que en un 70% de las simulaciones el valor de \\(X^2\\) ha sido mayor o igual que el de nuestra muestra, 3.4. Por lo tanto, nuestra muestra entra dentro de lo normal para un dado equilibrado, por lo que no hay evidencia de que el dado esté trucado. Como este p-valor se basa en simulaciones, en cada aplicación del test el p-valor puede dar resultados diferentes, pero en general la conclusión es robusta si se toma un número suficiente de simulaciones. chisq.test(freqs2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.70126 chisq.test(freqs2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.717656 chisq.test(freqs2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.708858 Como vemos, los p-valores son todos similares. Por curiosidad, ¿qué p-valor da el test \\(\\chi^2\\) usando la distribución de \\(\\chi^2_5\\)? chisq.test(freqs2)$p.value ## Warning in chisq.test(freqs2): Chi-squared approximation may be incorrect ## [1] 0.63857 El p-valor no es muy diferente y la conclusión en este caso sería la misma, pero fijaos en el mensaje de advertencia: para la muestra dada, la aproximación de la distribución de \\(X^2\\) mediante una \\(\\chi^2\\) no tiene por qué ser correcta. Ejemplo 6.4 Vamos a estudiar las frecuencias de los nucleótidos en una cadena de ADN, y contrastar si aparecen los cuatro con la misma probabilidad o no. En este caso, el espacio muestral son los cuatro nucleótidos: adenina (A), citosina (C), guanina (G) y timina (T). Identificaremos una cadena de ADN con un vector de letras a, c, g y t. Si llamamos \\(p_a\\), \\(p_c\\), \\(p_g\\) y \\(p_t\\) a las probabilidades de aparición de estas letras, el contraste que queremos realizar es \\[ \\left\\{\\begin{array}{l} H_0 : p_a=p_c=p_g=p_t=0.25\\\\ H_1: \\mbox{Algunos nucleótidos son más probables que otros} \\end{array} \\right. \\] Vamos a analizar una cadena de ADN “de verdad”, extraída de la base de datos GenBank. Para ello, utilizaremos el paquete ape, que incorpora una función read.GenBank que permite leer secuencias de genes incluidas en esta base de datos y convertirlas en vectores de letras a, c, g y t. En concreto, si la aplicamos al número de acceso (accession number) de una secuencia (entrado entre comillas, ya que es una palabra) y usamos el parámetro as.character=TRUE, nos devuelve dicha secuencia como un vector de letras junto con otra información sobre la secuencia. En este ejemplo, nos vamos a interesar por el gen que codifica la mioglobina humana, que es una proteína relativamente pequeña constituida por una sola cadena polipeptídica de 153 aminoácidos. Su número de acceso es AH002877.2. El código siguiente lee este gen y lo guarda en un objeto. library(ape) myoglobin=read.GenBank(&quot;AH002877.2&quot;, as.character=TRUE) Consultemos cómo es el objeto donde hemos guardado esta secuencia: str(myoglobin) ## List of 1 ## $ AH002877.2: chr [1:6889] &quot;g&quot; &quot;t&quot; &quot;a&quot; &quot;c&quot; ... ## - attr(*, &quot;species&quot;)= chr &quot;Homo_sapiens&quot; Vemos que se trata una list formada por una sola componente, el vector de bases, y un atributo. Vamos a extraer el vector, para poder trabajar con él. La manera más sencilla es añadiendo a la list el sufijo [[1]]: myoglobin=myoglobin[[1]] myoglobin[1:10] ## [1] &quot;g&quot; &quot;t&quot; &quot;a&quot; &quot;c&quot; &quot;t&quot; &quot;g&quot; &quot;t&quot; &quot;a&quot; &quot;t&quot; &quot;t&quot; Nos preguntamos si en esta secuencia las cuatro bases aparecen de manera equiprobable. Para responder esta pregunta, calculamos las frecuencias de las letras con la función table y aplicamos el test \\(\\chi^2\\) a los resultados. table(myoglobin) ## myoglobin ## a c g n t ## 1718 1453 2019 200 1499 Aparecen valores n, que corresponden a bases no resueltas. Vamos borrarlas de la secuencia: myoglobin=myoglobin[myoglobin!=&quot;n&quot;] table(myoglobin) ## myoglobin ## a c g t ## 1718 1453 2019 1499 Ahora ya estamos en condiciones de llevar a cabo el contraste deseado con la función chisq.test. Puesto que miramos si todos los resultados aparecen con la misma probabilidad, no hace falta especificar el vector p de probabilidades. chisq.test(table(myoglobin)) ## ## Chi-squared test for given probabilities ## ## data: table(myoglobin) ## X-squared = 119.8, df = 3, p-value &lt;2e-16 El p-valor es prácticamente 0, podemos rechazar que las cuatro bases aparezcan con la misma probabilidad: las diferencias entre las frecuencias de los cuatro aminoácidos son lo suficientemente grandes como para hacer inverosímil que se hayan generado con la misma probabilidad. Ejemplo 6.5 Siguiendo con el ejemplo anterior, vamos a contrastar ahora si las bases siguen una distribución en la que A y G aparecen un 25% de veces más que C y T. Usaremos p=c(1.25,1,1.25,1) para especificar estas proporciones. chisq.test(table(myoglobin),p=c(1.25,1,1.25,1)) ## Error in chisq.test(table(myoglobin), p = c(1.25, 1, 1.25, 1)): probabilities must sum to 1. ¡Vaya! Nos habíamos olvidado de especificar rescale.p=TRUE, para poder entrar como p un vector proporcional a las probabilidades. chisq.test(table(myoglobin),p=c(1.25,1,1.25,1),rescale.p=TRUE)$p.value ## [1] 1.30045e-05 De nuevo, tenemos que rechazar la hipótesis nula. Ejemplo 6.6 Vamos a realizar otro experimento con la cadena del gen de la mioglobina. Este gen consta de tres exones. El primero corresponde al número de acceso M10090.1. Vamos a comparar si la frecuencia de bases en este exón es similar a la de la cadena total, que hará de distribución teórica. Para ello, leemos el exón y lo guardamos en una cadena exon1=read.GenBank(&quot;M10090.1&quot;, as.character=TRUE)[[1]] Calculemos la tabla de frecuencias relativas de las bases en la cadena completa probs.tot=prop.table(table(myoglobin)) round(probs.tot,3) ## myoglobin ## a c g t ## 0.257 0.217 0.302 0.224 Vamos a usar esta tabla como parámetro p de la función chisq.test: chisq.test(table(exon1),p=probs.tot)$p.value ## [1] 9.78458e-06 El p-valor es muy pequeño, y por lo tanto podemos rechazar que en este exón las bases aparezcan con la misma probabilidad que en la cadena total. Ejemplo 6.7 Ahora vamos a llevar a cabo el experimento siguiente. Queremos contrastar si la aparición de pares “cg” en la mioglobina humana es aleatoria, en el sentido de que se debe simplemente a las apariciones al azar de sus dos bases, o si por el contrario hay algún otro mecanismo que los produce. Para ello, tomaremos la secuencia completa de la mioglobina humana, que tenemos almacenada en myoglobin, y repetiremos 100 veces el proceso siguiente: extraemos una muestra aleatoria simple de 20 posiciones de la secuencia y contamos cuántas de ellas contienen el par de bases “cg”. Luego contrastaremos si la muestra así obtenida proviene de una distribución binomial con la probabilidad de aparición de “cg” como si las dos bases fueran independientes. Fijamos la semilla de aleatoriedad para que se pueda reproducir el experimento.5 El código, que luego explicamos, y el resultado son los siguientes: cg.in.sample=function(x,S){#x un vector, S vector de índices length(which(x[S]==&quot;c&quot; &amp; x[S+1]==&quot;g&quot;)) } set.seed(1660) muestra=replicate(100, cg.in.sample(myoglobin, sample(1:(length(myoglobin)-1), 20,replace=TRUE))) muestra ## [1] 2 1 1 0 0 0 0 2 1 0 2 0 0 0 2 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 2 0 ## [36] 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 ## [71] 1 0 0 0 1 0 0 0 0 2 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 La función cg.in.sample que hemos definido toma un vector x y un vector de índices S y cuenta el número de índices s de S en los que x[s] es c y x[s+1] es g. Entonces, con el replicate, hemos repetido 100 veces el proceso de extraer una muestra aleatoria simple de 20 índices de myoglobin (excluyendo el último, para que sean posiciones donde empieza un par de letras) y aplicar la función cg.in.sample a myoglobin y a este vector de índices. Queremos determinar si esta muestra sigue una distribución binomial. En concreto, vamos a plantear tres casos de esta pregunta: Con probabilidad de aparición de la pareja “cg” 0.25·0.25=0.0625, que correspondería al hecho de que las dos bases aparecieran de manera equiprobable e independiente. Con probabilidad de aparición de la pareja “cg” el producto de las frecuencias relativas de las bases en la secuencia global, que correspondería al hecho de que las dos bases aparecieran de manera independiente, pero no equiprobable sino con sus probabilidades dentro de la secuencia de la mioglobina. Estimando el valor “real” de \\(p\\), lo que correspondería al hecho de que su probabilidad de aparición no tuviera nada que ver con las probabilidades individuales de sus dos bases. Empezamos con el primer caso. Calculemos las frecuencias con las que aparecen los diferentes resultados en la muestra: table(muestra) ## muestra ## 0 1 2 ## 68 26 6 Y las frecuencias esperadas con las que deberían aparecer si siguieran una distribución B(20,0.0625): round(dbinom(0:20,20,0.0625)*100,2) ## [1] 27.51 36.67 23.23 9.29 2.63 0.56 0.09 0.01 0.00 0.00 0.00 ## [12] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Las frecuencias esperadas a partir de 4 son inferiores a 5 (recordad que la primera frecuencia corresponde al 0), y además su suma no llega a 5. Por lo tanto, vamos a agrupar en una sola clase los resultados mayores o iguales que 3. La nueva tabla de frecuencias esperadas es: round(c(dbinom(0:2,20,0.0625),1-pbinom(2,20,0.0625))*100,2) ## [1] 27.51 36.67 23.23 12.59 Ahora tenemos dos opciones: o bien tomamos como resultados posibles “0”, “1”, “2” y “3 o más”, en cuyo caso contaríamos que hemos observado 0 veces este último resultado en nuestra muestra, o bien tomamos como resultados posibles “0”, “1”, y “2 o más”, que se corresponde con los valores observados. Como norma general, es recomendable usar el mayor número de clases posible. Por consiguiente, vamos a optar por la primera estrategia: 4 clases y no 3. Por lo tanto, hay que añadir a la tabla de frecuencias de la muestra un 0 en la columna correspondiente a 3 o más observaciones. freq.obs=c(table(muestra),0) prob.teor=c(dbinom(0:2,20,0.0625),1-pbinom(2,20,0.0625)) chisq.test(freq.obs,p=prob.teor)$p.value ## [1] 5.62801e-19 El p-valor es prácticamente 0, por lo que podemos concluir que la muestra no sigue una distribución B(20,0.0625): las apariciones de los pares “cg” no se explican por la aparición de sus dos bases de manera independiente y equiprobable. ¿Hubiera variado la conclusión si hubiéramos optado por solo considerar tres clases? freq.obs=table(muestra) prob.teor=c(dbinom(0:1,20,0.0625),1-pbinom(1,20,0.0625)) chisq.test(freq.obs,p=prob.teor)$p.value ## [1] 9.75924e-20 El p-valor es prácticamente el mismo, la conclusión es la misma. Pasemos al segundo caso de nuestro problema. Vamos a calcular la frecuencia relativa de “c” y “g” en la secuencia completa de la mioglobina humana y tomaremos como probabilidad \\(p\\) el producto de ambas frecuencias relativas. prop.table(table(myoglobin)) ## myoglobin ## a c g t ## 0.256840 0.217222 0.301839 0.224099 p=prod(prop.table(table(myoglobin))[2:3]) p ## [1] 0.0655661 Calculemos ahora las frecuencias esperadas tomando esta \\(p\\): round(dbinom(0:20,20,p)*100,2) ## [1] 25.76 36.15 24.10 10.15 3.03 0.68 0.12 0.02 0.00 0.00 0.00 ## [12] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Vamos a tener que agrupar de nuevo en una sola clase los resultados mayores o iguales que 3. freq.obs=c(table(muestra),0) prob.teor=c(dbinom(0:2,20,p),1-pbinom(2,20,p)) chisq.test(freq.obs,p=prob.teor)$p.value ## [1] 1.81625e-21 Obtenemos de nuevo un valor prácticamente 0: las apariciones de los pares “cg” no se explican por la aparición de sus dos bases de manera independiente. Finalmente, vamos a estimar el parámetro \\(p\\). La binomial es otra de las distribuciones no cubiertas por fitdistr, por lo que tendremos que apelar a lo que sabemos de teoría para hacerlo. Como el valor esperado de una variable aleatoria \\(X\\sim B(n,p)\\) es \\(np\\), estimaremos \\(p\\) mediante \\(\\overline{X}/n\\). De hecho, éste es el estimador máximo verosímil de \\(p\\) cuando \\(n\\) es conocida. p.estim=mean(muestra)/20 p.estim ## [1] 0.019 Repetimos el proceso: calculemos las frecuencias teóricas round(dbinom(0:20,20,p.estim)*100,2) ## [1] 68.14 26.39 4.86 0.56 0.05 0.00 0.00 0.00 0.00 0.00 0.00 ## [12] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 En este caso, hemos de agrupar los resultados en “0”, “1” y “2 o más”, para que las frecuencias teóricas sean mayores que 5. Coincide con los diferentes valores observados en la muestra. freq.obs=table(muestra) prob.teor=c(dbinom(0:1,20,p.estim),1-pbinom(1,20,p.estim)) chisq.test(freq.obs,p=prob.teor) ## ## Chi-squared test for given probabilities ## ## data: freq.obs ## X-squared = 0.05747, df = 2, p-value = 0.972 ¡Cuidado! Este p-valor no es el correcto. Hemos estimado un parámetro, pero R no lo sabe. Por lo tanto tenemos que bajar en 1 los grados de libertad y calcular el p-valor a mano, mediante \\[ P(\\chi_1^2{\\geqslant}X^2)=1-P(\\chi_1^2{\\leqslant}0.057) \\] 1-pchisq(chisq.test(freq.obs,p=prob.teor)$statistic,1) ## X-squared ## 0.810537 El p-valor es grande. Por lo tanto, no podemos rechazar la hipótesis nula de que las apariciones de “cg” en muestras aleatorias de 20 posiciones sigan una ley binomial de parámetro \\(p=0.019\\). Que es, por otro lado, lo que debería pasar si las “cg” estuvieran repartidas de manera aleatoria en la secuencia original, por lo que no podemos rechazar esto último. La conclusión es, por lo tanto, que aceptamos que las “cg” aparecen distribuidas de manera aleatoria en la secuencia de la mioglobina humana, pero tenemos evidencia estadísticamente significativa de que las “c” y las “g” que las forman no aparecen de manera independiente. 6.3 El test \\(\\chi^2\\) para distribuciones continuas El procedimiento de contraste de bondad de ajuste mediante el test \\(\\chi^2\\) para variables continuas tiene la particularidad de que es necesario un paso preliminar que consiste en definir los intervalos de clase para los que realizaremos el conteo de las frecuencias observadas. El proceso es similar al que estudiamos en la Lección ?? de la primera parte del curso para dibujar histogramas. Por lo tanto, necesitaremos definir unos intervalos de clase para el conteo de frecuencias absolutas observadas, y con las funciones cut y table obtendremos las frecuencias observadas de estas clases en la muestra de la variable continua. Para obtener los intervalos podemos seguir dos estrategias razonables: reutilizar los generados por la función hist, o dividir el rango de la variable en un número prefijado \\(k\\) de intervalos de amplitud fija. Vamos a ver en detalle un ejemplo de cada tipo. Ejemplo 6.8 Vamos a contrastar si las longitudes de los sépalos de las plantas iris recogidas en la tabla de datos iris siguen una distribución normal. Recordaréis que el Q-Q-plot de estas longitudes que mostrábamos en la Figura 6.3 mostraba evidencia de que no la siguen. Primero vamos a estimar de nuevo la media y la desviación típica de la distribución de estas longitudes. iris.sl=iris$Sepal.Length mu=fitdistr(iris.sl,&quot;normal&quot;)$estimate[1] sigma=fitdistr(iris.sl,&quot;normal&quot;)$estimate[2] round(c(mu,sigma),3) ## mean sd ## 5.843 0.825 En este ejemplo, usaremos los intervalos en los que la función hist agrupa por defecto estos datos. h=hist(iris.sl, plot=FALSE) h$breaks ## [1] 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 Ahora se nos presenta el problema de que los intervalos que definen estos puntos de corte (breaks) no cubren toda la recta real, que es el espacio muestral de una variable aleatoria normal. Así que tenemos que reemplazar los extremos de este vector de breaks por los límites del espacio muestral de la variable, que en este caso son \\(-\\infty\\) e \\(\\infty\\). breaks2=h$breaks breaks2[1]=-Inf #Cambiamos el primer elemento por -Infinito breaks2[length(breaks2)]=Inf #Cambiamos el último elemento por Infinito breaks2 ## [1] -Inf 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Inf Ahora podemos calcular las frecuencias de la muestra en los intervalos definidos por estos puntos de corte: freq.obs=table(cut(iris.sl,breaks=breaks2)) freq.obs ## ## (-Inf,4.5] (4.5,5] (5,5.5] (5.5,6] (6,6.5] (6.5,7] ## 5 27 27 30 31 18 ## (7,7.5] (7.5, Inf] ## 6 6 Ahora calcularemos las probabilidades teóricas. Para cada intervalo \\((x,y]\\) en los que hemos cortado la recta real, tenemos que calcular \\(P(x &lt; X{\\leqslant}y)= P(X{\\leqslant}y)-P( X{\\leqslant}x)\\), para lo que usaremos expresiones de la forma pnorm(y,mu,sigma)-pnorm(x,mu,sigma). Definimos dos vectores que nos den los extremos izquierdo y derecho de cada intervalo. extremo.izq=breaks2[-length(breaks2)] extremo.der=breaks2[-1] extremo.izq ## [1] -Inf 4.5 5.0 5.5 6.0 6.5 7.0 7.5 extremo.der ## [1] 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Inf Ahora podemos calcular las probabilidades teóricas y las frecuencias esperadas de todos los intervalos de golpe. La probabilidades teóricas son: probs.teor=pnorm(extremo.der,mu,sigma)-pnorm(extremo.izq,mu,sigma) probs.teor ## [1] 0.0517955 0.1016307 0.1852753 0.2365772 0.2116091 0.1325812 0.0581747 ## [8] 0.0223563 Las frecuencias esperadas son: freq.esp=probs.teor*length(iris.sl) round(freq.esp,3) ## [1] 7.769 15.245 27.791 35.487 31.741 19.887 8.726 3.353 La frecuencia esperada de la última clase es inferior a 5, así que vamos a fundirla con la penúltima y así la clase resultante tendrá una frecuencia esperada superior a 5. k=length(probs.teor) probs.teor2=c(probs.teor[1:(k-2)],sum(probs.teor[(k-1):k])) #Nuevas probabilidades teóricas freq.obs2=c(freq.obs[1:(k-2)],sum(freq.obs[(k-1):k])) #Nuevas frecuencias observadas chisq.test(freq.obs2,p=probs.teor2) ## ## Chi-squared test for given probabilities ## ## data: freq.obs2 ## X-squared = 11.12, df = 6, p-value = 0.0847 Recordemos que el p-valor obtenido no es el correcto: como hemos estimado dos parámetros, lo tenemos que calcular con una \\(\\chi^2\\) con 4 grados de libertad (dos menos de los que ha usado chisq.test): test.iris=chisq.test(freq.obs2,p=probs.teor2) 1-pchisq(test.iris$statistic,test.iris$parameter-2) ## X-squared ## 0.0252518 El p-valor es inferior a 0.05, por tanto obtenemos evidencia de que la muestra no proviene de una población normal, es decir, de que las longitudes de los sépalos de las flores iris no siguen una ley normal. Ejemplo 6.9 Vamos a repetir el estudio del ejemplo anterior, pero ahora calculando a mano los intervalos. En general, el número de intervalos debe ser suficiente para cubrir toda la forma de la distribución, pero tampoco conviene que haya muchos para evitar frecuencias esperadas pequeñas que obliguen a agrupar intervalos. Para una distribución normal se recomienda tomar entre 5 y 15 intervalos. Otra posibilidad es decidir el número de intervalos con alguna de las reglas explicadas en la Lección ?? de la primera parte del curso. En nuestro ejemplo, vamos a usar 10 intervalos. Para calcularlos, tomamos el máximo y el mínimo de las observaciones, los restamos y dividimos por el número de intervalos (y, si fuera necesario, redondearíamos adecuadamente). Ampl=(max(iris.sl)-min(iris.sl))/10 Ampl ## [1] 0.36 Los extremos de los intervalos en los que dividimos la muestra forman la secuencia que empieza en el mínimo y va sumando la amplitud hasta definir los \\(k=10\\) intervalos. Luego hay que adecuar los dos extremos para que cubran el dominio de la densidad de la distribución teórica, en nuestro caso toda la recta real. breaks=min(iris.sl)+Ampl*(0:10) breaks ## [1] 4.30 4.66 5.02 5.38 5.74 6.10 6.46 6.82 7.18 7.54 7.90 breaks2=breaks breaks2[1]=-Inf breaks2[length(breaks2)]=Inf breaks2 ## [1] -Inf 4.66 5.02 5.38 5.74 6.10 6.46 6.82 7.18 7.54 Inf Calculemos, como en el ejemplo anterior, las frecuencias observadas, las probabilidades teóricas y las frecuencias esperadas. frec.obs=table(cut(iris.sl,breaks=breaks2)) frec.obs ## ## (-Inf,4.66] (4.66,5.02] (5.02,5.38] (5.38,5.74] (5.74,6.1] (6.1,6.46] ## 9 23 14 27 22 20 ## (6.46,6.82] (6.82,7.18] (7.18,7.54] (7.54, Inf] ## 18 6 5 6 extremo.izq=breaks2[-length(breaks2)] extremo.der=breaks2[-1] prob.teor=pnorm(extremo.der,mu,sigma)-pnorm(extremo.izq,mu,sigma) frec.esp=round(prob.teor*length(iris.sl),2) frec.esp ## [1] 11.37 12.51 19.20 24.44 25.79 22.56 16.37 9.85 4.91 2.99 Agruparemos las frecuencias de los dos últimos intervalos y aplicaremos el test \\(\\chi^2\\) con el número adecuado de grados de libertad: frec.obs2=c(frec.obs[1:8], sum(frec.obs[9:10])) prob.teor2=c(prob.teor[1:8], sum(prob.teor[9:10])) test.iris.2=chisq.test(frec.obs2,p=prob.teor2) 1-pchisq(test.iris.2$statistic, test.iris.2$parameter-2) ## X-squared ## 0.0227734 El p-valor es de nuevo inferior a 0.05: volvemos a obtener evidencia significativa de que la muestra no proviene de una población normal. 6.4 El test de Kolgomorov-Smirnov El test de Kolgomorov-Smirnov (K-S) es un test genérico para contrastar la bondad de ajuste a distribuciones continuas. Se puede usar con muestras pequeñas (se suele recomendar 5 elementos como el tamaño mínimo para que el resultado sea significativo), pero la muestra no puede contener valores repetidos: si los contiene, la distribución del estadístico de contraste bajo la hipótesis nula no es la que predice la teoría sino que solo se aproxima a ella, y por lo tanto los p-valores que se obtienen son aproximados. Hay que tener en cuenta que el test K-S realiza un contraste en el que la hipótesis nula es que la muestra proviene de una distribución continua completamente especificada. Es decir, no sirve para contrastar si la muestra proviene, pongamos, de “alguna” distribución normal, sino solo para contrastar si proviene de una distribución normal con una media y una desviación típica concretas. Así pues, si queremos contrastar que la muestra proviene de alguna distribución de una familia concreta y estimamos sus parámetros a partir de la muestra, el test K-S solo nos permite rechazar o no la hipótesis de que la muestra proviene de la distribución de esa familia con exactamente esos parámetros. Por lo tanto, si el resultado es rechazar la hipótesis nula, esto no excluye que la muestra provenga de una distribución de la misma familia con otros parámetros. En la próxima sección veremos algunos tests que permiten contrastar, en general, si una muestra proviene de alguna distribución normal. La función básica para realizar el test K-S es ks.test. Su sintaxis básica para una muestra es ks.test(x, y, parámetros) donde: x es la muestra de una variable continua. y puede ser un segundo vector, y entonces se contrasta si ambos vectores han sido generados por la misma distribución continua, o el nombre de la función de distribución (empezando con p) que queremos contrastar, entre comillas; por ejemplo &quot;pnorm&quot; para la distribución normal. Los parámetros de la función de distribución si se ha especificado una; por ejemplo mean=0, sd=1 para una distribución normal estándar. Ejemplo 6.10 Efectuemos el test de Kolmogorov-Smirnov para contrastar si las longitudes de sépalos de flores iris siguen una distribución normal de media y desviación típica sus estimaciones máximo verosímiles a partir la muestra iris.sl. Recordemos que tenemos guardados de los dos últimos ejemplos los valores de estas estimaciones en las variables mu y sigma: round(c(mu,sigma),3) ## mean sd ## 5.843 0.825 ks.test(iris.sl, &quot;pnorm&quot;, mean=mu, sd=sigma) ## Warning in ks.test(iris.sl, &quot;pnorm&quot;, mean = mu, sd = sigma): ties should ## not be present for the Kolmogorov-Smirnov test ## ## One-sample Kolmogorov-Smirnov test ## ## data: iris.sl ## D = 0.08945, p-value = 0.181 ## alternative hypothesis: two-sided Obtenemos un p-valor de 0.181, que no nos permite rechazar la hipótesis de que siguen una ley N(5.843, 0.825). Pero R nos avisa de que hay empates. ¿Hay muchos? Vamos a calcular su frecuencia. La función unique aplicada a un vector nos da el vector de sus elementos sin repeticiones. De esta manera podemos saber cuántos elementos diferentes hay en un vector, y por consiguiente también cuántas repeticiones. length(unique(iris.sl)) ## [1] 35 1-length(unique(iris.sl))/length(iris.sl) ## [1] 0.766667 Por tanto, el vector (de 150 entradas) de longitudes de sépalos solo tiene 35 valores diferentes. El resto, un 76.67%, son valores repetidos. Hay muchos empates, y el resultado de este test en este caso es poco fiable Como hemos comentado, el test K-S también se puede usar para contrastar si dos muestras se han obtenido de poblaciones con la misma distribución continua. Para hacerlo, se ha de aplicar la función ks.test a las dos muestras. Ejemplo 6.11 La tabla de datos Salaries del paquete car contiene información sobre los sueldos de 397 profesores de una universidad norteamericana en el curso 2008-09. Démosle un vistazo. library(car) str(Salaries) ## &#39;data.frame&#39;: 397 obs. of 6 variables: ## $ rank : Factor w/ 3 levels &quot;AsstProf&quot;,&quot;AssocProf&quot;,..: 3 3 1 3 3 2 3 3 3 3 ... ## $ discipline : Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ yrs.since.phd: int 19 20 4 45 40 6 30 45 21 18 ... ## $ yrs.service : int 18 16 3 39 41 6 23 45 20 18 ... ## $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 2 2 2 2 2 1 ... ## $ salary : int 139750 173200 79750 115000 141500 97000 175000 147765 119250 129000 ... La variable sex nos da el sexo del profesor y la variable salary su sueldo anual en dólares. Queremos contrastar si los sueldos de hombres y mujeres siguen la misma distribución. Para ello, vamos a suponer que provienen de distribuciones continuas y usaremos el test K-S. Primero miraremos si hay muchos empates. sal.female=Salaries[Salaries$sex==&quot;Female&quot;,]$salary #Salarios de mujeres sal.male=Salaries[Salaries$sex==&quot;Male&quot;,]$salary #Salarios de hombres 1-length(unique(sal.female))/length(sal.female) #Proporción de salarios de mujeres repetidos ## [1] 0.0512821 1-length(unique(sal.male))/length(sal.male) #Proporción de salarios de hombres repetidos ## [1] 0.0502793 1-length(unique(Salaries$salary))/length(Salaries$salary) #Proporción global de salarios repetidos ## [1] 0.0654912 Las repeticiones en cada lista significan alrededor del 5% de los datos, y en total un 6.5%. No son muchas, así que vamos a arriesgarnos con el test K-S. ks.test(sal.male,sal.female) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: sal.male and sal.female ## D = 0.2472, p-value = 0.0271 ## alternative hypothesis: two-sided El p-valor pequeño nos permite rechazar que los salarios de hombres y mujeres sigan la misma distribución. Pero no nos paremos aquí. Si dibujamos un boxplot (véase la Figura 6.5) de los salarios según el sexo, observaremos que los sueldos de los hombres tienen mayor mediana y variabilidad que los de las mujeres, incluyendo algunos valores atípicos grandes (¿el rector y otros altos cargos académicos?). boxplot(salary~sex, data=Salaries, main=&quot;&quot;) Figura 6.5: Boxplot de sueldos según el sexo en la tabla de datos Salaries Si cancelamos este efecto, estandarizando las muestras, ¿siguen saliendo distribuciones diferentes? ks.test(scale(sal.male),scale(sal.female)) ## Warning in ks.test(scale(sal.male), scale(sal.female)): p-value will be ## approximate in the presence of ties ## ## Two-sample Kolmogorov-Smirnov test ## ## data: scale(sal.male) and scale(sal.female) ## D = 0.139, p-value = 0.505 ## alternative hypothesis: two-sided Al estandarizar, ya no tenemos evidencia de que provengan de distribuciones diferentes. Es decir, podemos aceptar que sus valores tipificados siguen la misma distribución. 6.5 Tests de normalidad Existen algunos tests específicos de normalidad que permiten contrastar si una muestra proviene de alguna distribución normal. El más conocido es el test de normalidad de Kolmogorov-Smirnov-Lilliefors (K-S-L). Se trata de una variante del test K-S, y se puede realizar aplicando a la muestra la función lillie.test del paquete nortest. Vamos a usar el test K-S-L para contrastar si las longitudes de los sépalos de las iris siguen una ley normal. library(nortest) iris.sl=iris$Sepal.Length lillie.test(iris.sl) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: iris.sl ## D = 0.08865, p-value = 0.00579 El p-valor es muy pequeño, y nos permite rechazar que la muestra provenga de una población normal. La ventaja del test K-S-L es que es muy conocido, ya que es una variante del K-S (incluso usa el mismo estadístico), pero tiene un inconveniente: aunque es muy sensible a las diferencias entre la muestra y la distribución teórica alrededor de sus valores medios, le cuesta detectar diferencias prominentes en un extremo u otro de la distribución. Esto afecta su potencia. Por ejemplo, sabemos que una t de Student se parece bastante a una normal estándar, pero su densidad es algo más aplanada y hace que en los dos extremos esté por encima de la de la normal. Al test K-S-L le cuesta detectar esta discrepancia, como podemos ver en el siguiente ejemplo: set.seed(100) x=rt(50,3) #Una muestra de una t de Student con 3 g.l. lillie.test(x) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: x ## D = 0.1033, p-value = 0.201 Este inconveniente del test K-S-L lo resuelve el test de normalidad de Anderson-Darling (A-D). Para realizarlo podemos usar la función ad.test del paquete nortest. Encontraréis los detalles del estadístico que usa en la Ayuda de la función. ad.test(iris.sl) ## ## Anderson-Darling normality test ## ## data: iris.sl ## A = 0.8892, p-value = 0.0225 De nuevo obtenemos un p-valor muy pequeño. Veamos ahora que este test sí que detecta que la muestra anterior de una t de Student con 3 grados de libertad no proviene de una normal: set.seed(100) x=rt(50,3) ad.test(x) ## ## Anderson-Darling normality test ## ## data: x ## A = 1.166, p-value = 0.00433 Un inconveniente común a los tests K-S-L y A-D es que, si bien pueden usarse con muestras pequeñas (pongamos de más de 5 elementos), se comportan mal con muestras grandes, de varios miles de elementos. En muestras de este tamaño, cualquier pequeña divergencia de la normalidad se magnifica y en estos dos tests aumenta la probabilidad de errores de tipo I. Un test que resuelve este problema es el de Shapiro-Wilk (S-W), implementado en la función shapiro.test de la instalación básica de R. Este test es importante, porque un experimento reciente ha mostrado evidencia significativa de que su potencia es mayor que la de los tests anteriores.6 De nuevo, los detalles del estadístico que usa los encontraréis en la Ayuda de la función. shapiro.test(iris.sl) ## ## Shapiro-Wilk normality test ## ## data: iris.sl ## W = 0.9761, p-value = 0.0102 set.seed(100) x=rt(50,3) shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.8949, p-value = 0.000329 Un último inconveniente que afecta a todos los tests explicados hasta ahora es el de los empates. Sus estadísticos tienen las distribuciones que se usan para calcular los p-valores cuando la muestra no tiene datos repetidos, y por lo tanto, si hay muchos, el p-valor puede no tener ningún significado. De los tres, el menos sensible a repeticiones es el S-W, pero si hay muchas es conveniente usar un test que no sea sensible a ellas, como por ejemplo el test omnibus de D’Agostino-Pearson. Este test se encuentra implementado en la función dagoTest del paquete fBasics, y lo que hace es cuantificar lo diferentes que son la asimetría y la curtosis de la muestra (dos parámetros estadísticos relacionados con la forma de la gráfica de la función de densidad muestral) respecto de los esperados en una distribución normal, y resume esta discrepancia en un p-valor con el significado usual. library(fBasics) dagoTest(iris.sl) ## ## Title: ## D&#39;Agostino Normality Test ## ## Test Results: ## STATISTIC: ## Chi2 | Omnibus: 5.7356 ## Z3 | Skewness: 1.5963 ## Z4 | Kurtosis: -1.7853 ## P VALUE: ## Omnibus Test: 0.05682 ## Skewness Test: 0.1104 ## Kurtosis Test: 0.07421 ## ## Description: ## Sat Apr 20 10:14:58 2019 by user: El p-valor relevante es el del “Omnibus test”, en este caso 0.0568 cae en la zona de penumbra. Queremos hacer una última advertencia en esta sección. Aunque los tests que hemos explicado se pueden aplicar a muestras pequeñas, es muy difícil rechazar la normalidad de una muestra muy pequeña. Por ejemplo, una muestra de 10 valores escogidos con distribución uniforme entre 0 y 5 pasa holgadamente todos los tests de normalidad (salvo el de D’Agostino-Pearson, que requiere una muestra de al menos 20 elementos): set.seed(100) x=runif(10,0,5) lillie.test(x) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: x ## D = 0.1459, p-value = 0.79 ad.test(x) ## ## Anderson-Darling normality test ## ## data: x ## A = 0.1663, p-value = 0.912 shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.9803, p-value = 0.967 dagoTest(x) ## Error in .omnibus.test(x): sample size must be at least 20 6.6 Guía rápida qqPlot del paquete car, sirve para dibujar un Q-Q-plot de una muestra contra una distribución teórica. Sus parámetros principales son: distribution: el nombre de la familia de distribuciones, entre comillas. Los parámetros de la distribución: mean para la media, sd para la desviación típica, df para los grados de libertad, etc. Los parámetros usuales de plot. chisq.test sirve para realizar tests \\(\\chi^2\\) de bondad de ajuste. Sus parámetros principales son: p: el vector de probabilidades teóricas. rescale.p: igualado a TRUE, indica que los valores de p no son probabilidades, sino sólo proporcionales a las probabilidades. simulate.p.value: igualado a TRUE, R calcula el p-valor mediante simulaciones. B: en este último caso, permite especificar el número de simulaciones. ks.test realiza el test de Kolmogorov-Smirnov. Tiene dos tipos de uso: ks.test(x,y): contrasta si los vectores x e y han sido generados por la misma distribución continua. ks.test(x, &quot;distribución&quot;, parámetros): contrasta si el vector x ha sido generado por la distribución especificada, que se ha de indicar con el nombre de la función de distribución de R (la que empieza con p). lillie.test del paquete nortest, realiza el test de normalidad de Kolmogorov-Smirnov-Lilliefors. ad.test del paquete nortest, realiza el test de normalidad de Anderson-Darling. shapiro.test, realiza el test de normalidad de Shapiro-Wilk. dagoTest del paquete fBasics, realiza el test ómnibus de D’Agostino-Pearson. 6.7 Ejercicios Modelo de test (1) Un determinado experimento tiene cinco resultados posibles: A, B, C, D, E. Lo repetimos un cierto número de veces y obtenemos 65 veces el resultado A, 95 veces el resultado B, 87 veces el resultado C, 70 veces el resultado D y 193 veces el resultado E. Realizad un test \\(\\chi^2\\) para contrastar si los resultados A, B, C y D tienen la misma probabilidad y E tiene el doble de probabilidad que cada uno de los otros resultados. Dad el p-valor del contraste (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decid (contestando SI, en mayúsculas y sin acento, o NO) si tendríamos que rechazar la hipótesis nula con un nivel de significación \\(\\alpha=0.05\\). Tenéis que dar las respuestas en este orden y separadas por un único espacio en blanco. (2) Queremos contrastar si una determinada variable sigue una distribución de Poisson. Hemos efectuado algunas observaciones y hemos obtenido 10 veces el resultado 0, 32 veces el resultado 1, 18 veces el resultado 2, 19 veces el resultado 3 y 6 veces el resultado 4. Tenéis que calcular el estimador máximo verosímil del parámetro \\(\\lambda\\) de una variable de Poisson que haya generado estas observaciones (redondeado a 3 cifras decimales, y sin ceros innecesarios a la derecha), calcular el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del test \\(\\chi^2\\) para determinar si la muestra sigue alguna distribución de Poisson habiendo estimado como su parámetro \\(\\lambda\\) este valor redondeado, y decir (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.1\\), tendríamos que rechazar la hipótesis nula de que esta muestra proviene de una variable aleatoria de Poisson. Dad las tres respuestas en este orden y separadas por un único espacio en blanco. (3) Queremos contrastar si una cierta variable sigue una distribución normal. Hemos efectuado 150 observaciones y hemos obtenido 9 veces un valor dentro de ]0,3], 27 veces un valor dentro de ]3,6], 51 veces un valor dentro de ]6,9], 46 veces un valor dentro de ]9,12] y 17 veces un valor dentro de ]12,15]. Tenéis que: calcular los estimadores máximo verosímiles del parámetro \\(\\mu\\) y del parámetro \\(\\sigma\\) de una variable normal que haya generado estas observaciones (ambos redondeados a 2 cifras decimales y sin ceros innecesarios a la derecha): calcular el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del test \\(\\chi^2\\) para contrastar si la muestra sigue alguna distribución normal, empleando estos valores estimados redondeados de los parámetros que habéis dado para especificar la distribución teórica; y decir (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.05\\), tendríamos que rechazar la hipótesis nula de que esta muestra proviene de una variable aleatoria normal. Dad las cuatro respuestas en este orden y separadas por un único espacio en blanco. (4) Generad una muestra aleatoria x de 25 valores de una distribución \\(\\chi^2\\) con 10 grados de libertad, fijando antes set.seed(2014), y aplicad el test de Kolmogorov-Smirnov para contrastar si x proviene de una distribución N(10,3.16). Dad el p-valor del test (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decid (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.05\\), tendríamos que rechazar la hipótesis nula de que esta muestra proviene de esta distribución. Tenéis que dar las dos respuestas en este orden y separadas por un único espacio en blanco. (5) Queremos contrastar si la muestra siguiente sigue una distribución normal: 4.6, 0.97, 0.3, 1.11, 2.16, 15.52, 1.13, 0.17, 0.64, 2.00. Dad el p-valor (redondeado a 3 cifras decimales y sin ceros innecesarios a la derecha) del test de Kolmogorov-Smirnov-Lilliefors para esta muestra y decid (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.05\\), tendríamos que rechazar la hipótesis nula de que esta muestra sigue una distribución normal. Tenéis que dar las dos respuestas en este orden y separadas por un único espacio en blanco. (6) Generad una muestra aleatoria x de 15 valores de una distribución normal con \\(\\mu=2\\) y \\(\\sigma=0.8\\) fijando antes set.seed(2014), y una muestra aleatoria y de 25 valores de una distribución exponencial de parámetro \\(1/\\lambda=0.5\\) fijando antes set.seed(1007). Aplicad el test de Kolmogorov-Smirnov para contrastar si x e y provienen de una misma distribución continua. Tenéis que dar el p-valor del test (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decir (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.05\\), tendríamos que rechazar la hipótesis nula de que estas dos muestras provienen de la misma distribución continua. Dad las dos respuestas en este orden y separadas por un único espacio en blanco. Respuestas al test (1) 0.02 SI Nosotros lo hemos calculado con x=c(65,95,87,70,193) round(chisq.test(x,p=c(1,1,1,1,2),rescale.p=T)$p.value,3) ## [1] 0.02 (2) 1.753 0.064 SI Nosotros lo hemos calculado con x=c(10,32,18,19,6) lambda=round(fitdistr(rep(0:4,x),&quot;poisson&quot;)$estimate,3) #Estimamos la lambda c(dpois(0:3,lambda),1-ppois(3,lambda))*85 #Frecuencias esperadas ## [1] 14.7265 25.8156 22.6274 13.2219 8.6085 ChiT2=chisq.test(x,p=c(dpois(0:3,lambda),1-ppois(3,lambda))) #Test p.valor2=1-pchisq(ChiT2$statistic,3) #p-valor correcto c(lambda,round(p.valor2,3)) ## lambda X-squared ## 1.753 0.064 (3) 8.2 3.18 0.692 NO Nosotros lo hemos calculado con x=c(9,27,51,46,17) sum(x) ## [1] 150 muestra=rep(c(1.5,4.5,7.5,10.5,13.5),x) fitdistr(muestra,&quot;normal&quot;) ## mean sd ## 8.200000 3.182766 ## (0.259872) (0.183757) mu=round(fitdistr(muestra,&quot;normal&quot;)$estimate[1],2) sigma=round(fitdistr(muestra,&quot;normal&quot;)$estimate[2],2) left=c(-Inf,3,6,9,12) right=c(3,6,9,12,Inf) probs=pnorm(right,mu,sigma)-pnorm(left,mu, sigma) ChiT3=chisq.test(x,p=probs) p.valor3=1-pchisq(ChiT3$statistic,2) c(mu,sigma,round(p.valor3,3)) ## mean sd X-squared ## 8.200 3.180 0.692 (4) 0.313 NO Nosotros lo hemos calculado con set.seed(2014) x=rchisq(25,10) round(ks.test(x,&quot;pnorm&quot;,mean=10,sd=3.16)$p.value,3) ## [1] 0.313 (5) 0.001 SI Nosotros lo hemos calculado con x=c(4.6, 0.97, 0.3, 1.11, 2.16, 15.52, 1.13, 0.17, 0.64, 2.00) round(lillie.test(x)$p.value,3) ## [1] 0.001 (6) 0.117 NO Nosotros lo hemos calculado con set.seed(2014) x=rnorm(15,2,0.8) set.seed(1007) y=rexp(25,0.5) round(ks.test(x,y)$p.value,3) ## [1] 0.117 A las repeticiones se las suele llamar empates, ties en inglés, porque la función de distribución acumulada muestral ordena los datos y las repeticiones producen empates en las posiciones de valores sucesivos.↩ Lo confesamos, hemos elegido la semilla de aleatoriedad no porque seamos fans de Scarlatti sino para que la muestra obtenida solo contenga ceros, unos y doses, lo que, como veréis, motivará una pequeña discusión sobre qué clases tomar.↩ Véase N. M. Razali, Y. B. Wah, “Power comparisons of Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests.” S. Stat. Model. Anal. 2 (2011), pp. 21–33.↩ "],
["chap-indep.html", "Lección 7 Contrastes de independencia y homogeneidad 7.1 Tablas de contingencia 7.2 Contraste de independencia 7.3 Contraste de homogeneidad 7.4 Potencia de un contraste \\(\\chi^2\\) 7.5 Guía rápida 7.6 Ejercicios", " Lección 7 Contrastes de independencia y homogeneidad El test \\(\\chi^2\\) explicado en la lección anterior permite contrastar, en situaciones adecuadas, si una muestra proviene de una determinada distribución y por lo tanto también si una muestra sigue la misma distribución que otra muestra. Como, en última instancia, la independencia de dos variables cualitativas se puede describir en términos de igualdades de probabilidades, el test \\(\\chi^2\\) también nos permitirá contrastar si dos variables cualitativas son independientes, tanto en el sentido de que las probabilidades conjuntas sean el producto de las probabilidades marginales (con un contraste de independencia) como en el sentido de que las distribuciones condicionadas de una respecto de los valores de la otra sean todas iguales (con un contraste de homogeneidad). Aunque, como veremos, los contrastes de independencia y homogeneidad son idénticos desde el punto de vista matemático e incluso utilizan el mismo estadístico \\(\\chi^2\\) y la misma definición de p-valor, provienen de diseños experimentales diferentes: En un contraste de independencia se toma una muestra transversal de la población, es decir, se selecciona al azar una cierta cantidad de individuos de la población, se observan las dos variables sobre cada uno de ellos, y se contrasta si las probabilidades conjuntas son iguales al producto de las probabilidades marginales de cada variable. Formalmente, si \\(X\\) e \\(Y\\) son las dos variables, se contrasta si para cada par de posibles valores \\(x\\) de \\(X\\) e \\(y\\) de \\(Y\\) se tiene que \\[ P(X=x,Y=y)=P(X=x)\\cdot P(Y=y) \\] o si por el contrario hay algún par de valores \\(x\\), \\(y\\) para los que esta igualdad sea falsa. En un contraste de homogeneidad se escoge una de las variables y para cada uno de sus posibles valores se toma una muestra aleatoria, de tamaño prefijado, de individuos con ese valor para esa variable; su unión forma una muestra estratificada en el sentido de la Sección 2.1. A continuación, se observa sobre cada uno de estos individuos la otra variable. En esta situación contrastamos si la distribución de probabilidades de la segunda variable es la misma en los diferentes estratos definidos por los niveles de la primera variable. Formalmente, si \\(Y\\) es la variable que usamos en primer lugar para clasificar los individuos de la población y tomar una muestra de cada clase, con posibles valores \\(y_1,\\ldots,y_k\\), y \\(X\\) es la variable que medimos en segundo lugar sobre los individuos escogidos, se contrasta si, para cada posible valor \\(x\\) de \\(X\\), \\[ P(X=x|Y=y_1)=P(X=x|Y=y_2)=\\cdots=P(X=x|Y=y_k) \\] o si por el contrario existen \\(x\\), \\(y_i\\), \\(y_j\\) tales que \\(P(X=x|Y=y_i)\\neq P(X=x|Y=y_j)\\). En ambos contrastes, la hipótesis nula es que las variables son independientes, bajo una u otra formulación matemática, y la hipótesis alternativa es que son dependientes (o que hay asociación entre ellas). La hipótesis nula se rechaza si se obtiene evidencia que hace inverosímiles las igualdades de probabilidades que se contrastan. Para ilustrar esta lección, hemos generado una muestra aleatoria de cadenas formadas por las bases “a”, “c”, “g” y “t”. En concreto, hemos generado cadenas de longitud 100 de tres tipos: A, B y C. Estos tipos se distinguen por los vectores de probabilidades que han determinado las frecuencias de las cuatro bases en las secuencias. Queremos investigar si hay relación entre el tipo (A, B o C) de una cadena, y la base de frecuencia máxima en ella. Los datos y el método de generación se encuentran en el repositorio siguiente: https://github.com/biocom-uib/Experimento-Cadenas. Este directorio contiene: El fichero LeemeGeneracionDatos, que explica cómo se han generado las muestras. El fichero MuestraTotalBases.txt, que contiene una tabla de datos de 10000 observaciones de las dos variables siguientes sobre cadenas: el tipo, que es un factor con los niveles A, B y C, y max.frec, que es otro factor que indica qué base tiene mayor frecuencia en la cadena. Este fichero es de formato texto, con una primera fila con el nombre de las variables y sus columnas separadas por comas El código siguiente carga la tabla de datos, comprueba que no ha habido problemas, y extrae tres subtablas, una para cada tipo de cadena. Observad cómo se hace para cargar una tabla de datos de un url que empieza con https, por ejemplo en un repositorio GitHub: library(RCurl) datos=getURL(&quot;https://raw.githubusercontent.com/biocom-uib/Experimento-Cadenas/master/MuestraTotalBases.txt&quot;) poblacion=read.table(text=datos,header=TRUE,sep=&quot;,&quot;) str(poblacion) ## &#39;data.frame&#39;: 10000 obs. of 2 variables: ## $ tipo : Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 2 1 1 3 1 1 1 3 2 2 ... ## $ max.frec: Factor w/ 4 levels &quot;a&quot;,&quot;c&quot;,&quot;g&quot;,&quot;t&quot;: 3 1 2 4 4 1 1 3 2 2 ... head(poblacion) ## tipo max.frec ## 1 B g ## 2 A a ## 3 A c ## 4 C t ## 5 A t ## 6 A a poblacionA=subset(poblacion,tipo==&quot;A&quot;) poblacionB=subset(poblacion,tipo==&quot;B&quot;) poblacionC=subset(poblacion,tipo==&quot;C&quot;) 7.1 Tablas de contingencia Ya estudiamos en la Lección ?? de la primera parte las tablas de contingencia. En esta sección vamos a repasar y ampliar algunas de las funciones de R para el manejo de esta clase de tablas. La tabla de contingencia de frecuencias absolutas conjuntas de las dos variables del data frame poblacion se calcula de la manera siguiente: tabla=table(poblacion$tipo,poblacion$max.frec) tabla ## ## a c g t ## A 1027 1003 998 998 ## B 1259 1297 209 232 ## C 245 602 1453 677 Su tabla de frecuencias relativas conjuntas en el total de la muestra, en términos de proporciones (tantos por uno), es: prop.table(tabla) ## ## a c g t ## A 0.1027 0.1003 0.0998 0.0998 ## B 0.1259 0.1297 0.0209 0.0232 ## C 0.0245 0.0602 0.1453 0.0677 Para añadir las distribuciones marginales de la tabla de contingencia (o márgenes de la tabla), se añade una nueva fila con las sumas de cada columna y una nueva columna con las sumas de cada fila. Con R, esto se puede llevar a cabo fácilmente con la función addmargins. Su sintaxis básica es addmargins(tabla, margin=..., FUN=...) donde: tabla es una table. margin es un parámetro que puede tomar los valores siguientes: 1 si queremos una nueva fila con las marginales de cada columna. 2 si queremos una nueva columna con las marginales de cada fila. c(1,2), que es el valor por defecto para tablas de contingencia bidimensionales (y por lo tanto no hace falta especificarlo), si queremos las marginales por filas y por columnas.7 FUN es la función que se aplica a las filas o columnas para obtener el valor marginal. Por defecto es la suma, que es la función que nos interesa en esta lección, y por tanto tampoco hace falta especificarlo. El resultado es otro objeto de la clase table al que se le han añadido una o varias filas o columnas. Éstas contienen los márgenes resultantes de aplicar la función indicada por FUN. La etiqueta de las nuevas filas o columnas es la función que se aplica. Por ejemplo, para obtener las tablas marginales completas de frecuencias absolutas y relativas en nuestro ejemplo, haríamos: addmargins(tabla) ## ## a c g t Sum ## A 1027 1003 998 998 4026 ## B 1259 1297 209 232 2997 ## C 245 602 1453 677 2977 ## Sum 2531 2902 2660 1907 10000 addmargins(prop.table(tabla)) ## ## a c g t Sum ## A 0.1027 0.1003 0.0998 0.0998 0.4026 ## B 0.1259 0.1297 0.0209 0.0232 0.2997 ## C 0.0245 0.0602 0.1453 0.0677 0.2977 ## Sum 0.2531 0.2902 0.2660 0.1907 1.0000 También podemos calcular la tabla de proporciones por filas y con su marginal por filas comprobar que efectivamente la suma de cada fila es 1: addmargins(prop.table(tabla,margin=1),margin=2) ## ## a c g t Sum ## A 0.2550919 0.2491307 0.2478887 0.2478887 1.0000000 ## B 0.4200868 0.4327661 0.0697364 0.0774107 1.0000000 ## C 0.0822976 0.2022170 0.4880752 0.2274101 1.0000000 Y viceversa, podemos calcular la tabla de proporciones por columnas y con su marginal por columnas comprobar que efectivamente la suma de cada columna es 1: addmargins(prop.table(tabla,margin=2),margin=1) ## ## a c g t ## A 0.4057685 0.3456237 0.3751880 0.5233351 ## B 0.4974318 0.4469331 0.0785714 0.1216571 ## C 0.0967997 0.2074431 0.5462406 0.3550079 ## Sum 1.0000000 1.0000000 1.0000000 1.0000000 Observad que el significado de margin en addmargins es diferente de, por ejemplo, en prop.table o en apply: en estas dos últimas instrucciones indica la dimensión en la que calculamos las proporciones o aplicamos la función, mientras que en addmargins indica la dimensión en la que añadimos el margen, que, por lo tanto, se calcula aplicando la función en la otra dimensión. Si sólo nos interesa la fila o la columna de marginales, podemos usar las instrucciones colSums y rowSums, que suman una tabla por columnas y por filas, respectivamente. Por ejemplo, para obtener los vectores de marginales por columnas y por filas, respectivamente, podríamos entrar: colSums(tabla) ## a c g t ## 2531 2902 2660 1907 rowSums(tabla) ## A B C ## 4026 2997 2977 También podemos obtener estos márgenes extrayendo los márgenes de la tabla con márgenes, obtenida aplicando addmargins a la tabla original, por medio de las instrucciones usuales para extraer filas y columnas. addmargins(tabla)[&quot;Sum&quot;,-dim(addmargins(tabla))[2]] ## a c g t ## 2531 2902 2660 1907 addmargins(tabla)[-dim(addmargins(tabla))[1],&quot;Sum&quot;] ## A B C ## 4026 2997 2977 Observad, por ejemplo, la construcción addmargins(tabla)[&quot;Sum&quot;,-dim(addmargins(tabla))[2]] Con addmargins(tabla)[&quot;Sum&quot;,] obtendríamos la fila Sum de la tabla con las marginales, incluyendo la última entrada, correspondiente a la columna Sum. Por lo tanto, hay que eliminarla: como dim(addmargins(tabla))[2] es el número de columnas de la tabla addmargins(tabla), es decir, la longitud del vector addmargins(tabla)[&quot;Sum&quot;,], la última entrada (correspondiente a la última columna) se puede eliminar especificando -dim(addmargins(tabla))[2] en las columnas al extraer la fila Sum. 7.2 Contraste de independencia El contraste de independencia para tablas de contingencia bidimensionales consiste en decidir si las dos variables de la tabla tienen distribuciones independientes, es decir, si la distribución de probabilidades conjunta es igual al producto de las probabilidades marginales. En nuestro ejemplo, queremos decidir si podemos aceptar que las variables tipo y max.frec son independientes o si por el contrario hay evidencia de que la distribución de las bases de máxima frecuencia depende del tipo de cadena. Vamos a extraer una muestra aleatoria simple de la población y observar los valores de las dos variables. En concreto, seleccionaremos una muestra transversal de 150 filas, al azar y con reposición, de entre las 10000 filas del data frame poblacion. El código es el siguiente (fijamos la semilla de aleatoriedad para que sea reproducible): set.seed(42) n=150 indices.muestra=sample(1:10000, size=n, replace=TRUE) muestra.test.indep= poblacion[indices.muestra, ] #Las filas que forman la muestra Ahora calculamos la tabla de contingencia con sus marginales. tabla.ind=table(muestra.test.indep$tipo, muestra.test.indep$max.frec) tabla.ind ## ## a c g t ## A 15 10 16 17 ## B 19 18 0 4 ## C 3 10 25 13 tabla.ind.marg=addmargins(tabla.ind) tabla.ind.marg ## ## a c g t Sum ## A 15 10 16 17 58 ## B 19 18 0 4 41 ## C 3 10 25 13 51 ## Sum 37 38 41 34 150 Extraemos sus dos márgenes. Las frecuencias marginales de las filas: frec.abs.tipo=tabla.ind.marg[-dim(tabla.ind.marg)[1],&quot;Sum&quot;] frec.abs.tipo ## A B C ## 58 41 51 Las frecuencias marginales de las columnas: frec.abs.max.frec=tabla.ind.marg[&quot;Sum&quot;,-dim(tabla.ind.marg)[2]] frec.abs.max.frec ## a c g t ## 37 38 41 34 El test de independencia usa las frecuencias absolutas esperadas bajo la hipótesis nula de independencia, que se obtienen, para cada celda (i,j), multiplicando la frecuencia marginal de la fila i por la de la columna j y dividiendo por el tamaño de la muestra. En nuestro ejemplo estas frecuencias esperadas son \\[ \\begin{array}{l|cccc} &amp; \\mbox{a} &amp; \\mbox{c} &amp; \\mbox{g} &amp; \\mbox{t} \\\\ \\hline \\mbox{A} &amp; 58\\cdot 37/150 &amp; 58\\cdot 38/150 &amp; 58\\cdot 41/150 &amp; 58\\cdot 34/150 \\\\ \\mbox{B} &amp; 41\\cdot 37/150 &amp; 41\\cdot 38/150 &amp; 41\\cdot 41/150 &amp; 41\\cdot 34/150 \\\\ \\mbox{C} &amp; 51\\cdot 37/150 &amp; 51\\cdot 38/150 &amp; 51\\cdot 41/150 &amp; 51\\cdot 34/150 \\end{array} \\] y podemos obtenerlas fácilmente mediante un producto de matrices: \\[ \\frac{1}{150}\\cdot\\left(\\begin{array}{c} 58 \\\\ 41 \\\\ 51\\end{array}\\right)\\cdot \\big( 37 ,38 , 41 , 34\\big). \\] Por lo tanto, con R obtenemos esta tabla de frecuencias esperadas de la manera siguiente: frec.esperadas=frec.abs.tipo%*%t(frec.abs.max.frec)/n frec.esperadas ## a c g t ## [1,] 14.3067 14.6933 15.8533 13.14667 ## [2,] 10.1133 10.3867 11.2067 9.29333 ## [3,] 12.5800 12.9200 13.9400 11.56000 Aunque vayamos a realizar el test de independencia con una función de R, es necesario comprobar que todas estas frecuencias esperadas (o al menos la gran mayoría) son mayores o iguales que 5, por lo que no podemos evitar este cálculo. En este caso vemos que se cumple esta condición. Si queremos realizar el test \\(\\chi^2\\) de independencia a mano, podemos calcular el estadístico de forma directa con chi2.estadistico=sum((tabla.ind-frec.esperadas)^2/frec.esperadas) chi2.estadistico ## [1] 47.1842 y el p-valor del contraste, con p.valor=1-pchisq(chi2.estadistico,df=(dim(tabla.ind)[1]-1)*(dim(tabla.ind)[2]-1)) p.valor ## [1] 1.71932e-08 Para realizar el test \\(\\chi^2\\) de independencia con R, es suficiente aplicar la función chisq.test a la tabla de contingencia de frecuencias absolutas: chisq.test(tabla.ind) ## ## Pearson&#39;s Chi-squared test ## ## data: tabla.ind ## X-squared = 47.18, df = 6, p-value = 1.72e-08 Como el p-valor es muy pequeño, podemos rechazar la hipótesis de que las variables objeto de estudio sean independientes: hemos obtenido evidencia estadísticamente significativa de que la distribución de las bases de frecuencia máxima sí que depende del tipo de cadena. Si algunas frecuencias absolutas esperadas fueran inferiores a 5, la aproximación del p-valor por una distribución \\(\\chi^2\\) podría no ser adecuada. En este caso, al ser las variables cualitativas, no podemos recurrir al agrupamiento de valores consecutivos, puesto que no tienen orden. Si se da esta situación, lo mejor es recurrir a simular el p-valor usando el parámetro simulate.p.value=TRUE. Por ejemplo, consideremos la situación siguiente: set.seed(300) n2=100 indices.muestra2=sample(1:10000,size=n2,replace=TRUE) muestra.test.indep2= poblacion[indices.muestra2,] tabla.ind2=table(muestra.test.indep2$tipo,muestra.test.indep2$max.frec) tabla.ind2 ## ## a c g t ## A 11 12 10 9 ## B 10 18 1 1 ## C 4 7 13 4 Si aplicamos a esta tabla la función chisq.test, obtenemos: chisq.test(tabla.ind2) ## Warning in chisq.test(tabla.ind2): Chi-squared approximation may be ## incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: tabla.ind2 ## X-squared = 23.63, df = 6, p-value = 0.00061 ¡Vaya! Veamos la tabla de frecuencias esperadas: frec.abs.tipo2=rowSums(tabla.ind2) frec.abs.max.frec2=colSums(tabla.ind2) frec.esperadas2=frec.abs.tipo2%*%t(frec.abs.max.frec2)/n2 frec.esperadas2 ## a c g t ## [1,] 10.5 15.54 10.08 5.88 ## [2,] 7.5 11.10 7.20 4.20 ## [3,] 7.0 10.36 6.72 3.92 Hay frecuencias esperadas inferiores a 5. Por lo tanto, lo recomendable es calcular el p-valor del test \\(\\chi^2\\) de independencia mediante simulaciones. Pero ahora tenemos que ir con cuidado en una cosa: hemos fijado la semilla de aleatoriedad para obtener una muestra de cadenas con frecuencias esperadas inferiores a 5. Lo recomendable es reiniciar esta semilla a un valor aleatorio con set.seed(NULL). set.seed(NULL) chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.00059988 chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.00119976 chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.00039992 chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.00039992 El p-valor es sistemáticamente pequeño, lo que nos permite rechazar la hipótesis de que las variables son independientes. 7.3 Contraste de homogeneidad Como ya hemos dicho, la diferencia entre el contraste de homogeneidad y el de independencia está en el diseño del experimento: en cada contraste se selecciona la muestra de una manera diferente. En nuestro caso, para contrastar si la distribución de probabilidades de la base de mayor frecuencia es la misma para cada tipo de cadena o no, lo que vamos a hacer es tomar una muestra aleatoria de 50 cadenas de cada tipo, juntarlas en una sola muestra estratificada, y aplicar el test \\(\\chi^2\\) a esta muestra. El código siguiente realiza el muestreo en cada subpoblación de tipo y guarda la muestra total en el data frame muestra.test.homo. Fijamos de nuevo la semilla de aleatoriedad (otra), para que el test sea reproducible. set.seed(100) Generamos los vectores de índices de las muestras: n3=50 indices.muestraA=sample(1:dim(poblacionA)[1],size=n3,replace=TRUE) indices.muestraB=sample(1:dim(poblacionB)[1],size=n3,replace=TRUE) indices.muestraC=sample(1:dim(poblacionC)[1],size=n3,replace=TRUE) Finalmente, tomamos las filas de cada muestra y las combinamos en un data frame: muestraA.50=poblacionA[indices.muestraA,] muestraB.50=poblacionB[indices.muestraB,] muestraC.50=poblacionC[indices.muestraC,] muestra.test.homo=rbind(muestraA.50,muestraB.50,muestraC.50) str(muestra.test.homo) ## &#39;data.frame&#39;: 150 obs. of 2 variables: ## $ tipo : Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ max.frec: Factor w/ 4 levels &quot;a&quot;,&quot;c&quot;,&quot;g&quot;,&quot;t&quot;: 4 2 2 4 3 2 1 1 3 4 ... Calculamos la tabla de contingencia de la muestra: tabla.homo=table(muestra.test.homo$tipo,muestra.test.homo$max.frec) tabla.homo ## ## a c g t ## A 10 16 16 8 ## B 21 21 6 2 ## C 5 12 23 10 Añadimos los márgenes: addmargins(tabla.homo) ## ## a c g t Sum ## A 10 16 16 8 50 ## B 21 21 6 2 50 ## C 5 12 23 10 50 ## Sum 36 49 45 20 150 Confirmamos que hemos tomado 50 cadenas de cada grupo. Ahora calculamos las frecuencias esperadas bajo la hipótesis nula, para comprobar si son todas mayores o iguales que 5: frec.abs.tipo=rowSums(tabla.homo) frec.abs.tipo ## A B C ## 50 50 50 frec.abs.max.frec=colSums(tabla.homo) frec.abs.max.frec ## a c g t ## 36 49 45 20 frec.esperadas=frec.abs.tipo%*%t(frec.abs.max.frec)/sum(frec.abs.tipo) frec.esperadas ## a c g t ## [1,] 12 16.3333 15 6.66667 ## [2,] 12 16.3333 15 6.66667 ## [3,] 12 16.3333 15 6.66667 Todas las frecuencias son mayores o iguales que 5, así que aplicamos la función chisq.test sin simular el p-valor: chisq.test(tabla.homo) ## ## Pearson&#39;s Chi-squared test ## ## data: tabla.homo ## X-squared = 28.59, df = 6, p-value = 7.27e-05 El p-valor es muy pequeño, por lo que podemos rechazar que las distribuciones de los valores de las bases de máxima frecuencia sean la misma para cada valor de la variable tipo. En definitiva, el tipo de cadena afecta a la distribución de la base de mayor frecuencia. Es la misma conclusión a la que habíamos llegado con el test de independencia, solo que ahora hemos realizado un tipo de experimento diferente. 7.4 Potencia de un contraste \\(\\chi^2\\) La potencia de un contraste \\(\\chi^2\\), tanto de bondad de ajuste como de independencia o de homogeneidad, se puede calcular de manera similar a cómo lo hacíamos en otros tipos de contrastes de uno o dos parámetros. La instrucción para llevarlo a cabo es pwr.chisq.test del paquete pwr. Su sintaxis básica es pwr.chisq.test(N=..., df=..., sig.level=..., w=..., power=...) donde: N es el tamaño de la muestra. df es el número de grados de libertad del estadístico (recordad que en un test de bondad de ajuste es el número de clases menos 1 y menos el número de parámetros estimados, y en un test de independencia o de homogeneidad es el número de niveles de una variable menos 1 por el número de niveles de la otra variable menos 1). sig.level es el nivel de significación \\(\\alpha\\). w es la magnitud del efecto, que en este tipo de tests se define como \\(\\sqrt{X^2/N}\\), siendo \\(X^2\\) el valor del estadístico de contraste y \\(N\\) el tamaño de la muestra completa. power es la potencia \\(1-\\beta\\). Si se especifican todos estos parámetros menos uno, la función da el valor del parámetro que falta. Normalmente, querremos saber la potencia de un contraste a posteriori o el tamaño de la muestra necesario para tener la potencia deseada para una magnitud del efecto esperada concreta. Veamos algunos ejemplos de uso de esta función. Ejemplo 7.1 Vamos a calcular la potencia del contraste del Ejemplo 6.3. En ese ejemplo, el tamaño de la muestra fue \\(N=40\\), el número de grados de libertad fue 5 y obtuvimos que \\(X^2=7.7\\), por lo que la magnitud del efecto fue \\(w=\\sqrt{7.7/40}\\). Tomaremos el nivel de significación usual, \\(\\alpha=0.05\\). library(pwr) pwr.chisq.test(N=40, df=5, sig.level=0.05, w=sqrt(7.7/40)) ## ## Chi squared power calculation ## ## w = 0.438748 ## N = 40 ## df = 5 ## sig.level = 0.05 ## power = 0.545751 ## ## NOTE: N is the number of observations La potencia del contraste ha sido de, aproximadamente, un 55%. Para obtener solo la potencia, podéis usar el sufijo $power: pwr.chisq.test(N=40, df=5, sig.level=0.05, w=sqrt(7.7/40))$power ## [1] 0.545751 Ejemplo 7.2 Vamos a calcular la potencia del contraste de normalidad de las longitudes de los sépalos de flores iris del Ejemplo 6.8. En ese ejemplo el tamaño de muestra fue \\(N=150\\); como usamos 7 clases, pero estimamos 2 parámetros, el número de grados de libertad fue 4; obtuvimos que \\(X^2=11.0637\\), por lo que \\(w=\\sqrt{11.0637/150}\\); y ahora, por variar, tomaremos \\(\\alpha=0.1\\). pwr.chisq.test(N=150, df=4, sig.level=0.1, w=sqrt(11.0637/15))$power ## [1] 1 La potencia da 1: la probabilidad de que aceptáramos que la muestra seguía una distribución normal si no fuera verdad es prácticamente 0. Ejemplo 7.3 En el contraste de homogeneidad de la Sección 7.3 hemos tomado tres muestras de 50 individuos cada una, en total 150 individuos. El estadístico de contraste ha valido \\(X^2=28.59\\), por lo que la magnitud del efecto en ese test ha sido de \\(w=\\sqrt{28.59/150}=0.4366\\), entre mediana y grande según la función cohen.ES, que nos da las magnitudes del efecto que por convención se entienden como pequeñas, medianas o grandes para los diferentes tests considerados en el paquete pwr: cohen.ES(test=&quot;chisq&quot;, size=&quot;medium&quot;)$effect.size ## [1] 0.3 cohen.ES(test=&quot;chisq&quot;, size=&quot;large&quot;)$effect.size ## [1] 0.5 ¿De qué tamaño deberíamos haber tomado las muestras para garantizar una potencia del 90%, suponiendo que esperásemos una magnitud del efecto mediana y tomásemos un nivel de significación \\(\\alpha=0.05\\)? pwr.chisq.test(df=6, sig.level=0.05, w=0.3, power=0.9) ## ## Chi squared power calculation ## ## w = 0.3 ## N = 193.543 ## df = 6 ## sig.level = 0.05 ## power = 0.9 ## ## NOTE: N is the number of observations Hubiéramos necesitado como mínimo un total de unos 194 individuos: si queríamos tomar las tres muestras del mismo tamaño, esto significa tres muestras de como mínimo 65 individuos cada una. Para obtener solo el tamaño de la muestra, se puede añadir el sufijo $N: pwr.chisq.test(df=6, sig.level=0.05, w=0.3, power=0.9)$N ## [1] 193.543 7.5 Guía rápida table calcula tablas de contingencia de frecuencias absolutas. prop.table calcula tablas de contingencia de frecuencias relativas. addmargins sirve para añadir a una table una fila o una columna obtenidas aplicando una función a todas las columnas o a todas las filas de la tabla, respectivamente. Sus parámetros principales son: margin: igualado a 1, se aplica la función por columnas, añadiendo una nueva fila; igualado a 2, se aplica la función por filas, añadiendo una nueva columna; igualado a c(1,2), que es su valor por defecto, hace ambas cosas. FUN: la función que se aplica a las filas o columnas; su valor por defecto es sum. colSums calcula un vector con las sumas de las columnas de una matriz o una tabla. rowSums calcula un vector con las sumas de las filas de una matriz o una tabla. chisq.test sirve para realizar tests \\(\\chi^2\\) de independencia y homogeneidad. El resultado es una list formada, entre otros, por los objetos siguientes: statistic (el valor del estadístico \\(X^2\\)), parameter (los grados de libertad) y p.value (el p-valor). Sus parámetros principales en el contexto de esta lección son: simulate.p.value: igualado a TRUE, calcula el p-valor mediante simulaciones. B: en este último caso, permite especificar el número de simulaciones. pwr.chisq.test del paquete pwr, sirve para calcular uno de los parámetros siguientes a partir de los otros cuatro: N: el tamaño de la muestra. df: el número de grados de libertad del contraste. sig.level: el nivel de significación \\(\\alpha\\). power: la potencia \\(1-\\beta\\). w: la magnitud del efecto. 7.6 Ejercicios Modelo de test (1) Hemos observado dos variables cualitativas en una muestra de una población. Cada variable tiene 3 niveles. La tabla de contingencia resultante ha sido la siguiente: \\[ \\begin{array}{c|ccc} &amp;X&amp;Y&amp;Z\\cr\\hline A &amp; 2 &amp; 17 &amp; 11\\cr B &amp; 8 &amp; 10 &amp; 25\\cr C &amp; 3 &amp; 14 &amp; 5 \\end{array} \\] ¿Es verdad que, si estas variables aleatorias fueran independientes, las frecuencias esperadas de cada combinación de niveles, uno de cada variable, serían todas \\({\\geqslant}5\\)? Tenéis que contestar SI, en mayúsculas y sin acento, o NO. (2) Hemos observado dos variables cualitativas en una muestra de una población. Una variable tiene 4 niveles y la otra 3. La tabla de contingencia resultante ha sido la siguiente: \\[ \\begin{array}{c|cccc} &amp;A&amp;B&amp;C &amp; D\\cr\\hline X &amp; 50 &amp; 19&amp;17 &amp; 21\\cr Y &amp;69 &amp; 47 &amp; 56 &amp; 37 \\cr Z &amp;33 &amp; 23 &amp; 18 &amp; 21 \\end{array} \\] Emplead la función chisq.test para contrastar si estas dos variables son independientes o no. Tenéis que dar el p-valor del test (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decir (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.1\\), podríamos rechazar la hipótesis nula de que estas dos variables son independientes. Dad las dos respuestas en este orden y separadas por un único espacio en blanco. (3) Hemos realizado un test \\(\\chi^2\\) de independencia sobre una muestra de 200 individuos, con un nivel de significación de 0.1. Las variables objeto de estudio tenían 5 y 6 niveles, respectivamente. El estadístico de contraste ha valido \\(16.56\\). ¿Cuál es el p-valor del contraste? ¿Cuál es la potencia del contraste realizado? Tenéis que dar ambos valores en este orden, redondeados a 3 cifras decimales sin ceros innecesarios a la derecha, y separados por un único espacio en blanco. Respuestas al test (1) NO Nosotros lo hemos mirado con A=matrix(c(2,17,11,8,10,25,3,14,5),nrow=3,byrow=TRUE) RS=rowSums(A) CS=colSums(A) frec.esperadas=RS%*%t(CS)/sum(A) frec.esperadas (2) 0.128 NO Nosotros lo hemos resuelto con X=matrix(c(50,19,17,21,69,47,56,37,33,23,18,21),nrow=3,byrow=T) round(chisq.test(X)$p.value,3) (3) 0.681 0.777 Nosotros lo hemos resuelto con p.valor=1-pchisq(16.56,(5-1)*(6-1)) potencia=pwr.chisq.test(N=200, df=(5-1)*(6-1), sig.level=0.1, w=sqrt(16.56/200))$power round(c(p.valor,potencia),3) El valor por defecto de margin es el vector de todas las dimensiones de la tabla. Hay que recordar que, aunque ahora sólo tratamos con tablas bidimensionales, con table se pueden especificar tablas de contingencia de un número arbitrario de dimensiones.↩ "],
["introduccion-a-la-estadistica-descriptiva-multidimensional.html", "Lección 8 Introducción a la estadística descriptiva multidimensional 8.1 Matrices de datos cuantitativos 8.2 Transformaciones lineales 8.3 Covarianzas y correlaciones 8.4 Correlación de Spearman 8.5 Contrastes de correlación 8.6 Un ejemplo 8.7 Representación gráfica de datos multidimensionales 8.8 Guía rápida 8.9 Ejercicios", " Lección 8 Introducción a la estadística descriptiva multidimensional En general, los datos que se recogen en experimentos son multidimensionales: medimos varias variables aleatorias sobre una misma muestra de individuos, y organizamos esta información en tablas de datos en las que las filas representan los individuos observados y cada columna corresponde a una variable diferente. En las lecciones finales de la primera parte ya aparecieron datos cualitativos y ordinales multidimensionales, para los que calculamos y representamos gráficamente sus frecuencias globales y marginales; en esta lección estudiamos algunos estadísticos específicos para resumir y representar la relación existente entre diversas variables cuantitativas. 8.1 Matrices de datos cuantitativos Supongamos que hemos medido los valores de \\(p\\) variables aleatorias \\(X_1,\\ldots,X_p\\) sobre un conjunto de \\(n\\) individuos u objetos. Es decir, tenemos \\(n\\) observaciones de \\(p\\) variables. En cada observación, los valores que toman estas variables forman un vector que será una realización del vector aleatorio \\(\\underline{X}=(X_1,X_2,\\ldots,X_p)\\). Para trabajar con estas observaciones, las dispondremos en una tabla de datos donde cada fila corresponde a un individuo y cada columna, a una variable. En R, lo más conveniente es definir esta tabla en forma de data frame, pero, por conveniencia de lenguaje, en el texto de esta lección la representaremos como una matriz \\[ {X}=\\begin{pmatrix} x_{1 1} &amp; x_{1 2} &amp;\\ldots &amp; x_{1 p}\\\\ x_{2 1} &amp; x_{2 2} &amp;\\ldots &amp; x_{2 p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots\\\\ x_{n 1} &amp; x_{n 2} &amp;\\ldots &amp; x_{n p} \\end{pmatrix}. \\] Utilizaremos las notaciones siguientes: Denotaremos la \\(i\\)-ésima fila de \\(X\\) por \\[ {x}_{i\\bullet}=(x_{i 1}, x_{i 2}, \\ldots, x_{i p}). \\] Este vector está compuesto por las observaciones de las \\(p\\) variables sobre el \\(i\\)-ésimo individuo. Denotaremos la \\(j\\)-ésima columna de \\(X\\) por \\[ x_{\\bullet j}=\\begin{pmatrix}x_{1 j} \\\\ x_{2 j}\\\\ \\vdots \\\\ x_{n j} \\end{pmatrix}. \\] Esta columna está formada por todos los valores de la \\(j\\)-ésima variable, es decir, es una muestra de \\(X_j\\). Observad que, en cada caso, la bolita \\(\\bullet\\) en el subíndice representa el índice “variable” de los elementos del vector o de la columna. De esta manera, podremos expresar la matriz de datos \\(X\\) tanto por filas (individuos) como por columnas (muestras de variables): \\[ {X}=\\begin{pmatrix}{x}_{1\\bullet}\\\\x_{2\\bullet}\\\\\\vdots \\\\ {x}_{n\\bullet}\\end{pmatrix}=({x}_{\\bullet1}, {x}_{\\bullet 2}, \\ldots, {x}_{\\bullet p}). \\] Con estas notaciones, podemos generalizar al caso multidimensional los estadísticos de una variable cuantitativa, definiéndolos como los vectores que se obtienen aplicando el estadístico concreto a cada columna de la tabla de datos. Así: El vector de medias de \\(X\\) es el vector formado por las medias aritméticas de sus columnas: \\[ \\overline{X}=(\\overline{{{x}}}_{\\bullet1}, \\overline{{x}}_{\\bullet 2}, \\ldots, \\overline{{x}}_{\\bullet p}), \\] donde, para cada \\(j=1, \\ldots, p\\), \\[ \\overline{{x}}_{\\bullet j}=\\frac{1}{n}\\sum\\limits_{i=1}^n x_{i j}. \\] Observemos que \\[ \\begin{array}{rl} \\overline{X} &amp; \\displaystyle = (\\overline{{{x}}}_{\\bullet1}, \\overline{x}_{\\bullet 2},\\ldots,\\overline{x}_{\\bullet p}) = \\frac{1}{n} \\Big(\\sum_{i=1}^n x_{i 1}, \\sum_{i=1}^n x_{i 2},\\ldots, \\sum_{i=1}^n x_{i p}\\Big)\\\\[1ex] &amp; \\displaystyle =\\frac{1}{n} \\sum_{i=1}^n (x_{i 1}, x_{i 2},\\ldots,x_{i p} ) = \\frac{1}{n} \\sum_{i=1}^n {{x}_{i\\bullet}} \\end{array} \\] Es decir, el vector de medias de \\(X\\) es la media aritmética de sus vectores fila. El vector de varianzas de \\(X\\) es el vector formado por las varianzas de sus columnas: \\[ s^2_{X}=(s^2_{1}, s^2_2, \\ldots, s^2_p), \\] donde \\[ s_j^2=\\frac{1}{n}\\sum_{i=1}^n {(x_{ij}-\\overline{{x}}_{\\bullet j})^2}. \\] El vector de varianzas muestrales de \\(X\\) está formado por las varianzas muestrales de sus columnas: \\[ \\widetilde{s}^2_{X}=(\\widetilde{s}^2_{1}, \\widetilde{s}^2_2, \\ldots, \\widetilde{s}^2_p), \\] donde \\[ \\widetilde{s}_j^2=\\frac{1}{n-1}\\sum_{i=1}^n {(x_{ij}-\\overline{{x}}_{\\bullet j})^2}=\\frac{n}{n-1}s_j^2. \\] Los vectores de desviaciones típicas \\(s_{X}\\) y de desviaciones típicas muestrales \\(\\widetilde{s}_{X}\\) de \\(X\\) son los formados por las desviaciones típicas y las desviaciones típicas muestrales de sus columnas, respectivamente: \\[ \\begin{array}{l} s_{X}=(s_{1}, s_2, \\ldots, s_p)=(+\\sqrt{\\vphantom{s_p^2}{s}^2_{1}}, +\\sqrt{\\vphantom{s_p^2}{s}^2_2}, \\ldots, +\\sqrt{s_p^2})\\\\[1ex] \\widetilde{s}_{X}=(\\widetilde{s}_{1}, \\widetilde{s}_2, \\ldots, \\widetilde{s}_p)=(+\\sqrt{\\vphantom{s_p^2}\\widetilde{s}^2_{1}}, +\\sqrt{\\vphantom{s_p^2}\\widetilde{s}^2_2}, \\ldots, +\\sqrt{\\widetilde{s}^2_p}) \\end{array} \\] Como en el caso unidimensional, \\(\\overline{X}\\) es un estimador insesgado de la esperanza \\(E(\\underline{X})=\\boldsymbol\\mu\\) del vector aleatorio \\(\\underline{X}\\) del cual \\(X\\) es una muestra. Por lo que refiere a \\({s}^2_{X}\\) y \\(\\widetilde{s}^2_{X}\\), ambas son estimadores del vector de varianzas de \\(\\underline{X}\\): \\(\\widetilde{s}^2_{X}\\) es insesgado y, cuando todas las variables aleatorias del vector son normales, \\({s}^2_{X}\\) es el máximo verosímil. Estos vectores de estadísticos se pueden calcular con R aplicando la función correspondiente al estadístico a todas las columnas de la tabla de datos. La manera más sencilla de hacerlo en un solo paso es usando la función sapply, si tenemos guardada la tabla como un data frame, o apply con MARGIN=2, si la tenemos guardada en forma de matriz. Ejemplo 8.1 Consideremos la tabla de datos \\[ X=\\left(\\begin{array}{rrr} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{array}\\right) \\] formada por 4 observaciones de 3 variables; por lo tanto, \\(n=4\\) y \\(p=3\\). Vamos a guardarla en un data frame y a calcular sus estadísticos. X=data.frame(V1=c(1,1,2,3),V2=c(-1,0,3,0),V3=c(3,3,0,1)) X ## V1 V2 V3 ## 1 1 -1 3 ## 2 1 0 3 ## 3 2 3 0 ## 4 3 0 1 Su vector de medias es: sapply(X, mean) ## V1 V2 V3 ## 1.75 0.50 1.75 Su vector de varianzas muestrales es: sapply(X, var) ## V1 V2 V3 ## 0.916667 3.000000 2.250000 Su vector de desviaciones típicas muestrales es: sapply(X, sd) ## V1 V2 V3 ## 0.957427 1.732051 1.500000 Su vector de varianzas es: var_ver=function(x){var(x)*(length(x)-1)/length(x)} #Varianza &quot;verdadera&quot; sapply(X, var_ver) ## V1 V2 V3 ## 0.6875 2.2500 1.6875 Su vector de desviaciones típicas es: sd_ver=function(x){sqrt(var_ver(x))} #Desv. típica &quot;verdadera&quot; sapply(X, sd_ver) ## V1 V2 V3 ## 0.829156 1.500000 1.299038 Nota. De ahora en adelante, supondremos que todos los vectores de datos cuantitativos que aparezcan en lo que queda de lección, incluidas las columnas de tablas de datos, son no constantes y, por lo tanto, tienen desviación típica no nula. 8.2 Transformaciones lineales A veces es conveniente aplicar una transformación lineal a una tabla de datos \\(X\\), sumando a cada columna un valor y luego multiplicando cada columna resultante por otro valor. Los dos ejemplos más comunes de trasformación lineal son el centrado y la tipificación de datos. Para centrar una matriz de datos \\(X\\), se resta a cada columna su media aritmética: \\[ \\widetilde{X}= \\begin{pmatrix} x_{1 1}- \\overline{x}_{\\bullet 1}&amp; x_{1 2}- \\overline{x}_{\\bullet 2} &amp;\\ldots &amp; x_{1 p}- \\overline{x}_{\\bullet p}\\\\ x_{2 1} - \\overline{x}_{\\bullet 1}&amp; x_{2 2}- \\overline{x}_{\\bullet 2} &amp;\\ldots &amp; x_{2 p}- \\overline{x}_{\\bullet p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots\\\\ x_{n 1} - \\overline{x}_{\\bullet 1}&amp; x_{n 2}- \\overline{x}_{\\bullet 2} &amp;\\ldots &amp; x_{n p}- \\overline{x}_{\\bullet p} \\end{pmatrix}. \\] Llamaremos a esta matriz la matriz de datos centrados de \\(X\\). Ejemplo 8.2 Consideremos de nuevo la matriz de datos del Ejemplo 8.1, \\[ {X}=\\left(\\begin{array}{rrr} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{array}\\right) \\] Para centrarla, hemos de restar a cada columna su media. Ya hemos calculado estas medias hace un momento: sapply(X, mean) ## V1 V2 V3 ## 1.75 0.50 1.75 Por lo tanto, su matriz de datos centrados es \\[ \\widetilde{X}=\\left(\\begin{array}{rrr} 1-1.75&amp;-1-0.5&amp;3-1.75\\\\ 1-1.75&amp;0-0.5&amp;3-1.75\\\\ 2-1.75&amp;3-0.5&amp;0-1.75 \\\\ 3-1.75&amp;0-0.5&amp;1-1.75 \\end{array}\\right)= \\left(\\begin{array}{rrr} -0.75&amp;-1.5&amp;1.25\\\\ -0.75&amp;-0.5&amp;1.25\\\\ 0.25&amp;2.5&amp;-1.75\\\\ 1.25&amp;-0.5&amp;-0.75 \\end{array}\\right). \\] Dado un vector de datos formado por una muestra de una variable cuantitativa, su vector de datos tipificados es el vector que se obtiene restando a cada entrada la media aritmética del vector y dividiendo el resultado por su desviación típica. De esta manera, se obtiene un vector de datos de media aritmética 0 y varianza 1. Tipificar un vector de datos es conveniente cuando se quiere trabajar con estos datos sin que influyan ni su media ni las unidades en los que están medidos: al dividir por su desviación típica, los valores resultantes son adimensionales. Por lo tanto, tipificar las variables de una tabla de datos permite compararlas dejando de lado las diferencias que pueda haber entre sus valores medios o sus varianzas. La matriz tipificada de una matriz de datos \\(X\\) es la matriz \\(Z\\) que se obtiene tipificando cada columna; es decir, para tipificar una matriz de datos \\(X\\), restamos a cada columna su media y a continuación dividimos cada columna por la desviación típica de la columna original en \\(X\\) (que coincide con la desviación típica de la columna “centrada”, puesto que sumar o restar constantes no modifica la desviación típica): \\[ Z=\\begin{pmatrix} \\frac{x_{1 1}- \\overline{x}_{\\bullet 1}}{s_1}&amp; \\frac{x_{1 2}- \\overline{x}_{\\bullet 2}}{s_2} &amp;\\ldots &amp; \\frac{x_{1 p}- \\overline{x}_{\\bullet p}}{s_p}\\\\[2ex] \\frac{x_{2 1} - \\overline{x}_{\\bullet 1}}{s_1}&amp; \\frac{x_{2 2}- \\overline{x}_{\\bullet 2}}{s_2} &amp;\\ldots &amp; \\frac{x_{2 p}- \\overline{x}_{\\bullet p}}{s_p}\\\\[2ex] \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots\\\\ \\frac{x_{n 1} - \\overline{x}_{\\bullet 1}}{s_1}&amp; \\frac{x_{n 2}- \\overline{x}_{\\bullet 2}}{s_2} &amp;\\ldots &amp; \\frac{x_{n p}- \\overline{x}_{\\bullet p}}{s_p} \\end{pmatrix}. \\] Ejemplo 8.3 Vamos a tipificar a mano la tabla de datos \\[ {X}=\\left(\\begin{array}{rrr} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{array}\\right) \\] del Ejemplo 8.1. Ya la hemos centrado en el Ejemplo 8.2. Para tipificarla, tenemos que dividir cada columna de esta matriz centrada por la desviación típica de la columna correspondiente en la matriz original. Hemos calculado estas desviaciones típicas en el Ejemplo 8.1: sapply(X, sd_ver) ## V1 V2 V3 ## 0.829156 1.500000 1.299038 Dividiendo cada columna de la matriz centrada \\(\\widetilde{X}\\) por la correspondiente desviación típica obtenemos: \\[ \\begin{array}{rl} Z &amp; =\\left(\\begin{array}{rrr} -0.75/0.829156&amp;-1.5/1.5&amp;1.25/1.299038\\\\ -0.75/0.829156&amp;-0.5/1.5&amp;1.25/1.299038\\\\ 0.25/0.829156&amp;2.5/1.5&amp;-1.75/1.299038\\\\ 1.25/0.829156&amp;-0.5/1.5&amp;-0.75/1.299038 \\end{array}\\right)\\\\ &amp; = \\left(\\begin{array}{rrr} -0.9045340&amp;-1.0000000&amp;0.96225\\\\ -0.9045340&amp;-0.333333&amp;0.96225\\\\ 0.301511&amp;1.666667&amp;-1.347151\\\\ 1.507557&amp;-0.333333&amp;-0.57735 \\end{array}\\right) \\end{array} \\] La manera más sencilla de aplicar con R una transformación lineal a una tabla de datos \\(X\\), y en particular de centrarla o tipificarla, es usando la instrucción scale(X, center=..., scale=...) donde: X puede ser tanto una matriz como un data frame; el resultado será siempre una matriz. El valor del parámetro center es el vector que restamos a sus columnas, en el sentido de que cada entrada de este vector se restará a todas las entradas de la columna correspondiente. Su valor por defecto (que no es necesario especificar, aunque también se puede especificar con center=TRUE) es el vector \\(\\overline{X}\\) de medias de \\(X\\); para especificar que no se reste nada, podemos usar center=FALSE. El valor del parámetro scale es el vector por el que dividimos las columnas de \\(X\\):cada columna se divide por la entrada correspondiente de este vector.Su valor por defecto (de nuevo, se puede especificar igualando el parámetro a TRUE) es el vector \\(\\widetilde{s}_X\\) de desviaciones típicas muestrales; para especificar que no se divida por nada, podemos usar scale=FALSE. En particular, la instrucción scale(X) centra la tabla de datos \\(X\\) y divide sus columnas por sus desviaciones típicas muestrales; por lo tanto, no la tipifica según nuestra definición, ya que no las divide por sus desviaciones típicas “verdaderas”. Ejemplo 8.4 Recordemos la tabla de datos \\(X\\) del Ejemplo 8.1. X ## V1 V2 V3 ## 1 1 -1 3 ## 2 1 0 3 ## 3 2 3 0 ## 4 3 0 1 Su matriz centrada es: X_centrada=scale(X, center=TRUE, scale=FALSE) X_centrada ## V1 V2 V3 ## [1,] -0.75 -1.5 1.25 ## [2,] -0.75 -0.5 1.25 ## [3,] 0.25 2.5 -1.75 ## [4,] 1.25 -0.5 -0.75 ## attr(,&quot;scaled:center&quot;) ## V1 V2 V3 ## 1.75 0.50 1.75 Coincide con la matriz obtenida en el Ejemplo 8.2. Observad la estructura del resultado: en primer lugar nos da la matriz centrada, y a continuación nos dice que tiene un atributo llamado &quot;scaled:center&quot; cuyo valor es el vector usado para centrarla. Este atributo no interferirá para nada en las operaciones que realicéis con la matriz centrada, pero, si os molesta, recordad que se puede eliminar sustituyendo el resultado de centrar la matriz en los puntos suspensivos de la instrucción siguiente: attr(... , &quot;scaled:center&quot;)=NULL En nuestro ejemplo: attr(X_centrada, &quot;scaled:center&quot;)=NULL X_centrada ## V1 V2 V3 ## [1,] -0.75 -1.5 1.25 ## [2,] -0.75 -0.5 1.25 ## [3,] 0.25 2.5 -1.75 ## [4,] 1.25 -0.5 -0.75 Como ya hemos avisado, para tipificar esta tabla de datos no podemos hacer lo siguiente: X_tip=scale(X) X_tip ## V1 V2 V3 ## [1,] -0.783349 -0.866025 0.833333 ## [2,] -0.783349 -0.288675 0.833333 ## [3,] 0.261116 1.443376 -1.166667 ## [4,] 1.305582 -0.288675 -0.500000 ## attr(,&quot;scaled:center&quot;) ## V1 V2 V3 ## 1.75 0.50 1.75 ## attr(,&quot;scaled:scale&quot;) ## V1 V2 V3 ## 0.957427 1.732051 1.500000 Para hacerlo bien según la definición que hemos dado, tenemos dos opciones. Una es multiplicar la matriz anterior por \\(\\sqrt{n/(n-1)}\\), donde \\(n\\) es el número de filas de la tabla. (El motivo es que, como \\(\\widetilde{s}_X=\\sqrt{\\frac{n}{n-1}}\\cdot s_X\\), se tiene que \\(\\frac{1}{s_X}=\\sqrt{\\frac{n}{n-1}}\\cdot \\frac{1}{\\widetilde{s}_X}\\); por lo tanto, si queríamos dividir por \\(s_X\\) y scale(X) ha dividido por \\(\\widetilde{s}_X\\), basta multiplicar su resultado por \\(\\sqrt{\\frac{n}{n-1}}\\) para obtener el efecto deseado.) n=dim(X)[1] #Número de filas de X X_tip=scale(X)*sqrt(n/(n-1)) X_tip ## V1 V2 V3 ## [1,] -0.904534 -1.000000 0.96225 ## [2,] -0.904534 -0.333333 0.96225 ## [3,] 0.301511 1.666667 -1.34715 ## [4,] 1.507557 -0.333333 -0.57735 ## attr(,&quot;scaled:center&quot;) ## V1 V2 V3 ## 1.75 0.50 1.75 ## attr(,&quot;scaled:scale&quot;) ## V1 V2 V3 ## 0.957427 1.732051 1.500000 Ahora sí que coincide con la matriz obtenida “a mano” en el Ejemplo 8.3. Otra posibilidad es usar, como valor del parámetro scale, el vector \\(s_X\\) de desviaciones típicas de las columnas. X_tip1=scale(X, scale=sapply(X, sd_ver)) X_tip1 ## V1 V2 V3 ## [1,] -0.904534 -1.000000 0.96225 ## [2,] -0.904534 -0.333333 0.96225 ## [3,] 0.301511 1.666667 -1.34715 ## [4,] 1.507557 -0.333333 -0.57735 ## attr(,&quot;scaled:center&quot;) ## V1 V2 V3 ## 1.75 0.50 1.75 ## attr(,&quot;scaled:scale&quot;) ## V1 V2 V3 ## 0.829156 1.500000 1.299038 Observaréis que la matriz resultante es la misma, pero el atributo que indica el vector por el que hemos dividido las columnas es diferente: en este caso, es el de desviaciones típicas. Ahora, en ambos casos, podemos usar la función attr para eliminar los dos atributos, &quot;scaled:center&quot; y &quot;scaled:scale&quot;, que se han añadido a la matriz tipificada. Por ejemplo: attr(X_tip, &quot;scaled:center&quot;)=NULL attr(X_tip, &quot;scaled:scale&quot;)=NULL X_tip ## V1 V2 V3 ## [1,] -0.904534 -1.000000 0.96225 ## [2,] -0.904534 -0.333333 0.96225 ## [3,] 0.301511 1.666667 -1.34715 ## [4,] 1.507557 -0.333333 -0.57735 8.3 Covarianzas y correlaciones La covarianza entre dos variables es una medida de la tendencia que tienen ambas variables a variar conjuntamente. Cuando la covarianza es positiva, si una de las dos variables crece o decrece, la otra tiene el mismo comportamiento; en cambio, cuando la covarianza es negativa, esta tendencia se invierte: si una variable crece, la otra decrece y viceversa. Puesto que interpretar el valor de la covarianza más allá de su signo es difícil, se suele usar una versión “normalizada” de la misma, la correlación de Pearson, que mide de manera más precisa la relación lineal entre dos variables. La covarianza generaliza la varianza, en el sentido de que la varianza de una variable es su covarianza consigo misma. Y como en el caso de la varianza, definiremos dos versiones de la covarianza: la “verdadera” y la muestral. La diferencia estará de nuevo en el denominador. Formalmente, la covarianza de las variables \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\) de una matriz de datos \\(X\\) es \\[ s_{i j}=\\frac{1}{n} \\sum_{k =1}^n\\big((x_{k i}-\\overline{{x}}_{\\bullet i})(x_{kj}-\\overline{{x}}_{\\bullet j})\\big)= \\frac{1}{n} \\Big(\\sum_{k =1}^n x_{k i} x_{k j}\\Big) - \\overline{{x}}_{\\bullet i} \\overline{{x}}_{\\bullet j}, \\] y su covarianza muestral es \\[ \\widetilde{s}_{ij} = \\frac{1}{n-1} \\sum_{k =1}^n\\big((x_{k i}-\\overline{{x}}_{\\bullet i})(x_{kj}-\\overline{{x}}_{\\bullet j})\\big)= \\frac{n}{n-1} s_{ij}. \\] El estadístico \\(\\tilde{s}_{ij}\\) es siempre un estimador insesgado de la covarianza \\(\\sigma_{i j}\\) de las variables aleatorias \\(X_i\\) y \\(X_j\\) de las que \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\) son muestras, mientras que \\(s_{i j}\\) es su estimador máximo verosímil cuando la distribución conjunta de \\(X_i\\) y \\(X_j\\) es normal bivariante. Es inmediato comprobar a partir de sus definiciones que ambas covarianzas son simétricas, y que la covarianza de una variable consigo misma es su varianza: \\[ s_{i j}= s_{j i}, \\quad \\widetilde{s}_{i j}= \\widetilde{s}_{j i}, \\quad s_{i i}=s_{i}^2, \\quad \\widetilde{s}_{ii}=\\widetilde{s}_i^2. \\] Ejemplo 8.5 La covarianza de las dos primeras columnas de la matriz de datos \\[ X=\\left(\\begin{array}{rrr} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{array}\\right) \\] del Ejemplo 8.1 se calcularía de la manera siguiente: \\[ s_{12}=\\frac{1}{4}(1\\cdot (-1)+1\\cdot 0+2\\cdot 3+3\\cdot 0)-1.75\\cdot 0.5= 1.25-0.875=0.375 \\] Su covarianza muestral se obtendría multiplicando por \\(4/3\\) este valor: \\[ \\widetilde{s}_{12} = \\frac{4}{3} s_{12}=0.5. \\] La covarianza muestral de dos vectores numéricos de la misma longitud \\(n\\) se puede calcular con R mediante la función cov. Para obtener su covarianza “verdadera”, hay que multiplicar el resultado de cov por \\((n-1)/n\\). Ejemplo 8.6 La covarianza muestral de las dos primeras columnas de la tabla de datos \\(X\\), que tenemos guardada en el data frame X, es: cov(X$V1, X$V2) ## [1] 0.5 y su covarianza “verdadera” es: n=dim(X)[1] ((n-1)/n)*cov(X$V1, X$V2) ## [1] 0.375 Queremos recalcar que, como en el caso de la varianza con var, R calcula con cov la versión muestral de la covarianza. Las matrices de covarianzas y de covarianzas muestrales de una tabla de datos \\(X\\) son, respectivamente, \\[ {S}= \\begin{pmatrix} s_{1 1} &amp; s_{1 2} &amp; \\ldots &amp; s_{1 p}\\\\ s_{2 1} &amp; s_{2 2} &amp; \\ldots &amp; s_{2 p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ s_{p 1} &amp; s_{p 2} &amp; \\ldots &amp; s_{p p} \\end{pmatrix},\\ \\widetilde{{S}}= \\begin{pmatrix} \\widetilde{s}_{1 1} &amp; \\widetilde{s}_{1 2} &amp; \\ldots &amp; \\widetilde{s}_{1 p}\\\\ \\widetilde{s}_{2 1} &amp; \\widetilde{s}_{2 2} &amp; \\ldots &amp; \\widetilde{s}_{2 p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\widetilde{s}_{p 1} &amp; \\widetilde{s}_{p 2} &amp; \\ldots &amp; \\widetilde{s}_{p p} \\end{pmatrix}, \\] donde cada \\(s_{i j}\\) y cada \\(\\widetilde{s}_{i j}\\) son, respectivamente, la covarianza y la covarianza muestral de las correspondientes columnas \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\). Estas matrices de covarianzas miden la tendencia a la variabilidad conjunta de los datos de \\(X\\) y, si \\(n\\) es el número de filas de \\(X\\), se tiene que \\[ S=\\frac{n-1}{n}\\widetilde{{S}}. \\] La matriz de covarianzas muestrales \\(\\widetilde{{S}}\\) es un estimador insesgado de la matriz de covarianzas \\(\\Sigma\\) del vector de variables aleatorias \\(\\underline{X}\\), y si este tiene distribución normal multivariante, \\(S\\) es un estimador máximo verosímil de \\(\\Sigma\\). Ambas matrices de covarianzas son simétricas, puesto que \\(s_{i j}=s_{j i}\\), y tienen todos sus valores propios \\({\\geqslant}0\\). La matriz de covarianzas muestrales de una tabla de datos se calcula aplicando la función cov al data frame o a la matriz que contenga dicha tabla. Para obtener su matriz de covarianzas “verdaderas”, es suficiente multiplicar el resultado de cov por \\((n-1)/n\\), donde \\(n\\) es el número de filas de la tabla de datos. Ejemplo 8.7 Continuemos con la matriz \\[ X=\\left(\\begin{array}{rrr} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{array}\\right) \\] del Ejemplo 8.1. Su matriz de covarianzas muestrales es cov(X) ## V1 V2 V3 ## V1 0.916667 0.50000 -1.08333 ## V2 0.500000 3.00000 -2.16667 ## V3 -1.083333 -2.16667 2.25000 y su matriz de covarianzas es n=dim(X)[1] ((n-1)/n)*cov(X) ## V1 V2 V3 ## V1 0.6875 0.375 -0.8125 ## V2 0.3750 2.250 -1.6250 ## V3 -0.8125 -1.625 1.6875 Como la matriz de covarianzas es difícil de interpretar como medida de variabilidad de una tabla de datos, debido a que no es una única cantidad sino toda una matriz, interesa cuantificar esta variabilidad mediante un único índice. No hay consenso sobre este índice, y entre los que se usan destacamos: La varianza total de \\(X\\): la suma de las varianzas de sus columnas. La varianza media de \\(X\\): la media de las varianzas de sus columnas, es decir, la varianza total partida por el número de columnas. La varianza generalizada de \\(X\\): el determinante de su matriz de covarianzas. La desviación típica generalizada de \\(X\\): la raíz cuadrada positiva de su varianza generalizada. De cada uno de estos índices se definen, naturalmente, una versión muestral y una “verdadera”, según el tipo de varianzas o covarianzas que se usen en su cálculo. Pasemos ahora a la correlación lineal de Pearson (o, de ahora en adelante, simplemente correlación de Pearson) de dos variables \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\) de \\(X\\), que se define como \\[ r_{i j}=\\frac{s_{i j}}{s_i\\cdot s_j}. \\] Observad que \\[ \\frac{\\widetilde{s}_{i j}}{\\widetilde{s}_i\\cdot \\widetilde{s}_j}= \\frac{\\frac{n}{n-1}\\cdot {s}_{i j}}{\\sqrt{\\frac{n}{n-1}}\\cdot {s}_i \\cdot\\sqrt{\\frac{n}{n-1}}\\cdot{s}_j}= \\frac{s_{i j}}{s_i \\cdot s_j}=r_{i j}, \\] y, por lo tanto, esta correlación se puede calcular también a partir de las versiones muestrales de la covarianza y las desviaciones típicas por medio de la misma fórmula. El estadístico \\(r_{ij}\\) es un estimador máximo verosímil de la correlación de Pearson \\(\\rho_{i j}=Cor(X_i,X_j)\\) de las variables aleatorias \\(X_i\\) y \\(X_j\\) cuando su distribución conjunta es normal bivariante, y aunque es sesgado, su sesgo tiende a 0 cuando \\(n\\) tiende a \\(\\infty\\). Las propiedades más importantes de \\(r_{i,j}\\) son las siguientes: Es simétrica: \\(r_{i j}=r_{j i}\\). \\(-1{\\leqslant}r_{i j}{\\leqslant}1\\). \\(r_{i i}=1\\). \\(r_{i j}\\) tiene el mismo signo que \\(s_{i j}\\). \\(r_{i j}=\\pm 1\\) si y, sólo si, existe una relación lineal perfecta entre las variables \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\): es decir, si, y sólo si, existen valores \\(a, b\\in \\mathbb{R}\\) tales que \\[ \\left(\\begin{array}{c} x_{1j}\\\\ \\vdots \\\\ x_{nj}\\end{array}\\right)= a\\cdot \\left(\\begin{array}{c} x_{1i}\\\\ \\vdots \\\\ x_{ni}\\end{array}\\right) +b. \\] La pendiente \\(a\\) de esta relación lineal tiene el mismo signo que \\(r_{i j}\\). El coeficiente de determinación \\(R^2\\) de la regresión lineal por mínimos cuadrados de \\({x}_{\\bullet j}\\) respecto de \\({x}_{\\bullet i}\\) es igual al cuadrado de su correlación de Pearson, \\(r_{i j}^2\\); por lo tanto, cuánto más se aproxime el valor absoluto de \\(r_{ij}\\) a 1, más se acercan las variables \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\) a depender linealmente la una de la otra. Así pues, la correlación de Pearson entre dos variables viene a ser una covarianza “normalizada”, ya que, como vemos, su valor está entre -1 y 1, y mide la tendencia de las variables a estar relacionadas según una función lineal. En concreto, cuanto más se acerca dicha correlación a 1 (respectivamente, a -1), más se acerca una (cualquiera) de las variables a ser función lineal creciente (respectivamente, decreciente) de la otra. Con R, la correlación de Pearson de dos vectores se puede calcular aplicándoles la función cor. Ejemplo 8.8 En ejemplos anteriores hemos calculado la covarianza y las varianzas de las dos primeras columnas de la matriz de datos \\[ {X}=\\left(\\begin{array}{rrr} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{array}\\right) \\] Hemos obtenido los valores siguientes \\[ s_{12}=0.375,\\quad s_1=0.829156,\\quad s_2=1.5. \\] Por lo tanto, su correlación de Pearson es \\[ r_{1 2}=\\frac{0.375}{0.829156\\cdot 1.5}=0.301511. \\] Ahora vamos a calcularla con R, y aprovecharemos para confirmar su relación con el valor de \\(R^2\\) de la regresión lineal de la segunda columna respecto de la primera. Recordemos que esta tabla de datos sigue guardada en el data frame X. X ## V1 V2 V3 ## 1 1 -1 3 ## 2 1 0 3 ## 3 2 3 0 ## 4 3 0 1 La correlación de sus dos primeras columnas es: cor(X$V1, X$V2) ## [1] 0.301511 que coincide con el valor obtenido “a mano”. Comprobemos ahora que su cuadrado es igual al valor de \\(R^2\\) de la regresión lineal: cor(X$V1, X$V2)^2 ## [1] 0.0909091 summary(lm(X$V2~X$V1))$r.squared ## [1] 0.0909091 La matriz de correlaciones de Pearson de \\(X\\) es \\[ {R}= \\begin{pmatrix} 1 &amp; r_{1 2} &amp; \\ldots &amp; r_{1 p}\\\\ r_{2 1} &amp; 1 &amp; \\ldots &amp; r_{2 p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ r_{p 1} &amp; r_{p 2} &amp; \\ldots &amp; 1 \\end{pmatrix}, \\] donde cada \\(r_{i j}\\) es la correlación de Pearson de las columnas correspondientes de \\(X\\). Esta matriz de correlaciones tiene siempre determinante entre 0 y 1 (ambos extremos incluidos) y todos sus valores propios son mayores o iguales que 0, y con R se puede calcular aplicando la misma instrucción cor a la tabla de datos, sea en forma de matriz o de data frame. Así, la matriz de correlaciones de nuestra tabla de datos \\(X\\) es: cor(X) ## V1 V2 V3 ## V1 1.000000 0.301511 -0.754337 ## V2 0.301511 1.000000 -0.833950 ## V3 -0.754337 -0.833950 1.000000 Se tiene el teorema siguiente, que se puede demostrar mediante un simple, aunque farragoso, cálculo algebraico: Teorema 8.1 La matriz de correlaciones de Pearson de \\(X\\) es igual a: La matriz de covarianzas de su matriz tipificada. La matriz de covarianzas muestrales de su matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales en vez de por las “verdaderas”. La importancia de este resultado es que, si la tabla de datos es muy grande, suele ser más eficiente calcular la matriz de covarianzas de su matriz tipificada que la matriz de correlaciones de Pearson de la tabla original. Observad, por otro lado, que las dos matrices de covarianzas mencionadas en el enunciado coinciden, puesto que la matriz tipificada se obtiene multiplicando por \\(\\sqrt{n/(n-1)}\\) la matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales. Esto implica que la matriz de covarianzas muestrales de la matriz tipificada se obtiene multiplicando por \\(n/(n-1)\\) la matriz de covarianzas muestrales de la matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales. Finalmente, la matriz de covarianzas se obtiene a partir de la de covarianzas muestrales multiplicándola por \\((n-1)/n\\). Entonces, los factores \\(n/(n-1)\\) y \\((n-1)/n\\) se compensan y resulta que la matriz de covarianzas de la matriz tipificada coincide con la matriz de covarianzas muestrales de la matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales. Recordemos que si aplicamos la función scale a una tabla de datos \\(X\\), la tipifica dividiendo por las desviaciones típicas muestrales. Por lo tanto, otra manera de reformular el teorema anterior es decir que cor(X) da lo mismo que cov(scale(X)). Comprobemos esta igualdad para nuestra matriz de datos \\(X\\). cor(X) ## V1 V2 V3 ## V1 1.000000 0.301511 -0.754337 ## V2 0.301511 1.000000 -0.833950 ## V3 -0.754337 -0.833950 1.000000 cov(scale(X)) ## V1 V2 V3 ## V1 1.000000 0.301511 -0.754337 ## V2 0.301511 1.000000 -0.833950 ## V3 -0.754337 -0.833950 1.000000 Cuando se calcula la covarianza o la correlación de Pearson de dos vectores que contienen valores NA, lo usual es no tenerlos en cuenta: es decir, si un vector contiene un NA en una posición, se eliminan de los dos vectores sus entradas en dicha posición. De esta manera, se tomaría como covarianza de \\[ \\left(\\begin{array}{c} 1\\\\ 2\\\\ NA\\\\ 4\\\\ 6\\\\ 2\\end{array}\\right)\\mbox{ y } \\left(\\begin{array}{c} 2\\\\ 4\\\\ -3\\\\ 5\\\\ 7\\\\ NA \\end{array}\\right) \\] la de \\[ \\left(\\begin{array}{c}1\\\\ 2\\\\ 4\\\\ 6\\end{array}\\right)\\mbox{ y } \\left(\\begin{array}{c} 2\\\\ 4\\\\ 5\\\\ 7\\end{array}\\right). \\] Como ya nos pasaba con las funciones de estadística descriptiva univariante com mean o var, cuando aplicamos cov o cor a un par de vectores que contengan entradas NA, obtenemos por defecto NA. En las funciones univariantes usábamos na.rm=TRUE para pedir a R que obviara los NA, pero esta solución ahora no es posible, porque las posiciones de los NA también cuentan, y si los borramos tal cual se desmonta el emparejamiento de los datos. Así que, si se quiere que R calcule el valor de cov o cor sin tener en cuenta los NA, se ha de especificar añadiendo el parámetro use=&quot;complete.obs&quot;, que le indica que ha de usar las observaciones completas, es decir, las posiciones que no tienen NA en ninguno de los dos vectores. Veamos el efecto sobre los dos vectores anteriores. Llamémosles \\(x\\) e \\(y\\), y sean \\(x_1\\) e \\(y_1\\) los vectores que se obtienen eliminando las entradas que contienen un NA en alguno de los dos vectores. x=c(1,2,NA,4,6,2) y=c(2,4,-3,5,7,NA) x1=x[is.na(x)!=TRUE &amp; is.na(y)!=TRUE] y1=y[is.na(x)!=TRUE &amp; is.na(y)!=TRUE] x1 ## [1] 1 2 4 6 y1 ## [1] 2 4 5 7 Si calculamos la covarianza de \\(x\\) e \\(y\\) tal cual con la función cov, da NA: cov(x, y) ## [1] NA Usando use=&quot;complete.obs&quot;, obtenemos la covarianza de \\(x_1\\) e \\(y_1\\): cov(x, y, use=&quot;complete.obs&quot;) ## [1] 4.5 cov(x1, y1) ## [1] 4.5 Lo mismo sucede con la función cor: cor(x, y) ## [1] NA cor(x, y, use=&quot;complete.obs&quot;) ## [1] 0.974913 cor(x1, y1) ## [1] 0.974913 Al calcular las matrices de covarianzas o correlaciones de una tabla de datos que contenga valores NA, se suele seguir una de las dos estrategias siguientes, según lo que interese al usuario: Para cada par de columnas, se calcula su covarianza o su correlación con la estrategia explicada más arriba para dos vectores, obviando el hecho de que forman parte de una tabla de datos mayor; es decir, al efectuar el cálculo para cada par de columnas concreto, se eliminan de cada una de ellas solo las entradas de las filas en las que alguna de las dos tiene un NA, sin tener en cuenta para nada las otras columnas. Esta opción se especifica dentro de la función cov o cor con el parámetro use=&quot;pairwise.complete.obs&quot;. Antes de nada, se eliminan las filas de la tabla que contienen algún NA en alguna columna, dejando solo en la tabla las filas “completas”, las que no contienen ningún NA. Luego se calcula la matriz de covarianzas o de correlaciones de la tabla resultante. Esta opción se especifica con el parámetro use=&quot;complete.obs&quot;. Veamos un ejemplo. Consideremos la matriz de datos \\(Y\\) siguiente, cuyas dos primeras columnas son los vectores \\(x\\) e \\(y\\) anteriores: Y=cbind(c(1,2,NA,4,6,2), c(2,4,-3,5,7,NA), c(-2,1,0,2,NA,0)) Y ## [,1] [,2] [,3] ## [1,] 1 2 -2 ## [2,] 2 4 1 ## [3,] NA -3 0 ## [4,] 4 5 2 ## [5,] 6 7 NA ## [6,] 2 NA 0 Supongamos que queremos calcular su matriz de correlaciones de Pearson. Como todas las filas de \\(Y\\) tienen entradas NA, todas las correlaciones fuera de la diagonal dan NA (R sabe que la correlación de un columna consigo misma siempre es 1, y ya no la calcula): cor(Y) ## [,1] [,2] [,3] ## [1,] 1 NA NA ## [2,] NA 1 NA ## [3,] NA NA 1 Una opción es calcular las correlaciones de Pearson de cada par de variables eliminando sus valores NA pero sin tener en cuenta los posibles valores NA de la otra variable: cor(Y, use=&quot;pairwise.complete.obs&quot;) ## [,1] [,2] [,3] ## [1,] 1.000000 0.974913 0.891902 ## [2,] 0.974913 1.000000 0.438727 ## [3,] 0.891902 0.438727 1.000000 Observad que la entrada (1,2) de esta matriz es la correlación de los vectores \\(x\\) e \\(y\\) calculada con use=&quot;complete.obs&quot;. Calculemos ahora la matriz de correlaciones de Pearson de la matriz con filas completas: cor(Y, use=&quot;complete.obs&quot;) ## [,1] [,2] [,3] ## [1,] 1.000000 0.928571 0.891042 ## [2,] 0.928571 1.000000 0.995871 ## [3,] 0.891042 0.995871 1.000000 Veamos que efectivamente coincide con la matriz de correlaciones de Pearson de la matriz que se obtiene eliminando las filas que contienen algún NA. Esta matriz es: noNAs=is.na(Y[,1])!=TRUE &amp; is.na(Y[,2])!=TRUE &amp; is.na(Y[,3])!=TRUE Y1=Y[noNAs,] Y1 ## [,1] [,2] [,3] ## [1,] 1 2 -2 ## [2,] 2 4 1 ## [3,] 4 5 2 y su matriz de correlaciones de Pearson es: cor(Y1) ## [,1] [,2] [,3] ## [1,] 1.000000 0.928571 0.891042 ## [2,] 0.928571 1.000000 0.995871 ## [3,] 0.891042 0.995871 1.000000 8.4 Correlación de Spearman La correlación de Pearson mide específicamente la tendencia de dos variables cuantitativas continuas a depender linealmente una de otra. En circunstancias en las que no esperemos esta dependencia lineal, o en las que nuestras variables sean cuantitativas discretas o simplemente cualitativas, usar la correlación de Pearson para analizar la relación entre dos variables no es lo más adecuado. Entre las propuestas alternativas, la más popular es la correlación de Spearman. Este índice asigna a cada valor de cada vector su rango (su posición en el vector ordenado de menor a mayor, y en caso de empates la media de las posiciones que ocuparían todos los empates) y calcula la correlación de Pearson de estos rangos. Con R, la correlación de Spearman se calcula directamente con la función cor entrándole el parámetro method=&quot;spearman&quot;. (El valor por defecto del parámetro method es &quot;pearson&quot; y por eso no lo indicamos cuando calculamos la correlación de Pearson.) Ejemplo 8.9 Vamos a calcular la correlación de Spearman de las dos primeras columnas de la matriz de datos \\(X\\) que hemos venido usando en nuestros ejemplos. En la tabla siguiente calculamos los rangos de sus entradas: \\[ \\begin{array}{|c|c|c|c|} \\hline {x}_{\\bullet 1}&amp; rango &amp; {x}_{\\bullet 2}&amp; rango \\\\\\hline\\hline 1&amp; 1.5 &amp; -1&amp; 1 \\\\ 1&amp;1.5 &amp; 0 &amp; 2.5\\\\ 2&amp;3 &amp; 3&amp; 4 \\\\ 3&amp;4 &amp; 0&amp; 2.5\\\\\\hline \\end{array} \\] ¿Cómo hemos obtenido los rangos? Fijaos por ejemplo en la primera columna: los dos 1 ocuparían la posición 1 y 2, les asignamos a ambos como rango la media de estas posiciones, 1.5; el 2 ocuparía la posición 3 y el 3 ocuparía la posición 4, y estos son también sus rangos. Dejamos como ejercicio que comprobéis los rangos de los elementos de \\(x_{\\bullet 2}\\). Con R estos rangos se calculan con la función rank. Así, los rangos de los elementos de \\(x_{\\bullet 1}\\) son rank(X$V1) ## [1] 1.5 1.5 3.0 4.0 y los de los elementos de \\(x_{\\bullet 2}\\) son rank(X$V2) ## [1] 1.0 2.5 4.0 2.5 Por lo tanto, la correlación de Spearman de \\[ (1,1,2,3)\\mbox{ y }(-1,0,3,0) \\] es la correlación de Pearson de \\[ (1.5, 1.5, 3, 4)\\mbox{ y }(1, 2.5, 4, 2.5) \\] Veámoslo: cor(X$V1,X$V2,method=&quot;spearman&quot;) ## [1] 0.5 cor(rank(X$V1),rank(X$V2)) ## [1] 0.5 8.5 Contrastes de correlación Como ya hemos comentado, podemos usar la correlación de Pearson \\(r_{xy}\\) de dos vectores \\(x\\) e \\(y\\), formados por los valores de dos variables cuantitativas \\(X,Y\\) medidos sobre una misma muestra de individuos, para estimar la correlación \\(\\rho_{XY}\\) de estas variables poblacionales. Cuando además ambas variables aleatorias son normales, disponemos de una fórmula para calcular intervalos de confianza para la correlación poblacional y de un método para efectuar contrastes de hipótesis con hipótesis nula \\(H_0: \\rho_{XY}=0\\) (“no hay correlación entre \\(X\\) e \\(Y\\)”). No vamos a entrar en los detalles de las fórmulas ni de los teoremas en que se basan, pero es importante que recordéis que la función de R que lleva a cabo dichos contrastes “de correlación” es la función cor.test. En particular, esta función calcula el intervalo de confianza asociado a un contraste de estos: si el contraste es bilateral, es decir, con hipótesis alternativa \\(H_1: \\rho_{XY}\\neq 0\\), el intervalo que produce esta función es el intervalo de confianza usual para \\(\\rho_{XY}\\) con nivel de confianza correspondiente al nivel de significación del contraste. La sintaxis de cor.test es la misma que la del resto de funciones para realizar contrastes de hipótesis básicos: cor.test(x, y, alternative=..., conf.level=...) donde x e y son los dos vectores de datos, que también se pueden especificar mediante una fórmula. Estos dos vectores han de tener la misma longitud, puesto que se entiende que son mediciones sobre el mismo conjunto de individuos. El parámetro alternative puede tomar los tres valores usuales y su valor por defecto es, como siempre, &quot;two.sided&quot;, que corresponde al contraste bilateral, con hipótesis alternativa \\(H_1: \\rho_{XY}\\neq 0\\). Los valores alternative=&quot;greater&quot; y alternative=&quot;less&quot; permiten contrastar si \\(X\\) e \\(Y\\) tienen correlación mayor o menor que 0, respectivamente. Como en el resto de funciones de contrastes, el resultado es una list que, entre otros objetos, contiene: p.value: El p-valor del test. conf.int: Un intervalo de confianza del nivel de confianza especificado. estimate: El valor de la correlación de Pearson (calculado con use=&quot;complete.obs&quot; si algún vector contiene valores NA). Ejemplo 8.10 Queremos contrastar si hay correlación positiva entre el peso de una madre en el momento de la concepción del hijo y el peso de su hijo en el momento de nacer. Para ello vamos a usar la tabla de datos birthwt incluida en el paquete MASS que ya usamos en una lección anterior, que contiene información sobre recién nacidos y sus madres, y que en particular dispone de las variables bwt, que da el peso del recién nacido en gramos, y lwt, que da el peso de la madre en libras en el momento de su última menstruación. Vamos a suponer que ambos pesos siguen distribuciones normales. Si denotamos por \\(X\\) e \\(Y\\) las correspondientes variables poblacionales, queremos realizar el contraste \\[ \\left\\{\\begin{array}{l} H_0: \\rho_{XY}=0\\\\ H_1: \\rho_{XY}&gt;0 \\end{array}\\right. \\] Vamos a usar la función cor.test. library(MASS) cor.test(birthwt$bwt, birthwt$lwt, alternative=&quot;greater&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: birthwt$bwt and birthwt$lwt ## t = 2.585, df = 187, p-value = 0.00525 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.0672064 1.0000000 ## sample estimates: ## cor ## 0.185733 El p-valor 0.005 nos da evidencia estadísticamente significativa de que, en efecto, hay una correlación positiva entre el peso de la madre y el peso del recién nacido. El último valor, el 0.185733 bajo el cor, es la correlación de Pearson de los dos vectores de pesos, y el 95 percent confidence interval es el intervalo de confianza del 95% del contraste unilateral planteado y nos dice que tenemos un 95% de confianza en que la correlación entre el peso de la madre y el peso del recién nacido es superior a 0.067. Podríamos haber obtenido el p-valor del contraste de correlación anterior directamente con la instrucción cor.test(birthwt$bwt, birthwt$lwt, alternative=&quot;greater&quot;)$p.value ## [1] 0.00525209 Si hubiéramos querido calcular un intervalo de confianza del 95% para \\(\\rho_{XY}\\) que repartiera por igual a ambos lados el 5% de probabilidad de no contener su valor real, hubiéramos podido usar el intervalo de confianza del contraste bilateral: cor.test(birthwt$bwt, birthwt$lwt)$conf.int ## [1] 0.044174 0.319981 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 La potencia de un contraste de correlación se calcula con la función pwr.r.test del paquete pwr. En este caso, el tamaño del efecto es simplemente la correlación de Pearson, que se entra en la función mediante el parámetro r. Apliquémosla para calcular la potencia del contraste de correlación anterior: library(pwr) dim(birthwt) ## [1] 189 10 round(cor(birthwt$bwt,birthwt$lwt),4) ## [1] 0.1857 pwr.r.test(n=189,r=0.1857,sig.level=0.05,alternative=&quot;greater&quot;) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 189 ## r = 0.1857 ## sig.level = 0.05 ## power = 0.822373 ## alternative = greater La probabilidad de error de tipo II en este contraste era de un poco menos del 18%. Si quisiéramos realizar este contraste de correlación con una potencia del 90% suponiendo que la magnitud del efecto va ser pequeña, usaríamos primero cohen.ES con test=&quot;r&quot; para determinar qué magnitud del efecto se considera pequeña y a continuación pwr.r.test dejando sin especificar la n: cohen.ES(test=&quot;r&quot;,size=&quot;small&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = r ## size = small ## effect.size = 0.1 pwr.r.test(power=0.9,r=0.1,sig.level=0.05,alternative=&quot;greater&quot;) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 852.647 ## r = 0.1 ## sig.level = 0.05 ## power = 0.9 ## alternative = greater Hubiéramos necesitado datos de al menos 853 recién nacidos. 8.6 Un ejemplo Recordaréis el data frame iris, que tabulaba las longitudes y anchuras de los pétalos y los sépalos de una muestra de flores iris de tres especies. Vamos a extraer una subtabla con sus cuatro variables numéricas, que llamaremos iris_num, y calcularemos sus matrices de covarianzas y correlaciones. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... iris_num=iris[, 1:4] n=dim(iris_num)[1] #Número de filas Su matriz de covarianzas muestrales es: cov(iris_num) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 0.685694 -0.042434 1.274315 0.516271 ## Sepal.Width -0.042434 0.189979 -0.329656 -0.121639 ## Petal.Length 1.274315 -0.329656 3.116278 1.295609 ## Petal.Width 0.516271 -0.121639 1.295609 0.581006 Su matriz de covarianzas “verdaderas” es: cov(iris_num)*(n-1)/n ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 0.6811222 -0.0421511 1.265820 0.512829 ## Sepal.Width -0.0421511 0.1887129 -0.327459 -0.120828 ## Petal.Length 1.2658200 -0.3274587 3.095503 1.286972 ## Petal.Width 0.5128289 -0.1208284 1.286972 0.577133 Su matriz de correlaciones de Pearson es: cor(iris_num) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.000000 -0.117570 0.871754 0.817941 ## Sepal.Width -0.117570 1.000000 -0.428440 -0.366126 ## Petal.Length 0.871754 -0.428440 1.000000 0.962865 ## Petal.Width 0.817941 -0.366126 0.962865 1.000000 Observamos, por ejemplo, una gran correlación de Pearson positiva entre la longitud y la anchura de los pétalos, 0.963, lo que indica una estrecha relación lineal con pendiente positiva entre estas magnitudes. Valdría la pena, entonces, calcular la recta de regresión lineal de una de estas medidas en función de la otra. lm(Petal.Length~Petal.Width, data=iris_num) ## ## Call: ## lm(formula = Petal.Length ~ Petal.Width, data = iris_num) ## ## Coefficients: ## (Intercept) Petal.Width ## 1.08 2.23 En cambio, la correlación de Pearson entre la longitud y la anchura de los sépalos es -0.11757, muy cercana a cero, lo que es señal de que la variación conjunta de las longitudes y anchuras de los sépalos no tiene una tendencia clara. Vamos a ordenar ahora los pares de variables numéricas de iris en orden decreciente de su correlación en valor absoluto, para saber cuáles están más correlacionadas (en positivo o negativo). Para ello, en primer lugar creamos un data frame cuyas filas están formadas por pares diferentes de variables numéricas de iris, su correlación de Pearson y el valor absoluto de esta última, y a continuación ordenamos las filas de este data frame en orden decreciente de estos valores absolutos. Todo esto lo llevamos a cabo en el siguiente bloque de código, que luego explicamos: medidas=names(iris_num) n=length(medidas) #En este caso, n=4 indices=upper.tri(diag(n)) medida1=matrix(rep(medidas, times=n), nrow=n, byrow=FALSE)[indices] medida2=matrix(rep(medidas, times=n), nrow=n, byrow=TRUE)[indices] corrs=as.vector(cor(iris_num))[indices] corrs.abs=abs(corrs) corrs_df=data.frame(medida1, medida2, corrs, corrs.abs) corrs_df_sort=corrs_df[order(corrs_df$corrs.abs, decreasing=TRUE), ] corrs_df_sort ## medida1 medida2 corrs corrs.abs ## 6 Petal.Length Petal.Width 0.962865 0.962865 ## 2 Sepal.Length Petal.Length 0.871754 0.871754 ## 4 Sepal.Length Petal.Width 0.817941 0.817941 ## 3 Sepal.Width Petal.Length -0.428440 0.428440 ## 5 Sepal.Width Petal.Width -0.366126 0.366126 ## 1 Sepal.Length Sepal.Width -0.117570 0.117570 Vemos que el par de variables con mayor correlación de Pearson en valor absoluto son Petal.Length y Petal.Width, como ya habíamos observado, seguidos por Petal.Length y Sepal.Length. Vamos a explicar el código. La función upper.tri, aplicada a una matriz cuadrada \\(M\\), produce la matriz triangular superior de valores lógicos del mismo orden que \\(M\\), cuyas entradas \\((i,j)\\) con \\(i&lt;j\\) son todas TRUE y el resto todas FALSE. Existe una función similar, lower.tri, para producir matrices triangulares inferiores de valores lógicos. upper.tri(diag(4)) ## [,1] [,2] [,3] [,4] ## [1,] FALSE TRUE TRUE TRUE ## [2,] FALSE FALSE TRUE TRUE ## [3,] FALSE FALSE FALSE TRUE ## [4,] FALSE FALSE FALSE FALSE lower.tri(diag(4)) ## [,1] [,2] [,3] [,4] ## [1,] FALSE FALSE FALSE FALSE ## [2,] TRUE FALSE FALSE FALSE ## [3,] TRUE TRUE FALSE FALSE ## [4,] TRUE TRUE TRUE FALSE Ambas funciones disponen del parámetro diag que, igualado a TRUE, define también como TRUE las entradas de la diagonal principal. upper.tri(diag(4), diag=TRUE) ## [,1] [,2] [,3] [,4] ## [1,] TRUE TRUE TRUE TRUE ## [2,] FALSE TRUE TRUE TRUE ## [3,] FALSE FALSE TRUE TRUE ## [4,] FALSE FALSE FALSE TRUE Si \\(M\\) es una matriz y \\(L\\) es una matriz de valores lógicos del mismo orden, M[L] produce el vector construido de la manera siguiente: de cada columna, se queda sólo con las entradas de \\(M\\) cuya entrada correspondiente en \\(L\\) es TRUE, y a continuación concatena estas columnas, de izquierda a derecha, en un vector. Así, por ejemplo, tomemos la matriz \\(M\\) siguiente: M=matrix(1:16, nrow=4, byrow=T) M ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ## [4,] 13 14 15 16 El vector formado por las entradas de su triángulo superior, concatenadas por columnas, se obtiene de la manera siguiente: M[upper.tri(diag(4))] ## [1] 2 3 7 4 8 12 Ahora definimos las matrices siguientes, formadas por 4 copias (la primera por columnas, la segunda, por filas) del vector, al que hemos llamado medidas, de nombres de las variables numéricas de iris: matrix(rep(medidas, times=4), nrow=4, byrow=FALSE) ## [,1] [,2] [,3] [,4] ## [1,] &quot;Sepal.Length&quot; &quot;Sepal.Length&quot; &quot;Sepal.Length&quot; &quot;Sepal.Length&quot; ## [2,] &quot;Sepal.Width&quot; &quot;Sepal.Width&quot; &quot;Sepal.Width&quot; &quot;Sepal.Width&quot; ## [3,] &quot;Petal.Length&quot; &quot;Petal.Length&quot; &quot;Petal.Length&quot; &quot;Petal.Length&quot; ## [4,] &quot;Petal.Width&quot; &quot;Petal.Width&quot; &quot;Petal.Width&quot; &quot;Petal.Width&quot; matrix(rep(medidas, times=4), nrow=4, byrow=TRUE) ## [,1] [,2] [,3] [,4] ## [1,] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [2,] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [3,] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [4,] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; Al aplicar estas dos matrices a la matriz de valores lógicos upper.tri(diag(4)) obtenemos los nombres de las variables correspondientes a las filas y las columnas del triángulo superior, respectivamente, y al aplicar la matriz de correlaciones a esta matriz de valores lógicos, obtenemos sus entradas en este triángulo; en los tres vectores, las entradas siguen el mismo orden. Esto nos permite construir el data frame corrs_df cuyas filas están formadas por pares diferentes de variables numéricas de iris, su correlación de Pearson (columna corrs) y, aplicando abs a esta última variable, dicha correlación en valor absoluto (columna corrs.abs). corrs_df ## medida1 medida2 corrs corrs.abs ## 1 Sepal.Length Sepal.Width -0.117570 0.117570 ## 2 Sepal.Length Petal.Length 0.871754 0.871754 ## 3 Sepal.Width Petal.Length -0.428440 0.428440 ## 4 Sepal.Length Petal.Width 0.817941 0.817941 ## 5 Sepal.Width Petal.Width -0.366126 0.366126 ## 6 Petal.Length Petal.Width 0.962865 0.962865 Finalmente, la función order ordena los valores del vector al que se aplica, en orden decreciente si se especifica el parámetro decreasing=TRUE. Cuando aplicamos un data frame a una de sus variables reordenada de esta manera, reordena sus filas según el orden de esta variable. En este caso hubiéramos conseguido lo mismo con la función sort, pero la función order se puede aplicar a más de una variable del data frame: esto permite ordenar las filas del data frame en el orden de la primera variable de manera que, en caso de empate, queden ordenadas por la segunda variable, y así sucesivamente. 8.7 Representación gráfica de datos multidimensionales La representación gráfica de tablas de datos multidimensionales tiene la dificultad de las dimensiones; para dos o tres variables es sencillo visualizar las relaciones entre las mismas, pero para más variables ya no nos bastan nuestras tres dimensiones espaciales y tenemos que usar algunos trucos, tales como representaciones gráficas conjuntas de pares de variables. La manera más sencilla de representar gráficamente una tabla de datos formada por dos variables numéricas es aplicando la función plot a la matriz de datos o al data frame. Esta función produce el diagrama de dispersión (scatter plot) de los datos: el gráfico de los puntos del plano definidos por las filas de la tabla. A modo de ejemplo, si extrajéramos de la tabla iris una subtabla conteniendo sólo las longitudes y anchuras de los pétalos y quisiéramos visualizar la relación entre estas dimensiones, podríamos dibujar su diagrama de dispersión con el código del bloque siguiente. El resultado es la Figura 8.1, que muestra una clara tendencia positiva: cuanto más largos son los pétalos, más anchos tienden a ser. Esto se corresponde con la correlación de Pearson de 0.963 que hemos obtenido en la sección anterior. iris.pet=iris[ ,c(&quot;Petal.Length&quot;,&quot;Petal.Width&quot;)] plot(iris.pet, pch=20, xlab=&quot;Largo&quot;, ylab=&quot;Ancho&quot;) Figura 8.1: Diagrama de dispersión de las longitudes y anchuras de los pétalos de las flores de la tabla iris. Para tablas de datos de tres columnas numéricas, podemos usar con un fin similar la instrucción scatterplot3d del paquete homónimo, que dibuja un diagrama de dispersión tridimensional. Como plot, se puede aplicar a un data frame o a una matriz; por ejemplo, para representar gráficamente las tres primeras variables numéricas de iris, podríamos usar el código siguiente y obtendríamos la Figura 8.2: library(scatterplot3d) scatterplot3d(iris[ , 1:3], pch=20) Figura 8.2: Diagrama de dispersión tridimensional de las tres primeras columnas de la tabla iris. Podéis consultar la Ayuda de la instrucción para saber cómo modificar su apariencia: cómo ponerle un título, poner nombres adecuados a los ejes, usar colores, cambiar el estilo del gráfico, etc. Una representación gráfica muy popular de las tablas de datos de tres o más columnas numéricas son las matrices formadas por los diagramas de dispersión de todos sus pares de columnas. Si la tabla de datos es un data frame, esta matriz de diagramas de dispersión se obtiene simplemente aplicando la función plot al data frame; por ejemplo, la instrucción plot(iris[ , 1:4]) produce el gráfico de la Figura 8.3. En este gráfico, los cuadrados en la diagonal indican a qué variables corresponden cada fila y cada columna, de manera que podamos identificar fácilmente qué variables compara cada diagrama de dispersión; así, en el diagrama de la primera fila y segunda columna de esta figura, las abscisas corresponden a anchuras de sépalos y las ordenadas a longitudes de sépalos. Observad que la nube de puntos no muestra una tendencia clara y en todo caso ligeramente negativa, lo que se corresponde con la correlación de Pearson entre estas variables de -0.118 que hemos obtenido en la sección anterior. Figura 8.3: Matriz de diagramas de dispersión de la tabla iris. Podemos usar los parámetros usuales de plot para mejorar el gráfico resultante; por ejemplo, podemos usar colores para distinguir las flores según su especie. Así, la instrucción siguiente produce el gráfico de la Figura 8.4. plot(iris[ , 1:4], col=iris$Species, pch=20, cex=0.7) Figura 8.4: Matriz de diagramas de dispersión de la tabla iris, con las especies distinguidas por colores. Para obtener la matriz de diagramas de dispersión de una tabla de datos multidimensional también se puede usar la función pairs: así, pairs(iris[, 1:4]) produce exactamente el mismo gráfico que plot(iris[, 1:4]). La ventaja principal de pairs es que se puede aplicar a una matriz para obtener la matriz de diagramas de dispersión de sus columnas, mientras que plot no. El paquete car incorpora una función que permite dibujar matrices de diagramas de dispersión enriquecidos con información descriptiva extra de las variables de la tabla de datos y que además facilita el control del gráfico resultante, por lo que os recomendamos su uso frente a las funciones básicas plot y pairs. Se trata de la función spm (abreviatura de scatterplotMatrix); por ejemplo, el código siguiente produce el gráfico la Figura 8.5. library(car) spm(iris[ , 1:4], var.labels=c(&quot;Long. Sep.&quot;,&quot;Ancho Sep.&quot;,&quot;Long. Pet.&quot;,&quot;Ancho Pet.&quot;)) Figura 8.5: Una matriz de diagramas de dispersión de la tabla iris producida con la función spm. Observad para empezar que hemos cambiado los nombres que identifican las variables en los cuadrados de la diagonal, con el parámetro var.labels, y que en dichos cuadrados aparecen además unas curvas: se trata de la curva de densidad estimada de la variable correspondiente de la que hablábamos en la Lección ?? de la primera parte del curso. La información gráfica contenida en estos cuadrados de la diagonal se puede modificar con el parámetro diagonal: podemos pedir, por ejemplo, que dibuje un histograma de cada variable (con diagonal=list(method =&quot;histogram&quot;)) o su boxplot (con diagonal=list(method=&quot;boxplot&quot;)) o un QQ-plot (con diagonal=list(method=&quot;qqplot&quot;)). Así, el código siguiente produce el gráfico la Figura 8.6. spm(iris[ , 1:4], var.labels=c(&quot;Long. Sep.&quot;,&quot;Ancho Sep.&quot;,&quot;Long. Pet.&quot;,&quot;Ancho Pet.&quot;), diagonal=list(method=&quot;boxplot&quot;), pch=20,cex=0.75) Figura 8.6: Matriz de diagramas de dispersión de la tabla iris con boxplots en la diagonal. Observad también que los diagramas de dispersión de la matriz producida con spm contienen algunas líneas. La línea recta es la recta de regresión por mínimos cuadrados y, sin entrar en detalle sobre su significado exacto, las curvas discontinuas representan la tendencia de los datos. Podéis eliminar la recta de regresión con regLine=FALSE (no os lo recomendamos) y las curvas discontinuas con smooth=FALSE; si las queréis mantener, consultad la Ayuda de la función para saber cómo cambiar su estilo, color, etc. A veces querremos agrupar los datos de las variables numéricas de una tabla de datos. Los motivos serán los mismos que cuando se trata de una sola variable: por ejemplo, si los datos son aproximaciones de valores reales, o si son muy heterogéneos. Cuando tenemos dos variables emparejadas agrupadas, se pueden representar gráficamente las frecuencias de sus pares de clases mediante un histograma bidimensional, que divide el conjunto de todos los pares de valores en rectángulos definidos por los pares de intervalos e indica sobre cada rectángulo su frecuencia absoluta, por ejemplo mediante colores o intensidades de gris (dibujar barras verticales sobre las regiones es una mala idea, las de delante pueden ocultar las de detrás). Hay muchos paquetes de R que ofrecen funciones para dibujar histogramas bidimensionales; aquí explicaremos la función hist2d del paquete gplots. Su sintaxis básica es hist2d(x,y, nbins=..., col=...) donde: x e y son los vectores de primeras y segundas coordenadas de los puntos. Si son las dos columnas de un data frame de dos variables numéricas, lo podemos entrar en su lugar. nbins sirve para indicar los números de clases: podemos igualarlo a un único valor, y tomará ese número de clases sobre cada vector, o a un vector de dos entradas que indiquen el número de clases de cada vector. col sirve para especificar los colores a usar. Por defecto, los rectángulos vacíos aparecen de color negro, y el resto se colorean con tonalidades de rojo, de manera que los tonos más cálidos indican frecuencias mayores. Además, podemos usar los parámetros usuales de plot para poner un título, etiquetar los ejes, etc. A modo de ejemplo, vamos a dibujar el histograma bidimensional de las longitudes y anchuras de los pétalos de las flores iris, agrupando ambas dimensiones en los números de clases que da la regla de Freedman-Diaconis (y que calcula la función nclass.FD): library(gplots) hist2d(iris$Petal.Length, iris$Petal.Width, nbins=c(nclass.FD(iris$Petal.Length),nclass.FD(iris$Petal.Width))) Obtenemos (junto con una serie de información en la consola que hemos omitido) la Figura 8.7, que podéis comparar con el diagrama de dispersión de los mismos datos de la Figura 8.1. Figura 8.7: Histograma bidimensional de longitudes y anchuras de pétalos de flores iris. En los histogramas bidimensionales con muchas regiones de diferentes frecuencias, es conveniente usar de manera adecuada los colores para representarlas. Una posibilidad es usar el paquete RColorBrewer, que permite elegir esquemas de colores bien diseñados. Las dos funciones básicas son: brewer.pal(n,&quot;paleta predefinida&quot;), que carga en un vector de colores (una paleta) una secuencia de \\(n\\) colores de la paleta predefinida en el paquete. Los nombres y contenidos de todas las paletas predefinidas que se pueden usar en esta función se obtienen, en la ventana de gráficos, ejecutando la instrucción display.brewer.all(). Por ejemplo, la paleta de colores de la Figura 8.8 se define con el código siguiente: brewer.pal(11,&quot;Spectral&quot;) Figura 8.8: Paleta brewer.pal(11,“Spectral”) colorRampPalette(brewer.pal(...))(m), produce una nueva paleta de \\(m\\) colores a partir del resultado de brewer.pal, interpolando nuevos colores. Luego se puede usar la función rev para invertir el orden de los colores, lo que es conveniente en los histogramas bidimensionales si queremos que las frecuencias bajas correspondan a tonos azules y las frecuencias altas a tonos rojos. Así, la paleta de colores que se define con rev(colorRampPalette(brewer.pal(11,&quot;Spectral&quot;))(50)) es la de la Figura 8.9. Figura 8.9: Paleta rev(colorRampPalette(brewer.pal(11,“Spectral”))(50)) Vamos a usar esta última paleta en un histograma bidimensional de la tabla de alturas de padres e hijos recogidas por Karl Pearson en 1903 y que tenemos guardada en el url https://raw.githubusercontent.com/AprendeR-UIB/Material/master/pearson.txt; el resultado es la Figura 8.10. library(RCurl) df_pearson=read.table(text=getURL(&quot;https://raw.githubusercontent.com/AprendeR-UIB/Material/master/pearson.txt&quot;),header=TRUE) hist2d(df_pearson, nbins=30, col=rev(colorRampPalette(brewer.pal(11,&quot;Spectral&quot;))(50))) Figura 8.10: Histograma bidimensional de las alturas de padres e hijos recogidas por Karl Pearson. Para terminar, veamos cómo producir un gráfico conjunto de un histograma bidimensional y los dos histogramas unidimensionales. Se trata de una modificación del gráfico similar explicado en http://www.everydayanalytics.ca/2014/09/5-ways-to-do-2d-histograms-in-r.html, el cual a su vez se inspira en un gráfico de la p. 62 de Computational Actuarial Science with R de Arthur Charpentier (Chapman and Hall/CRC, 2014). Considerad la función siguiente, cuyos parámetros son un data frame df de dos variables y un número n de clases, común para las dos variables: hist.doble=function(df,n){ par.anterior=par() h1=hist(df[,1], breaks=n, plot=F) h2=hist(df[,2], breaks=n, plot=F) m=max(h1$counts, h2$counts) par(mar=c(3,3,1,1)) layout(matrix(c(2,0,1,3), nrow=2, byrow=T), heights=c(1,3), widths=c(3,1)) hist2d(df, nbins=n, col=rev(colorRampPalette(brewer.pal(11,&quot;Spectral&quot;))(50))) par(mar=c(0,2,1,0)) barplot(h1$counts, axes=F, ylim=c(0, m), col=&quot;red&quot;) par(mar=c(2,0,0.5,1)) barplot(h2$counts, axes=F, xlim=c(0, m), col=&quot;red&quot;, horiz=T) par.anterior} Entonces, la instrucción hist.doble(df_pearson,25) produce la Figura 8.11. Figura 8.11: Histograma bidimensional con histogramas unidimensionales de las alturas de padres e hijos recogidas por Karl Pearson. Algunas explicaciones sobre el código, por si lo queréis modificar: Hemos “simulado” los histogramas mediante diagramas de barras de sus frecuencias absolutas, para poder dibujar horizontal el de la segunda variable. El parámetro axes=FALSE en los barplot indica que no dibuje sus ejes de coordenadas. La función par establece los parámetros generales básicos de los gráficos. Como con esta función los modificamos, guardamos los parámetros anteriores en par.anterior y al final los restauramos. El parámetro mar de la función par sirve para especificar, por este orden, los márgenes inferior, izquierdo, superior y derecho de la próxima figura, en números de líneas. La instrucción layout divide la figura a producir en sectores con la misma estructura que la matriz de su primer argumento. Dentro de esta matriz, cada entrada indica qué figura de las próximas se ha de situar en ese sector. Las alturas y amplitudes relativas de los sectores se especifican con los parámetros heights y widths, respectivamente. Así, la instrucción layout(matrix(c(2,0,1,3),nrow=2,byrow=T), heights=c(1,3),widths=c(3,1)) divide la figura en 4 sectores. Los sectores de la izquierda serán el triple de anchos que los de la derecha (widths=c(3,1)), y los sectores inferiores serán el triple de altos que los superiores (heights=c(1,3)). En estos sectores, R dibujará los próximos gráficos según el esquema definido por la matriz del argumento: \\[ \\left(\\begin{array}{cc} \\mbox{segundo} &amp; \\mbox{ninguno}\\\\ \\mbox{primero} &amp; \\mbox{tercero} \\end{array}\\right). \\] 8.8 Guía rápida sapply(data_frame,función) aplica la función a las columnas del data_frame. scale sirve para aplicar una transformación lineal a una matriz o a un data frame. Sus parámetros son: center: especifica el vector que restamos a sus columnas; por defecto, el vector de medias muestrales. scale: especifica el vector por el que dividimos sus columnas; por defecto, el vector de desviaciones típicas muestrales. cov, aplicada a dos vectores, calcula su covarianza muestral; aplicada a un data frame o a una matriz, calcula su matriz de covarianzas muestrales. Dispone del parámetro use, que: Para dos vectores: Igualado a &quot;complete.obs&quot;, calcula las covarianzas teniendo en cuenta sólo sus observaciones completas (las posiciones en las que ninguno de los dos vectores tiene un NA). Para más de dos vectores: Igualado a &quot;pairwise.complete.obs&quot;, calcula la covarianza de cada par de columnas teniendo en cuenta sólo sus observaciones completas, independientemente del resto de la tabla; es decir, como si en el cálculo de la covarianza de cada par de columnas usáramos use=&quot;complete.obs&quot;, sin tener en cuenta que forman parte de una tabla de datos con más columnas. Igualado a &quot;complete.obs&quot;, calcula las covarianzas de las columnas teniendo en cuenta sólo las filas completas de toda la matriz. cor, aplicada a dos vectores, calcula su correlación de Pearson; aplicada a un data frame o a una matriz, calcula su matriz de correlaciones de Pearson. Se puede usar el parámetro use de cov. Usando el parámetro method=&quot;spearman&quot; calcula la correlación (o la matriz de correlaciones, si se aplica a un data frame o a una matriz) de Spearman. cor.test realiza un contraste de correlación, con hipótesis nula que la correlación poblacional sea 0. Su sintaxis es la usual en funciones de contrastes. pwr.r.test del paquete pwr, sirve para calcular la potencia de un contraste de correlación. Sus parámetros son: n, el tamaño de las muestras; r, su correlación de Pearson; sig.level, el nivel de significación; power, la potencia; y alternative, el tipo de contraste. Si se entran los valores de tres de los cuatro primeros parámetros, se obtiene el cuarto. upper.tri, aplicada a una matriz cuadrada M, produce la matriz triangular superior de valores lógicos del mismo orden que M. Con el parámetro diag=TRUE se impone que el triángulo de valores TRUE incluya la diagonal principal. lower.tri, aplicada a una matriz cuadrada M, produce la matriz triangular inferior de valores lógicos del mismo orden que M. Dispone del mismo parámetro diag=TRUE. order ordena el primer vector al que se aplica, desempatando empates mediante el orden de los vectores subsiguientes a los que se aplica; el parámetro decreasing=TRUE sirve para especificar que sea en orden decreciente. plot, aplicado a un data frame de dos variables numéricas, dibuja su diagrama de dispersión; aplicado a un data frame de más de dos variables numéricas, produce la matriz formada por los diagramas de dispersión de todos sus pares de variables. pairs es equivalente a plot en el sentido anterior, y se puede aplicar a matrices. spm del paquete cars, produce matrices de dispersión más informativas y fáciles de modificar. scatterplot3d del paquete scatterplot3d, dibuja diagramas de dispersión tridimensionales. hist2d del paquete gplots, dibuja histogramas bidimensionales. Dispone de los parámetros específicos siguientes: nbins: indica los números de clases. col: especifica la paleta de colores que ha de usar para representar las frecuencias. brewer.pal(n,&quot;paleta predefinida&quot;) del paquete RColorBrewer, carga en una paleta de colores una secuencia de n colores de la paleta predefinida en dicho paquete. colorRampPalette(brewer.pal(...))(m) del paquete RColorBrewer, genera una nueva paleta de \\(m\\) colores a partir del resultado de brewer.pal, interpolando nuevos colores. display.brewer.all() del paquete RColorBrewer, muestra los nombres y contenidos de todas las paletas predefinidas en dicho paquete. par sirve para establecer los parámetros generales básicos de los gráficos. layout divide en sectores la figura a producir, para que pueda incluir varios gráficos independientes simultáneamente. 8.9 Ejercicios Modelo de test (1) Considerad la matriz de datos \\[\\left(\\begin{array}{ccc} 10.6 &amp; 2.4 &amp; 7.5 \\\\ 7.4 &amp; 3.7 &amp; 10.9\\\\ 10.7 &amp; 2.6 &amp; 9.6 \\\\ 8.4 &amp; 4.9 &amp; 9.9\\\\16.7 &amp; 6.2 &amp; 13.2 \\\\ 11.3 &amp; 4.3 &amp; 7.7\\end{array} \\right).\\] Calculad la entrada (4,2) de su matriz de datos tipificada, redondeada a 3 cifras decimales y sin ceros innecesarios a la derecha. (2) Considerad la matriz de datos \\[\\left(\\begin{array}{ccc} 10.6 &amp; 2.4 &amp; 7.5 \\\\ 7.4 &amp; 3.7 &amp; 10.9\\\\ 10.7 &amp; 2.6 &amp; 9.6 \\\\ 8.4 &amp; 4.9 &amp; 9.9\\\\16.7 &amp; 6.2 &amp; 13.2 \\\\ 11.3 &amp; 4.3 &amp; 7.7\\end{array}\\right).\\] Calculad la covarianza muestral \\(\\widetilde{s}_{3,2}\\), redondeada a 3 cifras decimales y sin ceros innecesarios a la derecha. (3) Considerad la matriz de datos \\[\\left(\\begin{array}{ccc} 10.6 &amp; 2.4 &amp; 7.5 \\\\ 7.4 &amp; 3.7 &amp; 10.9\\\\ 10.7 &amp; 2.6 &amp; 9.6 \\\\ 8.4 &amp; 4.9 &amp; 9.9\\\\16.7 &amp; 6.2 &amp; 13.2 \\\\ 11.3 &amp; 4.3 &amp; 7.7\\end{array}\\right).\\] Calculad la correlación de Pearson \\(r_{3,2}\\), redondeada a 3 cifras decimales y sin ceros innecesarios a la derecha. (4) Usando la función cor.test, realizad el contraste bilateral de correlación entre el perímetro del tronco y la altura de los cerezos negros americanos usando la muestra del dataframe trees que viene con la instalación básica de R. Dad el p-valor redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha, e indicad si la conclusión, con un nivel de significación del 5%, es que hay correlación o no entre estas dos variables, escribiendo SI o NO, según corresponda. Separad el p-valor de la conclusión con un único espacio en blanco. (5) Calculad la correlación de Spearman de los vectores \\(x=(4,8,6,9,5,9 ,4,7,10, 8)\\) e \\(y=(0,6,2,1,4,4,3,7,11,5)\\). Dad el resultado redondeado a 3 cifras decimales sin ceros innecesarios a la derecha. (6) ¿Cuál de las cuatro matrices siguientes es la matriz de covarianzas de una tabla de datos de 2 columnas y 5 filas? Solo hay una. \\[ \\begin{array}{l} A=\\left(\\begin{array}{cc} 0.7 &amp; 3 \\cr 3 &amp; 1.2 \\end{array}\\right)\\\\ B=\\left(\\begin{array}{cc} 0.6 &amp; 0.8\\cr -0.8 &amp; 0.6 \\end{array}\\right)\\\\ C=\\left(\\begin{array}{cc} 0.6 &amp; 0.8\\cr 0.8 &amp; -0.6 \\end{array}\\right)\\\\ D=\\left(\\begin{array}{ccccc} 0.7 &amp; 0.2 &amp; 1.3 &amp; 0.5&amp; -0.1\\cr 0.2 &amp; 0.7 &amp; -0.3 &amp; -0.1 &amp;-0.1\\cr 1.3 &amp; -0.3 &amp; 3.1 &amp; 1.3 &amp; 0.4\\cr 0.5&amp; -0.1&amp; 1.3 &amp; 0.6 &amp; 0.2\\cr -0.1 &amp; -0.1 &amp; 0.4 &amp; 0.2&amp; 2.9\\end{array}\\right) \\end{array} \\] Respuestas al test (1) 0.673 Nosotros lo hemos calculado con X=matrix(c(10.6,2.4,7.5,7.4,3.7,10.9,10.7,2.6,9.6,8.4,4.9,9.9,16.7,6.2,13.2,11.3,4.3,7.7), nrow=6, byrow=TRUE) n=dim(X)[1] X.tip.Ex=scale(X)*sqrt(n/(n-1)) round(X.tip.Ex[4,2],3) ## [1] 0.673 (2) 2.114 Nosotros lo hemos calculado con X=matrix(c(10.6,2.4,7.5,7.4,3.7,10.9,10.7,2.6,9.6,8.4,4.9,9.9,16.7,6.2,13.2,11.3,4.3,7.7), nrow=6, byrow=TRUE) round(cov(X)[3,2],3) ## [1] 2.114 (3) 0.692 Nosotros lo hemos calculado con X=matrix(c(10.6,2.4,7.5,7.4,3.7,10.9,10.7,2.6,9.6,8.4,4.9,9.9,16.7,6.2,13.2,11.3,4.3,7.7), nrow=6, byrow=TRUE) round(cor(X)[3,2],3) ## [1] 0.692 (4) 0.003 SI Nosotros lo hemos calculado con round(cor.test(trees$Height,trees$Girth)$p.value,3) ## [1] 0.003 (5) 0.488 Nosotros lo hemos calculado con x=c(4,8,6,9,5,9 ,4,7,10, 8) y=c(0,6,2,1,4,4,3,7,11,5) round(cor(x,y,method=&quot;spearman&quot;),3) ## [1] 0.488 (6) A "],
["chap-ANOVA.html", "Lección 9 ANOVA básico 9.1 Los modelos del ANOVA en R 9.2 ANOVA de un factor 9.3 ANOVA de bloques completos aleatorios 9.4 ANOVA de dos vías 9.5 Condiciones del ANOVA 9.6 Comparaciones de pares de medias 9.7 Métodos no paramétricos 9.8 Guía rápida 9.9 Ejercicios", " Lección 9 ANOVA básico En esta lección explicamos cómo efectuar con R los ANOVA básicos que se estudian en cursos introductorios de estadística inferencial: de uno y dos factores y de bloques completos aleatorios. El tema central de la lección son los aspectos técnicos del ANOVA con R y los tests posteriores de comparación de pares de medias. Incluimos además una sección con algunas instrucciones que permiten contrastar las condiciones necesarias sobre los datos para que un contraste ANOVA tenga sentido y una sección con algunos contrastes no paramétricos alternativos al ANOVA que se puedan usar justamente cuando no tiene sentido realizar un ANOVA. 9.1 Los modelos del ANOVA en R Los modelos a los que se aplica un ANOVA u otras muchas funciones, como por ejemplo la función lm para calcular la recta de regresión lineal, se especifican en R mediante fórmulas. El operador básico para construir una fórmula es la tilde, ~. Las fórmulas suelen tener la forma Y~modelo, donde la Y es un vector y el modelo es una combinación de vectores o factores que representa el modelo con el que queremos explicar el vector Y (en palabras técnicas, al que queremos ajustar los datos del vector Y). Por ejemplo, para calcular la recta de regresión por mínimos cuadrados de un vector Y respecto de un vector X, usamos lm(Y~X). Esto significa que aplicamos la función lm a la fórmula Y~X que indica que queremos explicar Y en función de X. En los ANOVA que consideramos en esta lección se usan cuatro tipos de fórmulas. Concretamente, si X es una variable numérica y F1 y F2 son dos factores: La fórmula X~F1 se usa para indicar el ANOVA de un factor, F1, de la variable X. La fórmula X~F1+F2 se usa para indicar el ANOVA de dos factores, F1 y F2, de la variable X, sin tener en cuenta la interacción entre los factores; es decir, suponiendo que sus efectos se suman, sin que haya interacción entre los mismos. Es el tipo de fórmula que se usa en los ANOVA de bloques. La fórmula X~F1*F2 se usa para indicar el ANOVA de dos factores, F1 y F2, de la variable X, teniendo en cuenta además la posible interacción entre estos factores. La fórmula X~F1:F2 se usa para indicar el ANOVA de un factor que tiene como niveles los pares ordenados de niveles de F1 y F2. La función básica de R para realizar un ANOVA es aov. Su sintaxis genérica es aov(fórmula, data=...) con los argumentos siguientes: fórmula: Una fórmula que especifique un modelo de ANOVA. data: Opcional, sirve para especificar, si es necesario, el data frame al que pertenecen las variables utilizadas en la fórmula. Así, por ejemplo, si tenemos un data frame llamado DF, con una variable numérica X y un factor Fact, para realizar el ANOVA de la variable X respecto del factor Fact con la función aov podríamos entrar aov(X~Fact, data=DF) o aov(DF$X~DF$Fact) Otra posibilidad, que por ahora no usaremos pero sí más adelante, es aplicar la función anova (no la confundáis con aov) al resultado de lm. La sintaxis sería entonces anova(lm(fórmula, data=...)) 9.2 ANOVA de un factor Para ilustrar el ANOVA de un factor con R utilizaremos un experimento en el que se quiso determinar si cuatro dietas concretas tenían alguna influencia en el tiempo de coagulación de la sangre en mamíferos.8 Para ello se escogieron 24 animales, se repartieron de manera aleatoria en 4 grupos de 6 ejemplares cada uno, y a cada grupo se le asignó de manera aleatoria una de las 4 dietas objeto de estudio, que indicaremos con A, B, C y D. Al cabo de un cierto tiempo se midió el tiempo de coagulación de la sangre en estos animales. Los resultados (redondeados a enteros) se muestran en la Tabla 9.1. Observad que estamos ante un diseño experimental de un solo factor: la dieta. Tabla 9.1: Tiempos de coagulación bajo diferentes dietas. A B C D 62 63 68 56 60 67 66 62 63 71 71 60 59 64 67 61 63 65 68 63 59 66 68 64 Para contrastar si los tiempos medios de coagulación son los mismos para las cuatro dietas o no, vamos a realizar un ANOVA de estos datos. Para ello, lo primero que tenemos que hacer es recoger los datos en un data frame, formado por una variable numérica con los valores de la tabla y un factor cuyos niveles sean las diferentes dietas, de manera que a cada valor en la variable numérica le corresponda la dieta con la que se obtuvo. Nosotros entraremos los datos de la tabla anterior por filas, y entonces el factor tendrá que ser A, B, C, D, A, B, C, D, A... Otra opción sería entrar los datos por columnas, y entonces entraríamos como factor A, A, A, A, A, A, B, B, B,... Entramos pues los datos de la tabla, por filas: coag=c(62,63,68,56,60,67,66,62,63,71,71,60,59,64,67,61,63,65,68,63,59,66,68,64) Definimos de manera adecuada el factor de las dietas, como 6 copias de la fila A,B,C,D: diet=rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), times=6) Finalmente, definimos el data frame y comprobamos que es correcto: coagulacion=data.frame(coag,diet) str(coagulacion) ## &#39;data.frame&#39;: 24 obs. of 2 variables: ## $ coag: num 62 63 68 56 60 67 66 62 63 71 ... ## $ diet: Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;: 1 2 3 4 1 2 3 4 1 2 ... head(coagulacion) ## coag diet ## 1 62 A ## 2 63 B ## 3 68 C ## 4 56 D ## 5 60 A ## 6 67 B Para analizar la igualdad de los tiempos medios de coagulación bajo las cuatro dietas, realizaremos un ANOVA de la variable coag separándola según (ajustándola a) el factor diet. Antes de empezar, es conveniente visualizar los datos para hacernos una idea de su distribución; por ejemplo, por medio de un diagrama de cajas con una caja por cada nivel: boxplot(coag~diet, data=coagulacion) Observad que también hemos empleado una fórmula para indicar que queremos los diagramas de cajas de la variable coag separada por la variable diet del data frame coagulacion. Obtenemos la Figura 9.1, donde podemos ver que las medias muestrales para las dietas A y C son muy diferentes, y que en cambio seguramente no podremos rechazar que las medias poblacionales de las dietas A y D sean iguales. Por lo tanto, el resultado que esperamos del ANOVA es que nos permita rechazar la hipótesis nula de que los cuatro tiempos medios de coagulación son iguales. Ahora bien, hasta que no realicemos el ANOVA no sabremos si las diferencias que observamos en este diagrama son estadísticamente significativas o no. Figura 9.1: Diagrama de cajas de los tiempos de coagulación según las diferentes dietas. Como hemos comentado, para realizar el ANOVA deseado, entramos: aov(coag~diet, data=coagulacion) ## Call: ## aov(formula = coag ~ diet, data = coagulacion) ## ## Terms: ## diet Residuals ## Sum of Squares 228 112 ## Deg. of Freedom 3 20 ## ## Residual standard error: 2.36643 ## Estimated effects may be unbalanced El resultado no es la tabla del ANOVA. Para obtenerla, hay que aplicar summary al resultado de aov: summary(aov(coag~diet, data=coagulacion)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## diet 3 228 76.0 13.6 4.7e-05 *** ## Residuals 20 112 5.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 El resultado de esta función summary es la tabla ANOVA usual: En la primera columna, dos etiquetas: el nombre del factor, en este caso diet, y Residuals, que representa los errores o residuos del ANOVA. La segunda columna, etiquetada Df, nos da los grados de libertad correspondientes al factor (su número de niveles menos 1) y a los residuos (el número de individuos en la tabla, menos el número de niveles del factor). La tercera columna, Sum Sq, nos muestra las sumas de los cuadrados del factor, \\(SS_{Tr}\\), y de los residuos, \\(SS_E\\). La cuarta columna, Mean Sq, contiene las medias de los cuadrados del factor, \\(MS_{Tr}\\), y de los residuos, \\(MS_E\\). La quinta columna, F value, nos da el valor del estadístico de contraste. En la sexta columna, Pr(&gt;F), aparece el p-valor del contraste. La séptima columna, sin etiqueta, indica el nivel de significación del p-valor según el código usual, explicado en la última línea del resultado. A mayor número de asteriscos, más significativo es el p-valor y por lo tanto es más fuerte la evidencia de que las medias comparadas no son todas iguales. En nuestro caso, hemos obtenido un p-valor del orden de \\(4.7\\times 10^{-5}\\). Esto nos permite rechazar la hipótesis nula y concluir que no todos los tiempos medios de coagulación para las diferentes dietas consideradas son iguales, como intuíamos. Recordad que esto no significa que hayamos obtenido evidencia de que todos los tiempos medios de coagulación son diferentes, solo de que hay al menos un par de dietas que dan tiempos medios diferentes. Si ahora queremos determinar de qué pares de dietas se trata, tendremos que realizar algún test de comparaciones de pares de medias: véase la Sección 9.6. Para ahorrar espacio vertical, en lo que queda de lección vamos a eliminar los símbolos que marcan los niveles de significación de los p-valores. Para ello entramos la siguiente instrucción: options(show.signif.stars=FALSE) A partir de ahora, y mientras no cerremos la sesión, estos símbolos no aparecerán más en las tablas ANOVA. summary(aov(coag~diet, data=coagulacion)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## diet 3 228 76.0 13.6 4.7e-05 ## Residuals 20 112 5.6 Si en algún momento de la sesión queréis volver a ver los códigos de significación, basta que entréis options(show.signif.stars=TRUE) Como hemos comentado, una manera alternativa de realizar un ANOVA es mediante anova(lm(...)). Con esta construcción obtenemos directamente la tabla, sin necesidad de aplicarle summary. anova(lm(coag~diet, data=coagulacion)) ## Analysis of Variance Table ## ## Response: coag ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## diet 3 228 76.0 13.57 4.66e-05 ## Residuals 20 112 5.6 Para extraer los datos de la tabla ANOVA obtenida con la función summary(aov(...)), y así poder operar directamente con ellos, hay que añadirle los sufijos adecuados. Consultemos su estructura. tabla=summary(aov(coag~diet, data=coagulacion)) str(tabla) ## List of 1 ## $ :Classes &#39;anova&#39; and &#39;data.frame&#39;: 2 obs. of 5 variables: ## ..$ Df : num [1:2] 3 20 ## ..$ Sum Sq : num [1:2] 228 112 ## ..$ Mean Sq: num [1:2] 76 5.6 ## ..$ F value: num [1:2] 13.6 NA ## ..$ Pr(&gt;F) : num [1:2] 4.66e-05 NA ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;summary.aov&quot; &quot;listof&quot; Vemos que la tabla ANOVA obtenida con summary(aov(...)) es una list formada por un solo objeto (List of 1), que a su vez es un data frame cuyas variables son las columnas numéricas de la tabla. Por lo tanto, para obtener una columna de estas, primero hemos de añadir el sufijo [[1]], que extrae el data frame de la list, y a continuación el sufijo $columna correspondiente a la columna de la tabla ANOVA que nos interesa. Por ejemplo, la columna de las sumas de los cuadrados es: tabla[[1]]$&quot;Sum Sq&quot; ## [1] 228 112 De manera similar, como el p-valor es el primer elemento de la columna Pr(&gt;F), lo obtenemos con: tabla[[1]]$&quot;Pr(&gt;F)&quot;[1] ## [1] 4.65847e-05 Fijaos en que, como los nombres de estas columnas contienen espacios en blanco u otros símbolos no aceptados en los nombres de objetos de R (recordad la Lección 2 del primer volumen), hay que especificarlos entre comillas. El resultado de anova(lm(...)) ya es directamente un data frame, por lo que para extraer los valores de la tabla ANOVA que produce podemos usar la sintaxis usual de los data frames. tabla2=anova(lm(coag~diet, data=coagulacion)) str(tabla2) ## Classes &#39;anova&#39; and &#39;data.frame&#39;: 2 obs. of 5 variables: ## $ Df : int 3 20 ## $ Sum Sq : num 228 112 ## $ Mean Sq: num 76 5.6 ## $ F value: num 13.6 NA ## $ Pr(&gt;F) : num 4.66e-05 NA ## - attr(*, &quot;heading&quot;)= chr &quot;Analysis of Variance Table\\n&quot; &quot;Response: coag&quot; tabla2$&quot;Sum Sq&quot; ## [1] 228 112 tabla2$&quot;Pr(&gt;F)&quot;[1] ## [1] 4.65847e-05 Veamos otro ejemplo de ANOVA de un factor. Ejemplo 9.1 En un experimento, se estudió el efecto de seis dietas sobre el crecimiento de crías de conejo doméstico.9 Los datos obtenidos están recogidos en la tabla de datos rabbit del paquete faraway. library(faraway) str(rabbit) ## &#39;data.frame&#39;: 30 obs. of 3 variables: ## $ treat: Factor w/ 6 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,..: 6 2 3 3 1 2 3 6 4 1 ... ## $ gain : num 42.2 32.6 35.2 40.9 40.1 38.1 34.6 34.3 37.5 44.9 ... ## $ block: Factor w/ 10 levels &quot;b1&quot;,&quot;b10&quot;,&quot;b2&quot;,..: 1 1 1 3 3 3 4 4 4 5 ... Consultando la Ayuda de rabbit nos enteramos de que el factor treat indica la dieta, con niveles a,b,c,d,e,f, y de que la variable gain indica el aumento de peso; la variable block, que indica la camada, es irrelevante en este análisis concreto. Si dibujamos el diagrama de cajas de los crecimientos para cada dieta, obtenemos la Figura 9.2, donde no observamos grandes diferencias en los crecimientos medios. boxplot(gain~treat, data=rabbit) Figura 9.2: Diagrama de cajas de los crecimientos de crías de conejo según las diferentes dietas. Para determinar si hay diferencia en los aumentos medios de peso bajo las seis dietas, realizaremos un ANOVA de la variable gain ajustándola al factor treat. summary(aov(gain~treat, data=rabbit)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 5 293 58.7 1.89 0.13 ## Residuals 24 747 31.1 El p-valor es 0.134, lo que indica que, efectivamente, no hay evidencia de que las dietas den lugar a crecimientos medios diferentes. Si hubiéramos querido obtener solo el p-valor, podríamos haber entrado: summary(aov(gain~treat, data=rabbit))[[1]]$&quot;Pr(&gt;F)&quot;[1] ## [1] 0.134212 Es muy importante tener presente que, al usar las instrucciones aov o anova(lm(...)) para realizar un ANOVA, se tiene que emplear un factor (o varios, en las próximas secciones) para separar la variable numérica en subpoblaciones. Veamos un ejemplo de lo que pasa si nos descuidamos en este punto. Ejemplo 9.2 En un experimento se estudió el efecto de la vitamina C en el crecimiento de los dientes.10 Se tomaron 60 cobayas y se trató cada uno de ellos con una combinación diferente de dosis de vitamina C (0.5, 1 o 2 mg) y método de suministro de la misma (mediante zumo de naranja o como ácido ascórbico) durante 6 semanas, y se cuantificó el crecimiento de sus dientes durante dicho período (más en concreto, se midió la longitud media de sus odontoblastos al final del mismo). El resultado es una tabla de 60 datos, que aparecen recogidos en el fichero ToothGrowth del paquete UsingR. library(UsingR) str(ToothGrowth) ## &#39;data.frame&#39;: 60 obs. of 3 variables: ## $ len : num 4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ... ## $ supp: Factor w/ 2 levels &quot;OJ&quot;,&quot;VC&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ dose: num 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ... La Ayuda de ToothGrowth nos dice que la variable len contiene la longitud media final de los odontoblastos del animal, la variable supp el método de suministro (OJ indica zumo de naranja, orange juice, y VC indica ácido ascórbico puro, vitamine C), y la variable dose la dosis. Nos vamos a olvidar por el momento de la variable supp, y vamos a contrastar si la dosis de vitamina C tiene influencia en el crecimiento de los dientes. Para ello realizamos un ANOVA de un factor. summary(aov(len~dose, data=ToothGrowth)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dose 1 2224 2224 105 1.2e-14 ## Residuals 58 1228 21 ¿Veis algo raro? Hemos comentado que se usaron tres dosis diferentes, por lo que el número de grados de libertad en la fila del factor, dose, tendría que ser 2, y no 1. ¿Qué ha pasado? Muy sencillo: dose es una variable numérica, y para usar aov el factor ha de ser eso, un factor. Así que lo primero que tenemos que hacer es convertir esta variable en un factor. Los haremos sobre un duplicado de la tabla ToothGrowth original, a la que llamaremos ToothGrowth2. ToothGrowth2=ToothGrowth ToothGrowth2$dose=as.factor(ToothGrowth2$dose) str(ToothGrowth2) ## &#39;data.frame&#39;: 60 obs. of 3 variables: ## $ len : num 4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ... ## $ supp: Factor w/ 2 levels &quot;OJ&quot;,&quot;VC&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ dose: Factor w/ 3 levels &quot;0.5&quot;,&quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... summary(aov(len~dose, data=ToothGrowth2)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dose 2 2426 1213 67.4 9.5e-16 ## Residuals 57 1026 18 Ahora está bien. El p-valor prácticamente 0 nos permite rechazar la hipótesis nula y concluir que hay dosis de vitamina C que dan lugar a diferentes crecimientos medios de los odontoblastos. 9.3 ANOVA de bloques completos aleatorios Para ilustrar este tipo de ANOVA, analizaremos un experimento sobre producción de penicilina.11 En dicho experimento, se evaluaron cuatro procesos diferentes para determinar si había diferencias en su efectividad. Los cuatro procesos usaban una técnica de cultivo sumergido y empleaban agua de macerado de maíz como fuente de nitrógeno orgánico. Como la composición de este líquido puede afectar la producción final de penicilina, para evaluar los cuatro procesos se prepararon 5 mezclas diferentes de agua de macerado de maíz, de cada mezcla se tomaron 4 muestras y se asignaron de manera aleatoria a los cuatro procesos de producción. Los resultados obtenidos son los de la Tabla 9.2, y aparecen recogidos en la tabla de datos penicillin del paquete faraway. Tabla 9.2: Producción de penicilina bajo diferentes procesos de producción y mezclas. Mezcla A B C D 1 89 88 97 94 2 84 77 92 79 3 81 87 87 85 4 87 92 89 84 5 79 81 80 88 Observad que estamos ante un diseño experimental de bloques completos aleatorios. Los bloques son las mezclas de agua de macerado de maíz y los tratamientos (los procesos de producción) se han asignado de manera aleatoria a las unidades experimentales (las muestras) de cada bloque, de tal manera que cada bloque contiene exactamente una unidad experimental para cada tratamiento. Este es un ejemplo paradigmático de uso de bloques: para evitar la influencia de la variable “extraña” dada por la composición del agua de macerado de maíz, que puede influir en la producción, se escogen al azar unas mezclas y se prueban todos los procesos de manera independiente sobre cada mezcla. Demos un vistazo a esta tabla de datos. str(penicillin) ## &#39;data.frame&#39;: 20 obs. of 3 variables: ## $ treat: Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;: 1 2 3 4 1 2 3 4 1 2 ... ## $ blend: Factor w/ 5 levels &quot;Blend1&quot;,&quot;Blend2&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... ## $ yield: num 89 88 97 94 84 77 92 79 81 87 ... head(penicillin) ## treat blend yield ## 1 A Blend1 89 ## 2 B Blend1 88 ## 3 C Blend1 97 ## 4 D Blend1 94 ## 5 A Blend2 84 ## 6 B Blend2 77 Según la Ayuda de penicillin, la variable treat es el proceso de producción, con valores A, B, C y D; la variable blend es la mezcla usada (los bloques), con valores Blend1 a Blend5; y la variable numérica yield es un valor que cuantifica la producción de penicilina. Veamos cómo son los diagramas de cajas de la producción de penicilina separada por procesos de producción y por mezclas: boxplot(yield~treat, data=penicillin) Figura 9.3: Diagrama de cajas de la producción de penicilina bajo los diferentes procesos de producción. boxplot(yield~blend, data=penicillin) Figura 9.4: Diagrama de cajas de la producción de penicilina según las diferentes mezclas. Vemos que no hay mucha diferencia entre las producciones para los diferentes procesos (sin tener en cuenta los bloques), y que sí que hay algunas diferencias en las producciones según la composición del agua de macerado de maíz; ya hemos comentado que es bien sabido que su composición influye en la producción. Si realizamos un ANOVA de un factor para cada uno de estos dos factores por separado, obtenemos resultados consistentes con esta observación visual: summary(aov(yield~treat, data=penicillin)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 3 70 23.3 0.76 0.53 ## Residuals 16 490 30.6 summary(aov(yield~blend, data=penicillin)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## blend 4 264 66.0 3.34 0.038 ## Residuals 15 296 19.7 El p-valor del ANOVA separando por procesos de producción es 0.532, lo que indica que no podemos rechazar la hipótesis nula de que los procesos de producción tengan igual productividad media (sin tener en cuenta las mezclas), y el p-valor del ANOVA separando por mezclas es 0.038, lo que, con un nivel de significación del 5%, nos permite rechazar que todas las mezclas produzcan la misma cantidad media de penicilina. Pero precisamente, el hecho de que la mezcla influya en la producción es lo que hace que el primer ANOVA no sea fiable: a lo mejor el efecto de las mezclas enmascara las diferencias en las productividades de los procesos bajo estudio. Para determinarlo, vamos a realizar un ANOVA de bloques, es decir, de dos factores y efectos acumulados. summary(aov(yield~treat+blend, data=penicillin)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 3 70 23.3 1.24 0.339 ## blend 4 264 66.0 3.50 0.041 ## Residuals 12 226 18.8 El p-valor que nos interesa en esta tabla es el de la fila treat: es 0.3387, por lo que en este experimento de bloques tampoco detectamos evidencia de que haya diferencias en la productividad media de los procesos estudiados. El segundo p-valor de la tabla, 0.0407, es el del ANOVA de bloques que resulta de intercambiar los bloques y el tratamiento, tomando las mezclas como el factor a analizar y los procesos de producción como los bloques: summary(aov(yield~blend+treat, data=penicillin)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## blend 4 264 66.0 3.50 0.041 ## treat 3 70 23.3 1.24 0.339 ## Residuals 12 226 18.8 Podemos extraer los resultados de una tabla de un ANOVA de bloques generada por summary(aov(...)) añadiendo los mismos sufijos que en el caso de un factor. Así, por ejemplo, los p-valores son: tabla=summary(aov(yield~treat+blend, data=penicillin)) tabla[[1]]$&quot;Pr(&gt;F)&quot; ## [1] 0.3386581 0.0407462 NA y si solo nos interesa el del factor treat: tabla[[1]]$&quot;Pr(&gt;F)&quot;[1] ## [1] 0.338658 Veamos otro ejemplo de ANOVA de bloques completos aleatorios. Ejemplo 9.3 Para estudiar si las diferentes actividades de evaluación llevadas a cabo en la asignatura de Matemáticas I tienen una dificultad similar (o mejor dicho, para mirar de confirmar nuestras sospechas de que no es así), escogimos una muestra aleatoria de 15 estudiantes de Biología o Bioquímica que se hubieran presentado al control 2 de dicha asignatura en el curso 2012-2013, y anotamos las notas obtenidas por estos estudiantes en los apartados de Tests, Talleres, Ejercicios de Casa, Control 1 y Control 2. Si obtenemos evidencia de que no todas las medias de las notas de las diferentes actividades fueron iguales, podremos concluir que no todas las actividades tienen la misma dificultad. Los datos obtenidos son los de la tabla 9.3. Tabla 9.3: Notas obtenidas por 15 estudiantes en diferentes actividades de evaluación. Estudiante Tests Casa Talleres Control.1 Control.2 1 48 42 85 31 20 2 94 86 100 52 48 3 98 94 93 93 90 4 60 58 71 66 46 5 52 56 79 64 24 6 78 73 84 80 95 7 84 84 94 83 70 8 83 76 99 51 55 9 75 55 70 85 48 10 24 49 57 19 55 11 47 47 64 44 35 12 53 42 78 50 8 13 82 93 92 66 30 14 66 62 74 24 56 15 49 40 74 35 21 Como podemos observar, se trata de un experimento de bloques completos aleatorios: hemos escogido de manera aleatoria unos bloques (los estudiantes) y para cada estudiante hemos apuntado el valor de cada uno de los niveles del factor a estudiar (las notas en las diferentes actividades).`Este diseño es el adecuado para este problema, puesto que hay una gran variabilidad en las notas obtenidas por estudiantes diferentes, desde matrículas a suspensos. Al tomar bloques, es decir, al considerar todas las notas de un conjunto aleatorio fijo de estudiantes, eliminamos el efecto de esta variabilidad. Vamos a construir un data frame con estos datos. Este data frame tendrá tres variables: notas}, con las notas obtenidas por los estudiantes;acts, con los tipos de actividades de evaluación realizados; ybloques, con el indicador de cada estudiante. Entraremos las notas siguiendo las filas de la tabla anterior, y por lo tanto tenemos que construir estos dos factores entrando sus niveles en el orden adecuado: el factoractsha de estar formado por 15 copias de la fila de actividades, y el factorbloquesha de estar formado por 5 copias de1, 5 copias de2, y así hasta 5 copias de15. Y recordad quebloques` ha de ser un factor, puesto que queremos usarlo en un ANOVA. notas=c(48,42,85,31,20,94,86,100,52,48,98,94,93,93,90,60,58, 71,66,46,52,56,79,64,24,78,73,84,80,95,84,84,94,83,70,83,76, 99,51,55,75,55,70,85,48,24,49,57,19,55,47,47,64,44,35,53,42, 78,50,8,82,93,92,66,30,66,62,74,24,56,49,40,74,35,21) acts=rep(c(&quot;Tests&quot;,&quot;Casa&quot;,&quot;Talleres&quot;,&quot;Control 1&quot;,&quot;Control 2&quot;), times=15) bloques=as.factor(rep(1:15,each=5)) notas.bloques=data.frame(notas,acts,bloques) str(notas.bloques) ## &#39;data.frame&#39;: 75 obs. of 3 variables: ## $ notas : num 48 42 85 31 20 94 86 100 52 48 ... ## $ acts : Factor w/ 5 levels &quot;Casa&quot;,&quot;Control 1&quot;,..: 5 1 4 2 3 5 1 4 2 3 ... ## $ bloques: Factor w/ 15 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 2 2 2 2 2 ... head(notas.bloques) ## notas acts bloques ## 1 48 Tests 1 ## 2 42 Casa 1 ## 3 85 Talleres 1 ## 4 31 Control 1 1 ## 5 20 Control 2 1 ## 6 94 Tests 2 Vamos a dibujar los diagramas de cajas de las notas de las diferentes actividades y de los diferentes estudiantes: boxplot(notas.bloques$notas~notas.bloques$acts) Figura 9.5: Diagrama de cajas de las notas en las diferentes actividades. boxplot(notas.bloques$notas~notas.bloques$bloques) Figura 9.6: Diagrama de cajas de las notas de los estudiantes. Podemos observar diferencias en las notas de algunas actividades: por ejemplo, las notas del control 2 son muy inferiores a las de los talleres. Asimismo, como nos temíamos, vemos una gran variabilidad en las notas de los estudiantes. Realicemos ahora el ANOVA. summary(aov(notas~acts+bloques, data=notas.bloques)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## acts 4 9646 2412 13.13 1.3e-07 ## bloques 14 19430 1388 7.56 1.5e-08 ## Residuals 56 10283 184 El p-valor que nos interesa es el de la primera fila, etiquetada con el factor cuyos niveles queremos comparar, en este caso acts. Este p-valor es muy pequeño, del orden de \\(10^{-7}\\), lo que es evidencia de que, como nos temíamos, no todas las notas medias de las actividades de evaluación fueron iguales. 9.4 ANOVA de dos vías Para ilustrar el ANOVA de dos vías, es decir, de dos factores que pueden interacccionar, analizaremos en primer lugar los resultados de un experimento sobre el efecto de venenos y antídotos.12 En este experimento se usaron tres venenos y cuatro antídotos, y cada combinación de veneno y antídoto se administró a cuatro ratas elegidas de manera aleatoria e independiente. A continuación, se anotó el tiempo de supervivencia de cada animal, en unidades de 10 horas. Se trata, pues, de un diseño experimental de dos factores, en el que cada individuo ha sido asignado al azar a cada nivel de los dos factores. Por otro lado, no podemos descartar a priori que haya interacción entre venenos y antídotos, puesto que un antídoto puede ser más efectivo para un veneno que para otro. El objetivo del experimento era contrastar la igualdad de los tiempos medios de supervivencia según el veneno, según el antídoto, y según la combinación veneno-antídoto. Los datos obtenidos en este experimento se han recogido en la tabla rats del paquete faraway. Pero ahora tenemos un problema con este data frame. Si cargamos en una misma sesión varios paquetes que contengan objetos con el mismo nombre, R entiende en cada momento que ese nombre refiere al objeto del paquete que se haya cargado más recientemente. Como en esta lección hemos cargado el paquete UsingR después del paquete faraway, en estos momentos rats refiere a una tabla de datos del paquete survival que se ha cargado con UsingR y que no tiene nada que ver con los datos de este experimento. Una solución posible sería en este punto volver a cargar el paquete faraway. Otra opción más rápida es definir rats como el objeto rats de faraway por medio de la instrucción rats=faraway::rats. La construcción paquete::objeto invoca el objeto (una función, una tabla de datos,…) del paquete y sirve para eliminar ambigüedades como la que nos encontramos aquí. Vamos a cargar y explorar estos datos. rats=faraway::rats str(rats) ## &#39;data.frame&#39;: 48 obs. of 3 variables: ## $ time : num 0.31 0.82 0.43 0.45 0.45 1.1 0.45 0.71 0.46 0.88 ... ## $ poison: Factor w/ 3 levels &quot;I&quot;,&quot;II&quot;,&quot;III&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ treat : Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;: 1 2 3 4 1 2 3 4 1 2 ... head(rats) ## time poison treat ## 1 0.31 I A ## 2 0.82 I B ## 3 0.43 I C ## 4 0.45 I D ## 5 0.45 I A ## 6 1.10 I B En la Ayuda de rats nos enteramos de que el factor treat contiene el antídoto, con niveles A, B, C y D; el factor poison contiene el veneno, con niveles I, II y III; y la variable numérica time contiene el tiempo de supervivencia de las ratas. Veamos cómo son los diagramas de cajas de estos tiempos, separados por venenos y por antídotos. boxplot(time~poison, data=rats) Figura 9.7: Diagrama de cajas de los tiempos de supervivencia según los venenos. boxplot(time~treat, data=rats) Figura 9.8: Diagrama de cajas de los tiempos de supervivencia según los antídotos Parece que hay diferencias en los tiempos medios de supervivencia tanto para los diferentes venenos como para los diferentes antídotos. Podemos también dibujar un diagrama de cajas de los tiempos de supervivencia separándolos por combinaciones de veneno y antídoto. La instrucción para hacerlo es la siguiente (observad la fórmula del argumento): boxplot(time~poison:treat, data=rats) Figura 9.9: Diagrama de cajas de los tiempos de supervivencia según las combinaciones de veneno y antídoto. El código para efectuar el ANOVA de dos vías del tiempo de supervivencia de las ratas bajo los efectos combinados de los venenos y los antídotos es el siguiente: summary(aov(time~poison*treat, data=rats)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## poison 2 1.033 0.517 23.22 3.3e-07 ## treat 3 0.921 0.307 13.81 3.8e-06 ## poison:treat 6 0.250 0.042 1.87 0.11 ## Residuals 36 0.801 0.022 En la última columna de la tabla que obtenemos, el primer p-valor es el del contraste de la igualdad de tiempos medios de supervivencia según el veneno (poison): su valor, \\(3.3\\times 10^{-7}\\), nos permite concluir que estos tiempos medios no son todos iguales. El segundo p-valor es el del contraste de la igualdad de tiempos medios de supervivencia según el antídoto (treat): que valga \\(3.8\\times 10^{-6}\\) indica que estos tiempos medios tampoco son todos iguales. Finalmente, el tercer p-valor es el del contraste de interacción entre veneno y antídoto (poison:treat). Recordad que existe dicha interacción cuando los cambios en la variable de respuesta (en este ejemplo, el tiempo de supervivencia) originados por uno de los factores no son los mismos para diferentes niveles del otro factor: es decir, en el contexto de este experimento, si algún antídoto es más (o menos) efectivo contra algún veneno que contra otro. Como aquí este p-valor vale 0.112, no obtenemos evidencia estadísticamente significativa de dicha interacción: aceptamos que cada uno de los antídotos tiene un efecto similar sobre cada veneno (pero no todos los antídotos son igual de efectivos). Observaréis que la tabla del ANOVA que produce R con esta fórmula no contiene la fila que permite contrastar si hay diferencia entre las medias de las poblaciones definidas por combinaciones de niveles, uno por cada factor: en este ejemplo, por las combinaciones de veneno y antídoto. Si se desea obtener esta fila, basta realizar un ANOVA de un factor que combine los dos factores del experimento. summary(aov(time~poison:treat, data=rats)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## poison:treat 11 2.204 0.2004 9.01 2e-07 ## Residuals 36 0.801 0.0222 Vemos que también hay diferencias estadísticamente significativas entre las combinaciones veneno–antídoto. En el caso del ANOVA de dos vías, es interesante dibujar el gráfico de interacción entre factores. En un gráfico de interacción del factor \\(F_1\\) respecto del factor \\(F_2\\) en el ANOVA de una variable numérica \\(X\\) respecto de estos factores, se dibuja una línea quebrada para cada nivel de \\(F_1\\). Esta línea une, mediante segmentos, los valores medios que toma la variable \\(X\\) en nuestra muestra para cada nivel de \\(F_2\\) en el nivel de \\(F_1\\) correspondiente. Si no hay ninguna interacción entre estos factores, las líneas resultantes serán paralelas. Cuanto más se alejen de ser paralelas, más evidencia de interacción habrá entre estos dos factores. Estos gráficos de interacción se dibujan en R con la instrucción interaction.plot aplicada, por este orden, a los dos factores y la variable numérica. Así, pues, para obtener el gráfico de interacción de la variable antídoto respecto de la variable veneno en el ANOVA anterior, entraríamos la siguiente instrucción: interaction.plot(rats$treat, rats$poison, rats$time) Figura 9.10: Gráfico de interacción del veneno y el antídoto. En el gráfico resultante observamos que los tres niveles del veneno producen líneas casi paralelas, aunque se observa una ligera interacción: la pendiente de la recta correspondiente al veneno II entre los valores medios de C y D es mucho mayor que la de las rectas correspondientes a los otros dos venenos, lo que indica que en el experimento el antídoto D ha sido menos efectivo contra el veneno II que contra los otros dos. No obstante, la diferencia no ha sido estadísticamente significativa. Este diagrama no ha quedado muy bonito, ya que las variables aparecen en él con sus nombres completos, y los factores no aparecen en la leyenda ordenados alfabéticamente. Una posibilidad para mejorarlo es cambiar las etiquetas de los ejes y la leyenda (eliminándola con legend=FALSE y añadiéndola a nuestro gusto con la función legend). Ya que estamos, dibujaremos algo más gruesas las líneas. En general, la instrucción interaction.plot admite todos los parámetros de plot más algunos de específicos que podéis conocer consultando su Ayuda. interaction.plot(rats$treat, rats$poison, rats$time, legend=FALSE, xlab=&quot;Antídoto&quot;, ylab=&quot;Tiempo medio de supervivencia&quot;, lwd=c(2,2,2)) legend(&quot;topright&quot;, lty=1:3, cex=0.6, title=&quot;Venenos&quot;, legend=c(&quot;I&quot;,&quot;II&quot;,&quot;III&quot;)) Figura 9.11: Gráfico de interacción del veneno y el antídoto. Veamos otro ejemplo de ANOVA de dos vías. Ejemplo 9.4 En el Ejemplo 9.2 hablábamos de un cierto experimento sobre el efecto de la vitamina C en el crecimiento de los dientes, y de la tabla de datos ToothGrowth del paquete UsingR que recoge los datos obtenidos en ese experimento. Ahora vamos a realizar un ANOVA de dos factores sobre esta tabla de datos para contrastar la influencia en dicho crecimiento de la dosis de vitamina C y del método de suministrarla, teniendo en cuenta que sus efectos pueden interaccionar. Recordad de aquel ejemplo que, si queremos llevar a cabo un ANOVA sobre esta tabla que involucre la variable dose que contiene la dosis, primero hay que convertir esta variable en un factor. Volveremos a copiar la tabla ToothGrowth en una nueva tabla ToothGrowth2 y modificaremos en esta tabla dicha variable. ToothGrowth2=ToothGrowth ToothGrowth2$dose=as.factor(ToothGrowth2$dose) str(ToothGrowth2) ## &#39;data.frame&#39;: 60 obs. of 3 variables: ## $ len : num 4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ... ## $ supp: Factor w/ 2 levels &quot;OJ&quot;,&quot;VC&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ dose: Factor w/ 3 levels &quot;0.5&quot;,&quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... Demos ahora una ojeada a los diagramas de cajas de esta tabla, separando las longitudes de los odontoblastos por dosis, por suministro y por ambos. boxplot(len~dose, data=ToothGrowth2) Figura 9.12: Diagrama de cajas de las longitudes de los odontoblastos según la dosis de vitamina C. boxplot(len~supp, data=ToothGrowth2) Figura 9.13: Diagrama de cajas de las longitudes de los odontoblastos según el método de suministro de vitamina C. boxplot(len~dose:supp, data=ToothGrowth2) Figura 9.14: Diagrama de cajas de las longitudes de los odontoblastos según la dosis y el método de suministro. Podemos observar diferencias tanto según la dosis y como según el método de suministro. ¿Serán significativas? summary(aov(len~supp*dose, data=ToothGrowth2)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## supp 1 205 205 15.57 0.00023 ## dose 2 2426 1213 92.00 &lt; 2e-16 ## supp:dose 2 108 54 4.11 0.02186 ## Residuals 54 712 13 summary(aov(len~supp:dose, data=ToothGrowth2)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## supp:dose 5 2740 548 41.6 &lt;2e-16 ## Residuals 54 712 13 El resultado muestra que, efectivamente, hay diferencias significativas en el crecimiento de los dientes tanto según la dosis (p-valor prácticamente 0) como según el método de suministro (p-valor \\(2\\times 10^{-4}\\)) o la combinación de ambos (p-valor prácticamente 0), y también que hay evidencia estadística de interacción entre los dos factores (p-valor 0.022). El gráfico de interacción entre las dosis y el método de suministro que produce la instrucción siguiente refleja esta interacción: la línea correspondiente a la dosis de 2 mg no es paralela a las otras. interaction.plot(ToothGrowth2$supp, ToothGrowth2$dose, ToothGrowth2$len, legend=FALSE, xlab=&quot;Método de suministro&quot;, ylab=&quot;Crecimiento medio&quot;, lwd=c(2,2,2)) legend(&quot;topright&quot;,lty=1:3,cex=0.6,title=&quot;Dosis&quot;,legend=c(&quot;0.5&quot;,&quot;1&quot;,&quot;2&quot;)) Figura 9.15: Gráfico de interacción de la dosis de vitamina C y el método de suministro. El hecho de haber detectado interacción entre los dos factores hace que nos interese estudiar el efecto de los niveles de un factor en el otro. Esto se puede llevar a cabo mediante varios ANOVA de un factor. En primer lugar, vamos estudiar, para cada método de suministro de vitamina C, si hay diferencias entre los crecimientos medios de los odontoblastos según la dosis cuando la vitamina C se suministra mediante ese método. Para ello vamos a realizar dos ANOVA de un factor, la dosis, sobre dos data frames extraídos de ToothGrowth2, uno con las filas donde supp toma el valor OJ y otro con las filas donde supp toma el valor VC. Vamos extraer estas subtablas de datos usando la instrucción subset. Recordad que la construcción subset(df, condición) define un data frame con las filas del data frame df que cumplen la condición. TG.OJ=subset(ToothGrowth2, supp==&quot;OJ&quot;) TG.VC=subset(ToothGrowth2, supp==&quot;VC&quot;) Ahora realizamos los dos ANOVA: summary(aov(len~dose, data=TG.OJ)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dose 2 885 443 31.4 8.9e-08 ## Residuals 27 380 14 summary(aov(len~dose, data=TG.VC)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dose 2 1649 825 67.1 3.4e-11 ## Residuals 27 332 12 En ambos casos el p-valor es muy pequeño, lo que indica que para cada tipo de suministro de vitamina C, el crecimiento medio de los dientes varía con la dosis. En este tipo de análisis, hay que tener en cuenta que si realizamos \\(N\\) contrastes independientes en un mismo experimento, cada uno de ellos con un nivel de significación \\(\\alpha\\), la probabilidad de rechazar en alguno de ellos la hipótesis nula si en todos es verdadera es \\[ 1-(1-\\alpha)^N\\approx N\\alpha. \\] Por lo tanto, si queremos garantizar un nivel de significación global \\(\\alpha\\), hay que realizar cada contraste con nivel de significación \\(\\alpha/N\\). (Para ser exactos, si llamamos \\(\\alpha_c\\) al nivel de significación de cada contraste necesario para obtener un nivel de significación global \\(\\alpha\\), se cumple que \\(1-(1-\\alpha_c)^N=\\alpha\\), de donde podemos despejar \\(\\alpha_c\\) y obtenemos \\(\\alpha_c=1-\\sqrt[N]{1-\\alpha}\\), que es un poco mayor que \\(\\alpha/N\\). Por ejemplo, para \\(\\alpha=0.05\\) y \\(N=20\\) tenemos que \\(\\alpha_c=0.0341\\) mientras que \\(\\alpha/N=0.025\\).) En este ejemplo concreto, si hubiéramos deseado un nivel de significación global 0.05, deberíamos haber realizado cada uno de los ANOVA con un nivel de significación 0.025, lo que no afecta a las conclusiones, ya que los p-valores han sido realmente muy pequeños. También podemos estudiar, para cada dosis, si hay diferencias entre las medias de crecimiento según el método de suministro. Como solo hay dos métodos de suministro, para cada dosis tanto da realizar un ANOVA o un simple test t suponiendo que las varianzas son iguales (ya que es una de las condiciones para poder realizar el ANOVA). TG.0.5=subset(ToothGrowth2, dose==0.5) summary(aov(len~supp, data=TG.0.5)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## supp 1 138 137.8 10.1 0.0053 ## Residuals 18 247 13.7 TG.1=subset(ToothGrowth2, dose==1) summary(aov(len~supp, data=TG.1)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## supp 1 176 175.8 16.3 0.00078 ## Residuals 18 195 10.8 TG.2=subset(ToothGrowth2, dose==2) summary(aov(len~supp, data=TG.2)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## supp 1 0 0.03 0 0.96 ## Residuals 18 271 15.03 En este caso, para garantizar un nivel de significación global de 0.05, hay que realizar cada contraste con un nivel de significación de 0.05/3=0.017. Obtenemos evidencia de que hay diferencia en el crecimiento medio de los odontoblastos según el tipo de suministro solo para las dosis de 0.5 y 1 mg, que es donde hemos obtenido p-valores inferiores al nivel de significación. Si la variable dose hubiera tenido, pongamos, 20 niveles en vez de 3, definir una subtabla para cada nivel y efectuar el ANOVA correspondiente hubiera sido muy largo. Para automatizar cálculos como estos, lo mejor es definir una función que calcule el p-valor del ANOVA de una variable respecto de un factor sobre la subtabla definida por un nivel variable \\(x\\) de un segundo factor, y luego aplicar esta función al vector de los niveles de este segundo factor. En este ejemplo concreto usaríamos el código siguiente. Anova_dosis=function(x){ summary(aov(len~supp, data=subset(ToothGrowth2, dose==x)))[[1]]$&quot;Pr(&gt;F)&quot;[1] } round(sapply(levels(ToothGrowth2$dose), FUN=Anova_dosis),4) ## 0.5 1 2 ## 0.0053 0.0008 0.9637 9.5 Condiciones del ANOVA Los resultados de un ANOVA nos permiten extraer conclusiones solo si las poblaciones a las que se aplica cumplen una serie de condiciones; las dos básicas son la normalidad y la igualdad de varianzas. La condición de normalidad requiere que cada nivel de cada factor defina una distribución normal, es decir, que cada muestra aleatoria simple de un tratamiento provenga de una población normal. La condición de igualdad de varianzas requiere que todas estas distribuciones tengan la misma varianza. En la Lección 6 ya estudiamos algunos tests específicos de normalidad: los tests de Kolmogorov-Smirnov-Lillliefors, de Anderson-Darling, de Shapiro-Wilk y de D’Agostino-Pearson. Cada uno tiene sus ventajas y sus inconvenientes: el primero es muy popular, por ser una variante del test de Kolmogorov-Smirnov; el segundo, detecta mejor las discrepancias en los extremos; el test de Shapiro-Wilk es más potente para muestras grandes y menos sensible a repeticiones que los dos anteriores; y el test de D’Agostino-Pearson permite repeticiones de datos, pero no se puede usar con muestras de tamaño inferior a 20. Veamos algunos ejemplos de aplicación de tests de normalidad en el contexto de un ANOVA. Ejemplo 9.5 Queremos contrastar si los tiempos de coagulación bajo cuatro dietas dados en la Tabla 9.1 provienen de distribuciones normales. Como hay algunas repeticiones de valores en las columnas de la tabla y no llegan a los 20 valores cada una, usaremos el test de Shapiro-Wilk que, recordemos, está implementado en la función shapiro.test. Teníamos los datos guardados en un data frame llamado coagulacion. str(coagulacion) ## &#39;data.frame&#39;: 24 obs. of 2 variables: ## $ coag: num 62 63 68 56 60 67 66 62 63 71 ... ## $ diet: Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;: 1 2 3 4 1 2 3 4 1 2 ... Apliquemos la función shapiro.test a los tiempos de coagulación para cada dieta: coagA=coagulacion[coagulacion$diet==&quot;A&quot;,]$coag shapiro.test(coagA)$p.value ## [1] 0.11303 coagB=coagulacion[coagulacion$diet==&quot;B&quot;,]$coag shapiro.test(coagB)$p.value ## [1] 0.522705 coagC=coagulacion[coagulacion$diet==&quot;C&quot;,]$coag shapiro.test(coagC)$p.value ## [1] 0.237537 coagD=coagulacion[coagulacion$diet==&quot;D&quot;,]$coag shapiro.test(coagD)$p.value ## [1] 0.522705 Los cuatro p-valores que obtenemos son grandes, mayores que 0.05, lo que quiere decir que en ninguno de los cuatro tests podemos rechazar la hipótesis nula: no hay evidencia de que los tiempos de coagulación bajo alguna de las cuatro dietas no sigan leyes normales, por lo que aceptamos que sí las siguen. Recordad además que, como hemos efectuado cuatro contrastes, para garantizar un nivel de significación global de 0.05 hemos de realizar cada uno con un nivel de significación de 0.05/4=0.0125: es decir, que con p-valores por encima de este nivel de significación no podríamos rechazar ninguna hipótesis nula. ¿Cómo podríamos haber automatizado el cálculo de estos p-valores, para no tener que definir “a mano” cada vector de tiempos de coagulación? Con el código siguiente, donde definimos una función Test.SW que calcula el p-valor del test de Shapiro-Wilk y la aplicamos con un aggregate a los tiempos de coagulación del data frame coagulacion separados según la dieta. De esta manera obtenemos un data frame cuya columna coag contiene, para cada nivel de diet, el p-valor del test de Shapiro-Wilk para los tiempos de coagulación de la dieta correspondiente. Test.SW=function(x){shapiro.test(x)$p.value} aggregate(coag~diet, data=coagulacion,FUN=Test.SW) ## diet coag ## 1 A 0.113030 ## 2 B 0.522705 ## 3 C 0.237537 ## 4 D 0.522705 Ejemplo 9.6 Vamos a contrastar si en el ANOVA de dos vías del ejemplo de venenos y antídotos explicado en la Sección 9.4 se satisface que cada combinación de veneno y antídoto define una población normal. Para ello, en principio deberíamos repetir 12 veces, una para cada combinación de veneno y antídoto, un par de instrucciones como las siguientes: time.I.A=rats[rats$poison==&quot;I&quot; &amp; rats$treat==&quot;A&quot;, ]$time shapiro.test(time.I.A)$p.value ## [1] 0.0741449 Para evitarnos trabajo, vamos a usar la estrategia explicada al final del ejemplo anterior: aplicaremos la función Test.SW que hemos definido en dicho ejemplo a la variable numérica time del data frame rats separándola por las combinaciones de veneno y antídoto. aggregate(time~poison:treat, data=rats, FUN=Test.SW) ## poison treat time ## 1 I A 0.0741449 ## 2 II A 0.8475641 ## 3 III A 0.5773549 ## 4 I B 0.6998338 ## 5 II B 0.7008372 ## 6 III B 0.1705700 ## 7 I C 0.4050349 ## 8 II C 0.9209111 ## 9 III C 0.9718771 ## 10 I D 0.4273912 ## 11 II D 0.9065096 ## 12 III D 0.6889364 Todos los p-valores que obtenemos son mayores que 0.05 (y de hecho, para garantizar un nivel de significación global de 0.05, deberíamos realizar cada test de Shapiro-Wilk con un nivel de significación 0.05/12=0.0042), por lo que no podemos rechazar que todas las combinaciones de veneno y antídoto definan unos tiempos de supervivencia que sigan leyes normales. Pasemos al contraste de igualdad de varianzas (su nombre técnico es contraste de homocedasticidad), en el que usaremos el llamado test de Bartlett. Supongamos, para fijar ideas, que tenemos \\(k\\) vectores numéricos \\[ x_i=(x_{i1},x_{i2},\\ldots,x_{in_i}),\\quad i=1,\\ldots, k, \\] cada uno de los cuales es una muestra aleatoria simple de una variable aleatoria normal (se supone que lo hemos contrastado previamente) de varianza \\(\\sigma_i^2\\). El contraste que queremos realizar es: \\[ \\left\\{ \\begin{array}{ll} H_0: &amp; \\sigma_1^2 = \\cdots = \\sigma_k^2\\\\ H_1: &amp; \\mbox{$\\sigma_i^2 \\neq \\sigma_j^2$ para algunos $i, j$} \\end{array} \\right. \\] Para realizar este contraste, se usa el estadístico de Bartlett: \\[ K^2 = \\frac{ (N-k)\\ln (\\widetilde{s}_p^2)-\\sum_{i=1}^k (n_i -1)\\ln (\\widetilde{s}_i^2)}{ 1+\\frac{1}{3(k-1)}\\left(\\sum_{i=1}^k \\left(\\frac{1}{n_i -1}\\right)-\\frac{1}{N-k}\\right)}, \\] donde \\[ N=\\sum_{i=1}^k n_i,\\quad \\widetilde{s}_p^2 = \\frac{\\sum_{i=1}^k (n_i -1) \\widetilde{s}_i^2}{N-k},\\quad \\widetilde{s}_i^2 =\\frac{\\sum_{j=1}^{n_i} (x_{ij}-\\overline{x}_{i})^2}{n_i -1},\\ i=1,\\ldots,k. \\] Si las variables poblacionales son normales, este estadístico sigue aproximadamente una distribución \\(\\chi^2_{k-1}\\), y si la hipótesis nula es verdadera, su valor esperado es pequeño (puesto que, en este caso, los valores esperados de \\(\\widetilde{s}_1^2,\\ldots, \\widetilde{s}_k^2\\) y \\(\\widetilde{s}_p^2\\) son todos iguales). El p-valor del contraste es entonces \\(P(\\chi^2_{k-1} &gt; K^2)\\). R tiene una función que efectúa este test, bartlett.test, y puede aplicarse a una fórmula. Por ejemplo, para contrastar si las tres dietas en el ejemplo de la coagulación de la Sección 9.2 definen poblaciones con la misma varianza, podemos entrar bartlett.test(coag~diet, data=coagulacion) ## ## Bartlett test of homogeneity of variances ## ## data: coag by diet ## Bartlett&#39;s K-squared = 1.946, df = 3, p-value = 0.584 Como el p-valor es alto, concluimos que las varianzas de los tiempos de coagulación bajo las tres dietas son la misma. En los ANOVA de dos vías, se ha de comprobar la igualdad de varianzas para las combinaciones de niveles de los dos factores. En la fórmula que se entra al bartlett.test esta combinación no se puede especificar con :, como hemos hecho hasta ahora, sino con interaction. Por ejemplo, para contrastar por medio del test de Bartlett si las 12 combinaciones veneno-dieta en el ejemplo de venenos y antídotos explicado en la Sección 9.4 tienen la misma varianza, tenemos que entrar: bartlett.test(time~interaction(poison,treat), data=rats) ## ## Bartlett test of homogeneity of variances ## ## data: time by interaction(poison, treat) ## Bartlett&#39;s K-squared = 45.14, df = 11, p-value = 4.59e-06 Obtenemos un p-valor de \\(4.6\\times 10^{-6}\\), por lo tanto, hemos de rechazar la igualdad de varianzas: las conclusiones de ese ANOVA de dos vías no tienen en principio ninguna validez, puesto que no hay evidencia de que se cumpla una de las condiciones para realizarlo. Otros tests de igualdad de varianzas son el de Fligner-Killeen, implementado en la función fligner.test (y que tiene la misma particularidad que bartlett.test a la hora de especificar combinaciones de factores) y el test de Levene, implementado en la función leveneTest del paquete car (donde las combinaciones de factores sí que se pueden especificar con :). En cada caso, el test produce un p-valor con el significado usual. El test de Levene es muy popular, porque no requiere que las poblaciones sean normales, pero en el contexto del ANOVA esta propiedad no tiene ningún interés; además, cuando las poblaciones son normales, el test de Bartlett es más potente que el de Levene. 9.6 Comparaciones de pares de medias Si en un ANOVA hemos rechazado la hipótesis nula de la igualdad de todas las medias, es posible que nos interese determinar qué medias son diferentes. Para ello podemos llevar a cabo algunos tests. En esta sección explicamos las instrucciones de R para tres de ellos. 9.6.1 Tests t por parejas En los tests t por parejas, realizamos un contraste de medias para cada par de niveles del factor, teniendo en cuenta que, para garantizar un nivel de significación global \\(\\alpha\\), se ha de ajustar el nivel de significación de cada test. Hay diversos métodos para llevar a cabo este ajuste: los más populares son el método de Bonferroni, el de Holm, que es más potente, y el de Hochberg, que, para contrastes ANOVA que no sean de bloques, aún tiene mayor potencia. La función que efectúa tests t por parejas es pairwise.t.test. Su sintaxis es pairwise.t.test(var.numérica, factor, paired=..., p.adjust.method=...) El valor de p.adjust.method es el método de ajuste de p-valores que deseamos usar, y se ha de entrar entrecomillado. El valor por defecto es el de Holm, indicado por &quot;holm&quot; aunque no es necesario especificarlo. Para usar el de Bonferroni, hay que especificar p.adjust.method=&quot;bonferroni&quot;, y para usar el de Hochberg, p.adjust.method=&quot;hochberg&quot;. Si no queréis ningún ajuste, tenéis que usar p.adjust.method=&quot;none&quot;. Para conocer qué otros métodos se pueden usar, sus ventajas e inconvenientes, consultad la Ayuda de p.adjust. Por lo que refiere al parámetro paired, sirve para indicar si las muestras son independientes (igualándolo a FALSE, que es su valor por defecto) o emparejadas (igualándolo a TRUE); cuando el ANOVA es de bloques, las muestras son emparejadas, y por lo tanto se ha de especificar paired=TRUE. Veamos algunos ejemplos de su uso. Ejemplo 9.7 Para determinar cuáles de las tres dietas en el ejemplo de la coagulación de la Sección 9.2 dan tiempos medios de coagulación diferentes, vamos a efectuar un test t por parejas de Bonferroni. pairwise.t.test(coagulacion$coag,coagulacion$diet,p.adjust.method=&quot;bonferroni&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: coagulacion$coag and coagulacion$diet ## ## A B C ## B 0.009 - - ## C 3e-04 0.953 - ## D 1.000 0.009 3e-04 ## ## P value adjustment method: bonferroni Los p-valores del resultado ya han sido ajustados, y por lo tanto se han de comparar directamente con el nivel de significación \\(\\alpha\\) global. Por ejemplo, con un nivel de significación \\(\\alpha=0.05\\), obtenemos evidencia de que los tiempos medios para los pares de dietas (A,B), (A,C), (B,D), (C,D) son diferentes, y en cambio no hay evidencia de que los tiempos medios para los pares (A,D) y (B,C) sean diferentes. Ejemplo 9.8 Para determinar qué actividades de evaluación de las consideradas en el Ejemplo 9.3 dan notas medias diferentes, vamos a realizar un test t por parejas de Holm. Primero vamos a comprobar que dichas actividades de evaluación definen poblaciones normales, condición necesaria para que los tests t por parejas tengan sentido. Contrastaremos la normalidad de las notas para cada actividad de evaluación con el test de Shapiro-Wilk, y para no trabajar más de la cuenta, usaremos la técnica explicada en el Ejemplo 9.5. Test.SW=function(x){round(shapiro.test(x)$p.value,4)} aggregate(notas~acts, data=notas.bloques, FUN=Test.SW) ## acts notas ## 1 Casa 0.1384 ## 2 Control 1 0.7054 ## 3 Control 2 0.5515 ## 4 Talleres 0.7612 ## 5 Tests 0.5984 Todos los p-valores son altos, los que nos permite aceptar que las notas de cada actividad provienen de una distribución normal. Así pues, podemos realizar el test t de Holm. En este caso tenemos que usar paired=TRUE, puesto que se trata de un ANOVA de bloques. pairwise.t.test(notas.bloques$notas, notas.bloques$acts, paired=TRUE) ## ## Pairwise comparisons using paired t tests ## ## data: notas.bloques$notas and notas.bloques$acts ## ## Casa Control 1 Control 2 Talleres ## Control 1 0.456 - - - ## Control 2 0.027 0.456 - - ## Talleres 0.001 0.004 0.001 - ## Tests 0.456 0.164 0.027 0.004 ## ## P value adjustment method: holm Los p-valores inferiores a 0.05 son los de los pares de actividades “Ejercicios de casa”-“Control 1”, “Control 1”-“Control 2”, “Ejercicios de casa”-“Tests” y “Control 1”-“Tests”. Por lo tanto, a un nivel de significación global de 0.05, no podemos rechazar que las notas medias de estos pares de actividades sean iguales, mientras que en los otros tenemos evidencia de que son diferentes. 9.6.2 Test de Duncan Otro método popular para comparar los pares de medias es el test de Duncan. Con R se puede efectuar con la instrucción duncan.test del paquete agricolae. Su sintaxis básica es duncan.test(aov, &quot;factor&quot;, alpha=..., group=...)$sufijo donde aov es el resultado de la función aov (sin summary) con la que hemos calculado el ANOVA de partida. El factor es el factor del ANOVA, y se ha de entrar entrecomillado y con el mismo nombre que se ha usado en la fórmula del aov. El parámetro alpha sirve para entrar el nivel de significación \\(\\alpha\\): por defecto vale 0.05. group puede valer TRUE o FALSE, y hace que el resultado se presente de forma diferente. El sufijo tiene que ser groups si group=TRUE y comparison si group=FALSE. Este test no se puede usar en el ANOVA de bloques, puesto que supone que las muestras de los tratamientos son independientes. Ejemplo 9.9 Vamos a usar el test de Duncan para determinar qué pares de dietas en el ejemplo de la coagulación de la Sección 9.2 tienen tiempos medios de coagulación diferentes, con un nivel de significación global del 5%. En primer lugar, guardamos en un objeto el ANOVA calculado con aov, y luego aplicamos la función duncan.test a este objeto. Aquí lo haremos dos veces, una para cada posible valor del parámetro group, para poder comparar y comentar cómo se muestran los resultados. No entraremos el parámetro alpha, puesto que usamos su valor por defecto. library(agricolae) anova.coag=aov(coag~diet, data=coagulacion) duncan.test(anova.coag, &quot;diet&quot;, group=FALSE)$comparison ## difference pvalue signif. LCL UCL ## A - B -5 0.0016 ** -7.84997 -2.150032 ## A - C -7 0.0001 *** -9.99151 -4.008491 ## A - D 0 1.0000 -2.84997 2.849968 ## B - C -2 0.1588 -4.84997 0.849968 ## B - D 5 0.0021 ** 2.00849 7.991509 ## C - D 7 0.0001 *** 3.91854 10.081462 duncan.test(anova.coag, &quot;diet&quot;, group=TRUE)$groups ## coag groups ## C 68 a ## B 66 a ## A 61 b ## D 61 b Observemos los resultados: Con group=FALSE y sufijo comparison, obtenemos una tabla donde, para cada pareja de niveles del factor, se da, entre otros datos, un p-valor (la columna p value). Estos p-valores tienen el significado usual. En este ejemplo, de los p-valores se desprende que no podemos rechazar que las medias para los pares A-D y B-C sean iguales (los p-valores son grandes), mientras que, en cambio, hay evidencia significativa de que el resto de los pares de dietas tienen medias diferentes (los p-valores son pequeños). Las dos últimas columnas de la tabla definen intervalos de confianza (del nivel de confianza correspondiente al nivel de significación que hayamos entrado con alpha) para las diferencias de la medias en el orden especificado en la primera columna. Con group=TRUE y sufijo groups, obtenemos una tabla donde se agrupan los niveles según la igualdad de medias. En este caso, asigna las dietas B y C al grupo a y las dietas A y D al grupo b: dietas en el mismo grupo podemos aceptar que tienen medias iguales, dietas en grupos diferentes podemos concluir que tienen medias diferentes. 9.6.3 Método de Tukey El método HSD de Tukey (las siglas HSD refieren a Honestly Significant Difference) es el método más preciso de comparación de parejas de medias para contrastes ANOVA de un factor en los que cada nivel tenga el mismo número de observaciones. Es un test similar al test t, pero usa otro estadístico con una distribución diferente, no vamos a entrar en detalles. Con R se efectúa aplicando la función TukeyHSD al resultado de aov. Para explicar qué información obtenemos con esta función, vamos a aplicar el método de Tukey al ANOVA del ejemplo sobre tiempos de coagulación de la Sección 9.2. anova.coag=aov(coag~diet, data=coagulacion) TukeyHSD(anova.coag) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = coag ~ diet, data = coagulacion) ## ## $diet ## diff lwr upr p adj ## B-A 5.00000e+00 1.17593 8.82407 0.007798 ## C-A 7.00000e+00 3.17593 10.82407 0.000280 ## D-A 1.42109e-14 -3.82407 3.82407 1.000000 ## C-B 2.00000e+00 -1.82407 5.82407 0.476601 ## D-B -5.00000e+00 -8.82407 -1.17593 0.007798 ## D-C -7.00000e+00 -10.82407 -3.17593 0.000280 De esta manera, para cada par de niveles obtenemos, entre otra información: La diferencia de sus medias muestrales, en la columna diff. Los extremos inferior y superior de un intervalo de confianza del 95% para la diferencia de medias poblacionales en el orden dado en la primera columna, en las columnas lwr y upr, respectivamente. El nivel de confianza se puede ajustar con el parámetro conf.level dentro de la función. El p-valor del contraste de igualdad de medias correspondiente, en la columna p adj. En nuestro ejemplo, los intervalos de confianza para las diferencias de medias de los pares A-B, A-C, B-D y C-D no contienen el valor 0 (y los p-valores correspondientes son pequeños), lo que indica que podemos rechazar que estas medias sean iguales. En cambio, los intervalos de confianza para las diferencias de medias de los pares A-D y B-C sí que contienen el 0 (y los p-valores correspondientes son grandes), lo que indica que podemos aceptar que estas medias son iguales. En este caso obtenemos, por lo tanto, la misma conclusión que con el test t de Bonferroni. 9.7 Métodos no paramétricos Cuando no se satisfacen las condiciones para poder aplicar un ANOVA, hay que usar algún otro método que no las requiera. En el caso del ANOVA de una vía, el método alternativo no paramétrico recomendado es el test de Kruskal-Wallis, que generaliza a más de dos muestras el test de Mann-Whitney-Wilcoxon para dos muestras independientes en el mismo sentido que el ANOVA generaliza el test t; en particular, un test de Kruskal-Wallis para un factor con solo dos niveles es equivalente al test de Mann-Whitney. Con R, el test de Kruskal-Wallis se lleva a cabo con la función kruskal.test, con la misma sintaxis que aov. Naturalmente, el resultado no es una tabla ANOVA, porque no estamos haciendo un ANOVA, pero da un p-valor que indica si podemos rechazar o no que las medias por niveles sean todas iguales. Por ejemplo, para efectuar el test de Kruskal-Wallis sobre la tabla de tiempos de coagulación del principio de la Sección 9.2, entraríamos: kruskal.test(coag~diet, data=coagulacion) ## ## Kruskal-Wallis rank sum test ## ## data: coag by diet ## Kruskal-Wallis chi-squared = 17.03, df = 3, p-value = 0.000698 El p-valor nos da evidencia estadísticamente significativa de que no todos los tiempos medios de coagulación son iguales. Para determinar entonces qué pares de dietas dan tiempos medios diferentes, podríamos llevar a cabo un contraste por parejas. En principio no podremos usar un test t por parejas, ya que este presupone las condiciones de normalidad de las poblaciones e igualdad de las varianzas que son necesarias para el ANOVA y que suponemos que no se satisfacen (o habríamos hecho un ANOVA en vez de un test de Kruskal-Wallis). En su lugar, podemos usar un test de Mann-Whitney por parejas, con la función pairwise.wilcox.test. Su sintaxis es la misma que la de pairwise.t.test. En particular, hay que indicar con p.adjust.method el método de ajuste de los p-valores, siendo de nuevo &quot;holm&quot; su valor por defecto. Así, por ejemplo, para realizar un test de Mann-Whitney por parejas con ajuste de Bonferroni, entraríamos: pairwise.wilcox.test(coagulacion$coag,coagulacion$diet,p.adjust.method=&quot;bonferroni&quot;) ## ## Pairwise comparisons using Wilcoxon rank sum test ## ## data: coagulacion$coag and coagulacion$diet ## ## A B C ## B 0.05 - - ## C 0.03 0.63 - ## D 1.00 0.08 0.03 ## ## P value adjustment method: bonferroni Con un nivel de significación de 0.05, obtenemos evidencia de que los pares de dietas (A,B), (A,C) y (C,D) dan tiempos medios de coagulación diferentes, y no podemos rechazar que los otros pares de dietas den el mismo tiempo de coagulación. El resultado ha sido ligeramente diferente del test t por parejas, donde también obteníamos evidencia de diferencia de medias entre las dietas B y D. Recordad que, en general, los métodos no paramétricos tienen potencia menor y por lo tanto les cuesta más detectar diferencias. Por lo que refiere al ANOVA de bloques completos aleatorios, el método no paramétrico alternativo más popular es el test de Friedman, que generaliza a más de dos muestras el test de Wilcoxon para muestras emparejadas. Está implementado en la función friedman.test, cuya sintaxis es de nuevo la misma que la de aov excepto que la suma de los tratamientos y los bloques no se indica en la fórmula con un signo de suma sino con una barra vertical. Por ejemplo, para efectuar el test de Friedman sobre la tabla de datos de notas en actividades de evaluación del Ejemplo 9.3, entraríamos: friedman.test(notas~acts|bloques, data=notas.bloques) ## ## Friedman rank sum test ## ## data: notas and acts and bloques ## Friedman chi-squared = 30.05, df = 4, p-value = 4.79e-06 El p-valor nos indica que hay evidencia estadísticamente significativa de que algunas notas medias fueron diferentes. Para determinar cuáles, podemos llevar a cabo un test de Wilcoxon por parejas, usando la función pairwise.wilcox.test con paired=TRUE. Por ejemplo, usando de nuevo el ajuste de Bonferroni: pairwise.wilcox.test(notas.bloques$notas, notas.bloques$acts, paired=TRUE, p.adjust.method=&quot;bonferroni&quot;) ## ## Pairwise comparisons using Wilcoxon signed rank test ## ## data: notas.bloques$notas and notas.bloques$acts ## ## Casa Control 1 Control 2 Talleres ## Control 1 1.000 - - - ## Control 2 0.082 1.000 - - ## Talleres 0.013 0.029 0.003 - ## Tests 1.000 0.569 0.182 0.013 ## ## P value adjustment method: bonferroni Encontramos evidencia de que las notas medias de los Talleres fueron diferentes de las del resto de actividades, y no podemos rechazar que las notas medias de las otras actividades fueran todas iguales. Finalmente, en el caso de experimentos con dos o más factores, no hay una solución simple cuando no se satisfacen las condiciones necesarias para poder hacer un ANOVA. Una posible salida es usar un método adecuado de bootstrap. Sin entrar en detalles, una buena opción en un experimento de dos factores es usar la función pbad2way del paquete WRS2. Su sintaxis básica es pbad2way(fórmula, est=&quot;median&quot;, nboot=...) En el parámetro nboot se ha de entrar el número de muestras que se han de tomar; tened en cuenta que cuántas más toméis, mas tardará la ejecución. El parámetro est indica qué parámetro se quiere comparar, la media no está entre los valores disponibles así que aquí usaremos la mediana. Consultad la Ayuda de la función (y las referencias que se citan) para saber qué otros estadísticos podéis usar. Por ejemplo, para llevar a cabo por medio de un bootstrap el contraste del Ejemplo 9.4 sobre el efecto de la vitamina C en el crecimiento de los dientes (y recordando que la tabla ToothGrowth2 contiene los datos con los tratamientos codificados como factores) podríamos entrar: library(WRS2) set.seed(42) #Fijamos la semilla de aleatoriedad, para que sea reproducible pbad2way(len~supp*dose, data=ToothGrowth2, est=&quot;median&quot;, nboot=500) ## Call: ## pbad2way(formula = len ~ supp * dose, data = ToothGrowth2, est = &quot;median&quot;, ## nboot = 500) ## ## p.value ## supp 0.002 ## dose 0.000 ## supp:dose 0.028 Grosso modo, los p-valores obtenidos tienen el significado usual en un bootstrap: la fracción de muestras en las que obtenemos un valor de un cierto estadístico tan o más extremo que el de la muestra global. Un p-valor pequeño nos da evidencia de que podemos rechazar la hipótesis nula. Los p-valores de las filas con los nombres de los factores son los de los contrastes de los mismos y el p-valor de la última fila es el del contraste de interacción. Por lo tanto, con este contraste concluimos que hay evidencia de que tanto la dosis de vitamina C como el método de suministrarla influyen en el crecimiento de los odontoblastos (en concreto, en su mediana) y que hay interacción entre ambos factores. Como en el ANOVA, el contraste de la combinación de factores se ha de efectuar aparte: en este caso (ya que no podemos llevar a cabo un ANOVA, o no nos estaríamos complicando tanto la vida) una buena opción es usar un test de Kruskal-Wallis tomando como factor la combinación de factores (entrada en la fórmula por medio de interaction). kruskal.test(len~interaction(supp,dose),data=ToothGrowth2) ## ## Kruskal-Wallis rank sum test ## ## data: len by interaction(supp, dose) ## Kruskal-Wallis chi-squared = 45.81, df = 5, p-value = 9.95e-09 Concluimos que también hay evidencia de que la combinación de la dosis de vitamina C y el método de suministrarla influye en el crecimiento de los odontoblastos. Para terminar, una advertencia sobre pbad2way: a veces da un error numérico, como en la siguiente ejecución en el otro ejemplo de experimento de 2 vías de la Sección 9.4: pbad2way(time~poison*treat, data=rats, est=&quot;median&quot;, nboot=500) ## Error in solve.default(cov, ...): system is computationally singular: reciprocal condition number = 1.34231e-20 En este caso, el inventor del método recomienda emplear el parámetro pro.dis=TRUE, que modifica el método de cálculo de los estadísticos que se usan en el bootstrap y evita el error. set.seed(42) pbad2way(time~poison*treat, data=rats, est=&quot;median&quot;,nboot=500,pro.dis=TRUE) ## Call: ## pbad2way(formula = time ~ poison * treat, data = rats, est = &quot;median&quot;, ## nboot = 500, pro.dis = TRUE) ## ## p.value ## poison 0.000 ## treat 0.000 ## poison:treat 0.134 9.8 Guía rápida Fórmulas: X~F1 ajusta la variable X al factor F1. X~F1+F2 ajusta la variable X a los factores F1 y F2 sin tener en cuenta la interacción entre los factores. X~F1*F2 ajusta la variable X a los factores F1 y F2 teniendo en cuenta además la interacción entre los factores. X~F1:F2 ajusta la variable X a un factor que tiene como niveles los pares de niveles de F1 y F2. options(show.signif.stars=...) sirve para desactivar (usando FALSE) o activar (con TRUE) los códigos de significación por medio de estrellitas. aov(fórmula) efectúa el ANOVA indicado por la fórmula. El parámetro adicional data sirve para indicar el data frame del que se extraen los vectores y factores de la fórmula. summary(aov(...)) muestra la tabla del ANOVA efectuado con la función aov. Añadiendo el sufijo [[1]]$&quot;Pr(&gt;F)&quot; nos da directamente la columna de p-valores de la tabla. anova(lm(fórmula, data=...)) también produce la tabla del ANOVA indicado por la fórmula. Con el sufijo $&quot;Pr(&gt;F)&quot;, nos da directamente la columna de p-valores de la tabla. interaction.plot(factor1,factor2,vector) produce el gráfico de interacción del vector respecto de los dos factores. subset(dataframe,condición) define un data frame con las filas del dataframe que cumplen la condición. shapiro.test(x) realiza el test de normalidad de Shapiro-Wilk sobre el vector numérico x. aggregate sirve para aplicar una función a una o varias variables de un data frame agrupando sus entradas por los niveles de uno o varios factores. bartlett.test, con la misma sintaxis que aov, efectúa el test de Bartlett de homocedasticidad. pairwise.t.test(x,factor) efectúa un test t por parejas de la variable x separada por los niveles del factor. Dos parámetros importantes: paired: se ha de igualar a TRUE en los ANOVA de bloques. p.adjust.method: sirve para indicar el método de ajuste de los p-valores. duncan.test(aov,factor)$sufijo del paquete agricolae, efectúa el test de Duncan del ANOVA aov. Sus dos parámetros más importantes para nosotros son alpha, que sirve para indicar el nivel de significación y group, que sirve para indicar cómo queremos que muestre el resultado. El sufijo ha de ser ser groups si group=TRUE y comparison si group=FALSE. TukeyHSD(aov) efectúa el test HSD de Tukey del ANOVA aov. kruskal.test efectúa el test de Kruskal-Wallis. Su sintaxis es la misma que la de aov. pairwise.wilcox.test efectúa un test de Wilcoxon (con paired=TRUE) o de Mann-Whitney-Wilcoxon (con paired=FALSE, el valor por defecto de este parámetro) por parejas. Su sintaxis es la misma que la de pairwise.t.test. friedman.test efectúa el test de Friedman. Su sintaxis es la misma que la de aov. pbad2way(fórmula, data=..., est=&quot;median&quot;) del paquete WRS2 lleva a cabo un contraste bootstrap de medianas de los datos de un experimento factorial de 2 vías. El parámetro nboot sirve para indicar el número de muestras a usar. 9.9 Ejercicios Test (1) En un experimento queremos comparar el valor medio de una cierta variable en cuatro poblaciones, A, B, C y D. Hemos tomado una muestra de 5 individuos de cada población y hemos medido en ellos la variable en cuestión. Los resultados para cada población han sido los siguientes: A=(23.1,17.2,20.1,19.5,18.8), B=(22.6,15.5,19.3,18.5,16.2), C=(21.1,26.4,24.7,21.4,20.9) y D=(20.4,24,23.4,24.9,21.5). Realizad un ANOVA de 1 vía sobre estos datos. Tenéis que dar el p-valor del contraste de la igualdad de las medias redondeado a 3 cifras decimales (sin ceros innecesarios a la derecha) y decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%, la igualdad de todas las medias. Dad el p-valor y la conclusión en este orden, separados por un único espacio en blanco. (2) En un experimento queremos comparar la influencia de cuatro tratamientos, A, B, C y D, sobre una determinada variable fisiológica. Para ello hemos aplicado cada tratamiento a los mismos 5 individuos, pero en órdenes y momentos escogidos al azar, y hemos medido en ellos la variable en cuestión tras cada tratamiento. Los resultados para cada tratamiento (ordenados en cada uno en el mismo orden, correspondiente a los individuos) han sido los siguientes: A=(23.1,17.2,20.1,19.5,18.8), B=(22.6,15.5,19.3,18.5,16.2), C=(21.1,26.4,24.7,21.4,20.9) y D=(20.4,24,23.4,24.9,21.5). Realizad un ANOVA de bloques sobre estos datos. Tenéis que dar el p-valor del contraste de la igualdad de las medias de los tratamientos redondeado a 3 cifras decimales (sin ceros innecesarios a la derecha) y decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%, la igualdad de todas las medias de los tratamientos. Dad el p-valor y la conclusión en este orden, separados por un único espacio en blanco. (3) En un experimento queremos comparar el valor medio de una cierta variable en cuatro poblaciones, A, B, C y D; los individuos de cada población además pueden clasificarse en dos tipos, X e Y, según una determinada característica que creemos que será relevante en nuestro estudio. Hemos tomado una muestra de 5 individuos de cada combinación población-característica y hemos medido en ellos la variable en cuestión. Los resultados para cada combinación población-característica han sido los siguientes: A-X=(29.2,30.9,31.3,30.0,30.0), B-X=(29.1,31.3,31.4,34.3,32.3), C-X=(31.3,40.7,32.6,34.8, 38.3), D-X=(32.2,36.0,31.6,31.8,32.6), A-Y=(26.3,25.9,25.6,25.3,24.9), B-Y=(30.7,29.8, 27.9,28.8,28.1), C-Y=(26.2,26.7,27.2,29.3,28.4), y D-Y=(20.5,26.2,27.9,23.3,27.8). Realizad un ANOVA de 2 vías sobre estos datos. Dad el p-valor del contraste de interacción entre el tratamiento y la característica redondeado a 3 cifras decimales (sin ceros innecesarios a la derecha) y decid si podéis concluir (con un SI) o no (con un NO), con un nivel de significación del 5%, que hay interacción entre el tratamiento y la característica. Dad el p-valor y la conclusión en este orden, separados por un único espacio en blanco. (4) En un experimento queremos comparar el valor medio de una cierta variable en cuatro poblaciones, A, B, C y D. Hemos tomado una muestra de 5 individuos de cada población y hemos medido en ellos la variable en cuestión. Los resultados para cada población han sido los siguientes: A=(23.1,17.2,20.1,19.5,18.8), B=(22.6,15.5,19.3,18.5,16.2), C=(21.1,26.4,24.7,21.4,20.9) y D=(20.4,24,23.4,24.9,21.5). Realizad un test de Bartlett de homocedasticidad sobre estos datos, para saber si las varianzas poblacionales de las cuatro poblaciones son todas iguales o no. Tenéis que dar el p-valor del contraste de la igualdad de las varianzas redondeado a 3 cifras decimales (sin ceros innecesarios a la derecha) y decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%, la igualdad de todas las varianzas. Dad el p-valor y la conclusión en este orden, separados por un único espacio en blanco. (5) En un experimento hemos comparado el valor medio de una cierta variable en cuatro poblaciones, A, B, C y D. Hemos tomado una muestra de 5 individuos de cada población y hemos medido en ellos la variable en cuestión. Los resultados para cada población han sido los siguientes: A=(23.1,17.2,20.1,19.5,18.8), B=(22.6,15.5,19.3,18.5,16.2), C=(21.1,26.4,24.7,21.4,20.9) y D=(20.4,24,23.4,24.9,21.5). Realizad un test t por parejas de Bonferroni sobre estos datos con un nivel de significación del 5%, para determinar qué pares de poblaciones podemos concluir que tienen medias diferentes con este nivel de significación. Tenéis que decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%, la igualdad de las medias de las poblaciones B y C. Respuestas al test (1) 0.013 SI Fijaos en que en el enunciado nos dan la tabla de datos siguiente por columnas: A B C D 23.1 22.6 21.1 20.4 17.2 15.5 26.4 24.0 20.1 19.3 24.7 23.4 19.5 18.5 21.4 24.9 18.8 16.2 21.9 21.5 Por lo tanto para entrarla de manera correcta como un data frame copiando los datos del enunciado hay que usar: x=c(23.1,17.2,20.1,19.5,18.8, 22.6,15.5,19.3,18.5,16.2, 21.1,26.4,24.7,21.4,21.9, 20.4,24.0,23.4,24.9,21.5) pobl=rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),each=5) X=data.frame(x,pobl) A partir de aquí, la respuesta la obtenemos con summary(aov(x~pobl,data=X))[[1]]$&quot;Pr(&gt;F)&quot; ## [1] 0.0125788 NA (2) 0.026 NO Nosotros lo hemos resuelto con x=c(23.1,17.2,20.1,19.5,18.8, 22.6,15.5,19.3,18.5,16.2, 21.1,26.4,24.7,21.4,21.9, 20.4,24.0,23.4,24.9,21.5) pobl=rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), each=5) bloques=as.factor(rep(1:5,4)) X.b=data.frame(x,pobl,bloques) summary(aov(x~pobl+bloques,data=X.b))[[1]]$&quot;Pr(&gt;F)&quot;[1] ## [1] 0.0256179 (3) 0.0234 SI Nosotros lo hemos resuelto con AX=c(29.2,30.9,31.3,30.0,30.0) BX=c(29.1,31.3,31.4,34.3,32.3) CX=c(31.3,40.7,32.6,34.8,38.3) DX=c(32.2,36.0,31.6,31.8,32.6) AY=c(26.3,25.9,25.6,25.3,24.9) BY=c(30.7,29.8,27.9,28.8,28.1) CY=c(26.2,26.7,27.2,29.3,28.4) DY=c(20.5,26.2,27.9,23.3,27.8) y=c(AX,BX,CX,DX,AY,BY,CY,DY) fact.A=rep(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),each=5),2) fact.X=rep(c(&quot;X&quot;,&quot;Y&quot;),each=20) DF=data.frame(y,fact.A,fact.X) summary(aov(y~fact.A*fact.X)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## fact.A 3 75 25 5.49 0.0037 ## fact.X 1 330 330 72.70 9.6e-10 ## fact.A:fact.X 3 49 16 3.62 0.0234 ## Residuals 32 145 5 (4) 0.881 NO Nosotros lo hemos resuelto con bartlett.test(x~pobl,data=X) ## ## Bartlett test of homogeneity of variances ## ## data: x by pobl ## Bartlett&#39;s K-squared = 0.6667, df = 3, p-value = 0.881 (5) SI Nosotros lo hemos resuelto con pairwise.t.test(X$x,X$pobl,p.adjust.method=&quot;holm&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: X$x and X$pobl ## ## A B C ## B 0.76 - - ## C 0.14 0.03 - ## D 0.15 0.04 0.86 ## ## P value adjustment method: holm Véase Statistics for Experimenters (2a edición), de G. P. Box, W. G. Hunter y J. S. Hunte (Wiley, 2005), p. 133.↩ Véase Experimental Design and Analysis, de M. Lentner y T. Bishop (Valley Book Co. 1986), p. 428.↩ Véase The Statistics of Bioassay de C. I. Bliss (Academic Press, 1952), p. 499-501.↩ Véase Practical Regression and Anova using R, de J. Faraway, p. 186. Este texto se puede descargar de la página web https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf.↩ Véase “An analysis of transformations”, de G. Box y D. Cox, J. Roy. Stat. Soc. Series B 26 (1964), pp. 211-252. Véase también Practical Regression and Anova using R, de J. Faraway, p. 182.↩ "]
]
