[
["index.html", "AprendeR: Parte II Presentación", " AprendeR: Parte II The UIB-AprendeR team 2021-03-05 Presentación Esto es una edición preliminar en línea de la 2a parte del libro “AprendeR”, producido por un grupo de profesores del Departamento de Ciencias Matemáticas e Informática de la UIB. El libro está escrito en R Markdown, usando RStudio como editor de texto y el paquete bookdown para convertir los ficheros markdown en un libro. Este trabajo se publica bajo licencia Atribución-No Comercial-SinDerivadas 4.0 "],
["chap-distr.html", "Lección 1 Distribuciones de probabilidad 1.1 Ejercicios", " Lección 1 Distribuciones de probabilidad R conoce los tipos de distribución de probabilidad más importantes, incluyendo las que mostramos en la tabla siguiente: \\[ \\begin{array}{lll} \\hline \\textbf{Distribución} &amp; {\\textbf{Nombre en R}} &amp; {\\textbf{Parámetros}}\\\\ \\hline \\mbox{Binomial} &amp;{\\texttt{binom}} &amp; \\mbox{tamaño de la muestra $n$, probabilidad $p$}\\\\ \\mbox{Geométrica} &amp; {\\texttt{geom}} &amp; \\mbox{$p$}\\\\ \\mbox{Hipergeométrica} &amp; {\\texttt{hyper}} &amp; \\mbox{tamaño de la población $N$, número poblacional}\\\\[-0.75ex] &amp; &amp; \\mbox{de éxitos $M$, tamaño de la muestra $n$}\\\\ \\mbox{Poisson} &amp; {\\texttt{pois}} &amp; \\mbox{esperanza $\\lambda$}\\\\ \\mbox{Uniforme} &amp; {\\texttt{unif}} &amp; \\mbox{mínimo, máximo}\\\\ \\mbox{Exponencial} &amp; {\\texttt{exp}} &amp; \\lambda\\\\ \\mbox{Normal} &amp; {\\texttt{norm}} &amp; \\mbox{media $\\mu$, desviación típica $\\sigma$}\\\\ \\mbox{Khi cuadrado} &amp; {\\texttt{chisq}} &amp; \\mbox{número de grados de libertad $df$}\\\\ \\mbox{t de Student} &amp; {\\texttt{t}} &amp; \\mbox{número de grados de libertad $df$}\\\\ \\mbox{F de Fisher} &amp; {\\texttt{f}} &amp; \\mbox{los dos números de grados de libertad} \\\\ \\hline \\end{array} \\] Para cada una de estas distribuciones, R sabe calcular cuatro funciones, que se obtienen añadiendo un prefijo al nombre de la distribución: La función de densidad, con el prefijo d. La función de distribución de probabilidad, con el prefijo p; esta función dispone además del parámetro lower.tail que igualado a FALSE calcula la función de distribución de cola superior: la probabilidad de que una variable aleatoria con esta distribución de probabilidad tome un valor estrictamente mayor que uno dado. Los cuantiles, con el prefijo q. Vectores de números aleatorios con esta distribución, con el prefijo r. La función correspondiente se aplica entonces al valor sobre el que queremos calcular la función y a los parámetros de la distribución (en este orden, y los parámetros en el orden en que los damos en la tabla anterior, cuando hay más de uno). Por ejemplo, sea \\(X\\) una variable aleatoria binomial \\(B(20,0.3)\\), es decir, de tamaño \\(n=20\\) y probabilidad \\(p=0.3\\), y sean \\(f_X\\) su función de densidad y \\(F_X\\) su función de distribución. Calculemos algunos valores de funciones asociadas a esta variable aleatoria. \\(f_X(5)=P(X=5)\\): dbinom(5,20,0.3) ## [1] 0.1788631 Comprobémoslo, recordando que si \\(X\\sim B(n,k)\\), entonces \\(P(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}\\): choose(20,5)*0.3^5*0.7^15 ## [1] 0.1788631 \\(f_X(8)=P(X=8)\\): dbinom(8,20,0.3) ## [1] 0.1143967 \\(F_X(5)=P(X\\leq 5)\\): pbinom(5,20,0.3) ## [1] 0.4163708 Comprobémoslo, usando que \\(P(X\\leq 5)=\\sum_{k=0}^5 P(X=k)\\): sum(dbinom(0:5,20,0.3)) ## [1] 0.4163708 \\(F_X(8)=P(X\\leq 8)\\): pbinom(8,20,0.3) ## [1] 0.8866685 \\(P(X&gt;8)\\) pbinom(8,20,0.3,lower.tail=FALSE) ## [1] 0.1133315 En efecto: 1-pbinom(8,20,0.3) ## [1] 0.1133315 El cuantil de orden \\(0.5\\) de \\(X\\), o sea, su mediana: el valor \\(x\\) más pequeño tal que \\(P(X\\leq x)\\geq 0.5\\) qbinom(0.5,20,0.3) ## [1] 6 Comprobemos que \\(P(X\\leq 6)\\geq 0.5\\) y en cambio \\(P(X\\leq 5)&lt; 0.5\\): pbinom(6,20,0.3) ## [1] 0.6080098 pbinom(5,20,0.3) ## [1] 0.4163708 El cuantil de orden \\(0.25\\) de \\(X\\), es decir, su primer cuartil: qbinom(0.25,20,0.3) ## [1] 5 Un vector aleatorio de 10 valores generado con la variable aleatoria \\(X\\): rbinom(10,20,0.3) ## [1] 6 8 7 5 10 7 3 4 3 9 Dos vectores aleatorios más, de 10 valores cada uno, generados con la variable aleatoria \\(X\\): rbinom(10,20,0.3) ## [1] 8 2 6 7 7 4 8 3 4 4 rbinom(10,20,0.3) ## [1] 7 6 9 8 5 4 5 4 9 4 Del mismo modo, si estamos trabajando con una variable aleatoria \\(Y\\) de Poisson con parámetro \\(\\lambda=5\\): \\(P(Y=8)\\): dpois(8,5) ## [1] 0.06527804 \\(P(Y\\leq 8)\\): ppois(8,5) ## [1] 0.9319064 El cuantil de orden 0.6 de \\(Y\\): qpois(0.6,5) ## [1] 5 Un vector aleatorio de 20 valores generado con la variable aleatoria \\(Y\\): rpois(20,5) ## [1] 2 3 10 6 5 10 6 6 5 8 4 3 3 5 2 4 4 2 1 4 Si no entramos ningún parámetro en las funciones asociadas a la distribución normal, R entiende que se trata de la normal estándar (con media \\(\\mu=0\\) y desviación típica \\(\\sigma=1\\)): por ejemplo, las dos instrucciones siguientes nos dan el valor \\(f_Z(0.3)\\) de la función densidad de una normal estándar \\(Z\\) aplicada a 0.3 (que no es igual a \\(P(Z=0.3)\\)): dnorm(0.3) ## [1] 0.3813878 dnorm(0.3,0,1) ## [1] 0.3813878 Las funciones densidad y distribución de una variable aleatoria se pueden dibujar con la función curve. Así, la función siguiente dibuja la gráfica de la densidad de una variable normal de la Figura 1.1: curve(dnorm(x,0,1.5),-5,5,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;&quot;) Figura 1.1: Función densidad de una variable N(0,1.5). De manera similar, la función siguiente dibuja la gráfica de la función de distribución de una variable normal de la Figura 1.2: curve(pnorm(x,0,1.5),-5,5,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;&quot;) Figura 1.2: Función distribución de una variable N(0,1.5). 1.1 Ejercicios Test (1) Sea \\(f\\) la función de densidad de una variable aleatoria normal con \\(\\mu=0.2\\) y \\(\\sigma=1.2\\). Dad el valor de \\(f(0.5)\\) redondeado a 4 cifras decimales. (2) Sea \\(X\\) una variable aleatoria normal con \\(\\mu=0.2\\) y \\(\\sigma=1.2\\). Dad el valor de \\(P(3\\leq X\\leq 7)\\) redondeado a 4 cifras decimales. (3) Sea \\(X\\) una variable aleatoria \\(B(10,0.2)\\). Dad el valor de \\(P(3\\leq X\\leq 7)\\) redondeado a 4 cifras decimales. (4) Dad una instrucción que calcule la mediana de una lista de 20 números aleatorios generados con distribución \\(B(10,0.2)\\). No deis el resultado, solo la instrucción. Problemas (1) Según la página web de Tall.Life, la altura de los hombres españoles adultos sigue una distribución aproximadamente normal de media 173.1 cm y desviación típica 7.42 cm, mientras que la de las mujeres españolas adultas sigue una distribución también aproximadamente normal, pero de media 160.3 cm y desviación típica 7.11 cm. Por otro lado, según el último FactBook sobre España de la CIA, aproximadamente un 51% de los españoles adultos son mujeres. Vamos a tomar todos estos datos como exactos y verdaderos. (a) ¿Qué es más probable, que un hombre adulto español sea más alto que Víctor Claver (2.05 m) o que una mujer adulta española sea más alta que Alba Torrens (1.90 m)? (b) Si tomamos un español adulto (hombre o mujer) al azar, ¿qué es más probable, que sea más alto que Marc Gasol (2.11 m) o que sea más bajo que Peter Dinklage (1.35m)? (2) He lanzado un dado de 6 caras 100 veces, y he obtenido 24 veces un 6. Suponed que el dado es honrado. Considerad la variable aleatoria \\(X\\) dada por el número de veces que obtenemos un 6 al lanzar el dado 100 veces al aire. (a) ¿Qué distribución de probabilidad tiene \\(X\\)? Tenéis que dar el tipo de distribución y sus parámetros. (b) ¿Qué vale \\(P(X=24)\\)? Es decir, ¿cuál es la probabilidad de sacar exactamente 24 seises en 100 lanzamientos del dado? (c) ¿Cuál es la probabilidad de sacar 24 seises o más en 100 lanzamientos del dado? (d) Simulad con R una secuencia aleatoria de 1000 realizaciones de \\(X\\): es decir, generad un vector de 1000 números aleatorios con la distribución de probabilidades de \\(X\\). Vamos a suponer que este vector nos da los números de seises obtenidos en 1000 secuencias de 100 lanzamientos del dado cada una. ¿En qué porcentaje de estas secuencias habéis obtenido exactamente 24 seises? ¿En qué porcentaje habéis obtenido al menos 24 seises? ¿Son consistentes estos resultados con los valores obtenidos en (b) y (c)? Respuestas al test (1) 0.3222 Nosotros lo hemos calculado con round(dnorm(0.5,0.2,1.2),4) ## [1] 0.3222 (2) 0.0098 Nosotros lo hemos calculado con round(pnorm(7,0.2,1.2)-pnorm(3,0.2,1.2),4) ## [1] 0.0098 (3) 0.3221 Nosotros lo hemos calculado con round(pbinom(7,10,0.2)-pbinom(2,10,0.2),4) ## [1] 0.3221 También se obtiene el resultado correcto con round(sum(dbinom(3:7,10,0.2)),4) ## [1] 0.3221 En cambio, con round(pbinom(7,10,0.2)-pbinom(3,10,0.2),4) ## [1] 0.1208 no se obtiene el resultado correcto. Pensad por qué aquí hay que restar pbinom(2,10,0.2) y en la pregunta anterior restábamos pnorm(3,0.2,1.2). (4) median(rbinom(20,10,0.2)) También sería correcto median(rbinom(20,size=10,prob=0.2)), pero no es necesario dar los nombres de los parámetros si entráis sus valores en el orden correcto, y por eso no los hemos explicado. Soluciones sucintas de los problemas (1) (a) La probabilidad que un hombre adulto español sea más alto que Víctor Claver es pnorm(205,173.1,7.42,lower.tail=FALSE) ## [1] 0.00000857112 y la probabilidad de que una mujer adulta española sea más alta que Alba Torrens es pnorm(190,160.3,7.11,lower.tail=FALSE) ## [1] 0.00001475499 (b) La probabilidad de que un español adulto sea más alto (o alta) que Marc Gasol es 0.49*pnorm(211,173.1,7.42,lower.tail=FALSE)+0.51*pnorm(211,160.3,7.11,lower.tail=FALSE) ## [1] 0.00000007984638 y la probabilidad de que un español adulto sea más bajo (o baja) que Peter Dinklage es 0.49*pnorm(135,173.1,7.42)+0.51*pnorm(135,160.3,7.11) ## [1] 0.00009522642 (2) (a) Binomial \\(B(100,1/6)\\) (b) dbinom(24,100,1/6)=0.016161 (c) 1-pbinom(23,100,1/6)=0.0378644 (d) El vector siguiente contiene una simulación como la pedida Simulación=rbinom(1000,100,1/6) En este vector, el porcentaje de entradas iguales a 24 es round(100*length(Simulación[Simulación==24])/1000,1) ## [1] 1.9 y el porcentaje de entradas mayores o iguales a 24 es round(100*length(Simulación[Simulación&gt;=24])/1000,1) ## [1] 3.8 Naturalmente, en cada simulación puede dar valores diferentes. Por ejemplo, si repetimos de nuevo la simulación Simulación2=rbinom(1000,100,1/6) obtenemos: round(100*length(Simulación2[Simulación2==24])/1000,1) ## [1] 2 round(100*length(Simulación2[Simulación2&gt;=24])/1000,1) ## [1] 3.3 "],
["chap-muestreo.html", "Lección 2 Conceptos básicos de muestreo 2.1 Tipos de muestreo 2.2 Muestreo aleatorio con R 2.3 Guía rápida 2.4 Ejercicios", " Lección 2 Conceptos básicos de muestreo En todo estudio estadístico hemos de distinguir entre población, que es un conjunto de sujetos con una o varias características que podemos medir y deseamos estudiar, y muestra, un subconjunto de una población. Por ejemplo, si quisiéramos estudiar alguna característica de los estudiantes de grado de la UIB, entenderíamos que estos forman la población de interés, y si entonces escogiéramos al azar 10 estudiantes de cada grado, obtendríamos una muestra de esta población. Pero también podríamos considerar los estudiantes de grado de la UIB como una muestra de la población de los estudiantes universitarios españoles: depende del estudio que queramos realizar. Recordad que, cuando disponemos de un conjunto de datos obtenidos midiendo una o varias características sobre los sujetos de una muestra, podemos llevar a cabo dos tipos de análisis estadístico: Exploratorio o descriptivo: su objetivo es resumir, representar y explicar los datos de la muestra. Para llevarlo a cabo, se usan técnicas de estadística descriptiva como las que hemos descrito en lecciones anteriores. Inferencial o confirmatorio: su objetivo es deducir (inferir), a partir de los datos de la muestra, información significativa sobre el total de la población. A menudo esta inferencia pasa por contrastar una hipótesis sobre alguna propiedad de la población. Las técnicas que se usan en los análisis inferenciales forman la estadística inferencial. Por ejemplo, supongamos que hemos tomado una muestra de estudiantes de la UIB y sabemos sus calificaciones en un semestre concreto y sus números de hermanos. En un estudio exploratorio simplemente describiríamos estos datos mediante estadísticos y gráficos, mientras que usaríamos técnicas de estadística inferencial para deducir información sobre la población de todos los estudiantes de la UIB a partir de esta muestra: ¿Cuál estimamos que ha sido la nota media de los estudiantes de la UIB en el semestre en cuestión? La distribución de los números de hermanos en estudiantes de la UIB, ¿es similar a la del conjunto de la población española? ¿Es verdad que los estudiantes de la UIB con más hermanos tienen tendencia a tener mejores notas? Un estudio inferencial suele desglosarse en los pasos siguientes: Establecer la característica que se desea estimar o la hipótesis que se desea contrastar. Determinar la información (los datos) que se necesita para hacerlo. Diseñar un experimento que permita recoger estos datos; este paso incluye: Decidir qué tipo de muestra se va a tomar y su tamaño. Elegir las técnicas adecuadas para realizar las inferencias deseadas a partir de la muestra que se tomará. Tomar una muestra y medir los datos deseados sobre los individuos que la forman. Aplicar las técnicas de inferencia elegidas con el software adecuado. Obtener conclusiones. Si las conclusiones son fiables y suficientes, redactar un informe; en caso contrario, volver a empezar. En la próxima sección nos centraremos en las técnicas de muestreo: los métodos generales para seleccionar muestras representativas de una población que tenemos a nuestra disposición en el tercer paso de la lista anterior. 2.1 Tipos de muestreo Existen muchos tipos de muestreo, cada uno de los cuales proporciona una muestra representativa de la población en algún sentido. A continuación describimos de forma breve algunas de estas técnicas. Muestreo aleatorio con y sin reposición Un muestreo aleatorio consiste en seleccionar una muestra de la población de manera que todas las muestras del mismo tamaño sean equiprobables; es decir, que si fijamos el número de individuos de la muestra, cualquier conjunto de ese número de individuos tenga la misma probabilidad de ser seleccionado. Hay dos tipos básicos de muestreo aleatorio que vale la pena distinguir. Para ilustrarlos, supongamos que disponemos de una urna con 100 bolas numeradas del 1 al 100, de la que queremos extraer una muestra de 15 bolas. La Figura 2.1 representa dicha urna. Figura 2.1: Una urna de 100 bolas Una manera de hacerlo sería repetir 15 veces el proceso de sacar una bola de la urna, anotar su número y devolverla a la urna. El tipo de muestra obtenida de esta manera recibe el nombre de muestra aleatoria con reposición, o simple (una m.a.s., para abreviar). Observad que con este procedimiento una misma bola puede aparecer varias veces en una muestra, y que todos los subconjuntos de 15 bolas “con posibles repeticiones” tienen la misma probabilidad de obtenerse. Un posible resultado serían las bolas azules de la Figura 2.2; la bola azul más oscuro ha sido escogida dos veces en la muestra. Figura 2.2: Una muestra aleatoria simple Otra manera de extraer nuestra muestra sería repetir 15 veces el proceso de sacar una bola de la urna pero ahora sin devolverla. Esto es equivalente a extraer de golpe 15 bolas de la urna. Estas muestras no tienen bolas repetidas, y cualquier selección de 15 bolas diferentes tiene la misma probabilidad de ser la obtenida. En este caso se habla de una muestra aleatoria sin reposición. Un posible resultado serían las bolas azules de la Figura 2.3. Figura 2.3: Una muestra aleatoria sin reposición Cuando el tamaño de la población es muy grande en relación a la muestra, la probabilidad de que haya repeticiones en una muestra aleatoria simple es muy pequeña. Esto nos permite entender en este caso que los muestreos aleatorios con y sin reposición son equivalentes en el sentido siguiente: puesto que si la población es muy, muy grande, un muestreo con reposición daría muy probablemente una muestra con todos sus elementos diferentes, si tomamos directamente la muestra sin reposición podemos aceptar que permitíamos repeticiones, pero que no se han dado, y que por tanto es simple. A modo de ejemplo, vamos a calcular la probabilidad de al menos una repetición en muestras aleatorias simples de diferentes tamaños de una población de 12,000 individuos (aproximadamente, el número de estudiantes de la UIB) y representar estas probabilidades en un gráfico. Recordad que la probabilidad de que los sujetos de una muestra aleatoria simple de tamaño n tomada de una población de N individuos no sean todos diferentes es \\[ 1-\\frac{N(N-1)(N-2)\\cdots (N-n+1)}{N^n}. \\] Esta probabilidad es la que calcula la función P.Rep(n,N) del bloque de código siguiente, y su gráfica es la de la Figura 2.4. La curva negra representa las probabilidades deseadas. Hemos añadido al gráfico una línea horizontal que marca la probabilidad 0.01 y que muestra que la probabilidad de alguna repetición en una m.a.s. de 16 o menos estudiantes de la UIB es inferior al 1%: en más de 99 de cada 100 veces que tomemos una m.a.s. de a lo sumo 16 estudiantes, nos saldrán todos diferentes P.Rep=function(n,N){1-prod((N:(N-n+1))/N)} prob=sapply(1:200,P.Rep,N=12000) plot(1:200,prob,type=&quot;l&quot;,lwd=2,xlab=&quot;n&quot;,ylab=&quot;probabilidad&quot;, main=&quot;&quot;,xaxp=c(0,200,20),yaxp=c(0,1,10)) abline(h=0.01,col=&quot;red&quot;) text(160,0.04,labels=&quot;probabilidad 0.01&quot;,col=&quot;red&quot;,cex=0.7) Figura 2.4: Probabilidad de repetición en una m.a.s. de n estudiantes de la UIB Así, por ejemplo, una muestra aleatoria de 10 estudiantes diferentes de la UIB podría haberse obtenido perfectamente tomando los estudiantes con reposición, porque la probabilidad de alguna repetición en una m.a.s. como esta es muy pequeña: 0.004. En cambio, es difícil de creer que una muestra aleatoria de 200 estudiantes diferentes de la UIB sea simple, porque la probabilidad de alguna repetición en una m.a.s. como esta es grande: 0.811. Por cierto, aunque en el bloque de código anterior hemos definido “a mano” la función P.Rep para recordar cómo se calculan estas probabilidades, R dispone de la distribución de probabilidad birthday relacionada con la probabilidad de repeticiones en muestras aleatorias simples. En concreto, pbirthday(n,N) nos da la probabilidad de que se produzca al menos una repetición en una muestra aleatoria simple de tamaño n tomada de una población de N individuos qbirthday(p,N) nos da el tamaño mínimo que ha de tener una muestra aleatoria simple de una población de N individuos para que la probabilidad de una repetición sea como mínimo \\(p\\). Veámoslo: La probabilidad de alguna repetición en una m.a.s. de 10 estudiantes de la UIB es: pbirthday(10,12000) ## [1] 0.003743964 P.Rep(10,12000) ## [1] 0.003743964 La probabilidad de alguna repetición en una m.a.s. de 200 estudiantes de la UIB es: pbirthday(200,12000) ## [1] 0.8113007 P.Rep(200,12000) ## [1] 0.8113007 El tamaño mínimo de una m.a.s. para que la probabilidad de alguna repetición sea al menos de 0.01 es: qbirthday(0.01,12000) ## [1] 17 En efecto: pbirthday(17,12000) ## [1] 0.01127449 pbirthday(16,12000) ## [1] 0.009954432 La mayoría de técnicas de estadística inferencial que se pueden usar para muestras aleatorias simples se pueden considerar igualmente válidas para muestras aleatorias sin reposición si el tamaño de la población es muy grande en relación al de la muestra (por dar una regla, digamos que, al menos, unas 1000 veces mayor). Si el tamaño de la población es relativamente pequeño por comparación a la muestra, algunas de estas técnicas se pueden salvar aplicando correcciones adecuadas para compensar la pequeñez de la población, y otras directamente pierden toda validez. En todo caso, conviene ser consciente de que si queremos tomar una muestra aleatoria con o sin reposición de una población, es necesario disponer de una lista completa de todos sus individuos para poder sortear a quién vamos a seleccionar. Esto no siempre es posible. ¿Alguien tiene la lista completa de, pongamos, todos los diabéticos de España? ¿Que incluya los que no saben que lo son? Por lo tanto, en la vida real no siempre podemos tomar muestras aleatorias en el sentido que hemos explicado. Muestreo sistemático Una manera muy sencilla de obtener una muestra de una población cuando disponemos de una lista ordenada de sus individuos es tomarlos a intervalos constantes: cada quinto individuo, cada décimo individuo. Podemos añadir una componente aleatoria escogiendo al azar el primer individuo que elegimos, y a partir del cual empezamos a contar. Así, por ejemplo, si de una clase de 100 estudiantes quisiéramos escoger una muestra de 10, podríamos elegir un estudiante al azar, y a partir de él, por orden alfabético, elegir el décimo estudiante, el vigésimo, el trigésimo, etc.; si al llegar al final de la lista de clase no hubiéramos completado la muestra, volveríamos al principio de la misma. A esta técnica se la llama muestreo sistemático, aleatorio si además el primer sujeto se escoge de manera aleatoria. Por ejemplo, la Figura 2.5 describe una muestra aleatoria sistemática de 15 bolas de nuestra urna de 100 bolas: hemos empezado a escoger por la bola roja oscura, que ha sido elegida al azar, y a partir de ella hemos tomado 1 de cada 7 bolas, volviendo al principio cuando hemos llegado al final de la lista de bolas Figura 2.5: Una muestra aleatoria sistemática Cuando no disponemos de una lista de toda la población pero sí que tenemos una manera de acceder de manera ordenada a sujetos de la misma (por ejemplo, enfermos que acuden a un hospital), podemos realizar un muestreo sistemático tomando los sujetos a intervalos constantes a medida que los encontramos y hasta completar el tamaño deseado de la muestra. Por ejemplo, para escoger una muestra de 10 estudiantes de la UIB, podríamos escoger cada décimo estudiante que entrase en un edificio del Campus por una puerta concreta hasta llegar a los 10. Cuando el orden de los individuos de la población en la lista es aleatorio, el muestreo sistemático aleatorio es equivalente al muestreo aleatorio sin reposición. Pero en general este no es el caso, y se pueden producir sesgos. Por poner un caso extremo, si una clase de 100 estudiantes estuviera formada por 50 parejas de hermanos y tomáramos una muestra sistemática de 50 estudiantes, eligiéndolos por orden alfabético de los apellidos uno sí, uno no, es seguro que no aparecería ninguna pareja de hermanos en la muestra (porque dos hermanos son siempre consecutivos en la lista, y en nuestra muestra no habría ningún par de sujetos consecutivos). En cambio, la probabilidad de que una muestra aleatoria sin reposición del mismo tamaño contuviera una pareja de hermanos es prácticamente 1; en concreto esta probabilidad sería \\[ 1-\\frac{100\\times 98\\times 96\\times\\cdots\\times 2}{100\\times 99\\times 98\\times\\cdots\\times 51}=1-\\frac{2^{50}\\cdot 50!^2}{100!}=1. \\] Muestreo aleatorio estratificado Este tipo de muestreo se utiliza cuando la población está clasificada en estratos que son de interés para la propiedad estudiada. En este caso, se toma una muestra aleatoria de cada estrato y se unen en una muestra global. A este proceso se le llama muestreo aleatorio estratificado. Las muestras de cada estrato se toman de manera independiente las unas de las otras y de tamaños prefijados. Por lo que refiere a estos tamaños, la estrategia usual es imponer que la composición por estratos de la muestra global mantenga las proporciones de la población original, de manera que el tamaño de la muestra de cada estrato represente el mismo porcentaje del total de la muestra que el estrato correspondiente en la población completa. Pero a veces se usa una estrategia completamente diferente, y se toman los tamaños de manera que estratos que representan una fracción muy pequeña de la población (tan pequeña que no esperaríamos que tuvieran representación en una muestra aleatoria transversal de la población, es decir, tomada del total de la población sin tener en cuenta su composición en estratos) tengan una representación en la muestra mucho mayor que la que les tocaría. Por ejemplo, los estratos podrían ser grupos de edad y podríamos tomar la muestra de cada grupo de edad de tamaño proporcional a la fracción que representa dicho grupo de edad en la población total. O podrían ser los sexos y procuraríamos que nuestra muestra estuviera formada por un 50% de hombres y un 50% de mujeres. O, en las Islas Baleares, los estratos podrían ser las islas, y entonces podríamos imponer que el número de representantes de cada isla en la muestra fuera proporcional a su población relativa dentro del conjunto total de la comunidad autónoma, o podríamos escoger la misma cantidad de individuos de cada isla, independientemente de su población. Por continuar con nuestra urna de 100 bolas, supongamos que contiene 40 bolas de un color y 60 de otro color según muestra la Figura 2.6. Figura 2.6: Nuestra urna ahora tiene 2 estratos Para tomar una muestra aleatoria estratificada de 15 bolas, considerando como estratos los dos colores e imponiendo que la muestra refleje la composición de la urna, tomaríamos una muestra aleatoria de 6 bolas del primer color y una muestra aleatoria de 9 bolas del segundo color. De esta manera, los porcentajes de colores en la muestra serían los mismos que en la urna. La Figura 2.7 describe una muestra obtenida de esta manera. Figura 2.7: Una muestra aleatoria estratificada En todo caso, el muestreo por estratos solo es necesario si esperamos que las características de la propiedad poblacional que queremos estudiar varíen según el estrato. Por ejemplo, si queremos tomar una muestra para estimar la altura media de los españoles adultos y no creemos que la altura de un español adulto dependa de su provincia de origen, no hay ninguna necesidad de esforzarse en tomar una muestra de cada provincia de manera que todas las provincias estén representadas adecuadamente en la muestra. Muestreo por conglomerados El proceso de obtener y estudiar una muestra aleatoria en algunos casos es caro o difícil, incluso aunque dispongamos de la lista completa de la población. Imaginemos que quisiéramos estudiar los hábitos de alimentación de los estudiantes de Primaria de Baleares. Para ello, previo permiso de la autoridad competente, tendríamos que seleccionar una muestra representativa de los escolares de Baleares. Seguramente podríamos disponer de su lista completa y por lo tanto podríamos tomar una muestra aleatoria, pero entonces acceder a las niñas y niños que la formasen seguramente significaría contactar con unos pocos alumnos de muchos centros de primaria, lo que volvería el proceso lento y costoso. Y eso si la Conselleria d’Educació nos facilitase la lista completa de alumnos. Una alternativa posible sería, en vez de extraer una muestra aleatoria de todos los estudiantes de Primaria, escoger primero al azar unas pocas aulas de primaria de colegios de las Baleares, a las que llamamos en este contexto conglomerados (clusters), y formar entonces nuestra muestra con todos los alumnos de estas aulas. Y es que es mucho más sencillo poseer la lista completa de estudiantes de unas pocas aulas que conseguir la lista completa de todos los estudiantes de todos los colegios, y mucho más barato ir a unos pocos colegios concretos que ir a todos los colegios de las Islas a entrevistar a unos pocos estudiantes en cada centro. Por poner otro ejemplo, efectuamos también un muestreo por conglomerados cuando para medir algunas características de los ejemplares de una planta en un bosque concreto, cuadriculamos la superficie del bosque, escogemos una muestra aleatoria de sectores de la cuadrícula (serían los conglomerados de este ejemplo) y estudiamos las plantas de interés contenidas en los sectores elegidas. Volviendo de nuevo a nuestra urna, supongamos que sus 100 bolas se agrupan en 20 conglomerados de 5 bolas cada uno según las franjas verticales de la Figura 2.8 (donde mantenemos la clasificación en dos colores para poder comparar el resultado del muestreo por conglomerados con el estratificado). Figura 2.8: Nuestra urna ahora tiene 2 estratos y 20 conglomerados Para obtener una muestra aleatoria por conglomerados de tamaño 15, escogeríamos al azar 3 conglomerados y la muestra estaría formada por sus bolas. La Figura 2.9 describe una muestra obtenida de esta manera: los conglomerados escogidos están marcados en azul. Figura 2.9: Una muestra aleatoria por conglomerados Observad la diferencia entre el muestreo estratificado y el muestreo por conglomerados: En una muestra estratificada se escoge una muestra aleatoria de cada estrato existente. En una muestra por conglomerados se escogen algunos conglomerados al azar y se incluye en la muestra todos sus elementos. Muestreos no aleatorios Cuando la selección de la muestra no es aleatoria, se habla de muestreo no aleatorio. En realidad es el tipo más frecuente de muestreo porque casi siempre nos tenemos que conformar con los sujetos disponibles. Por ejemplo, en la UIB, para estimar la opinión que de un profesor tienen los alumnos de una clase, se consulta solo a los estudiantes que voluntariamente rellenan la encuesta de opinión, que de ninguna manera forman una muestra aleatoria: el perfil del estudiante que contesta voluntariamente una encuesta de este tipo está muy definido y no viene determinado por el azar. En este caso se trataría de una muestra auto-seleccionada. Otro tipo de muestras no aleatorias son las oportunistas. Este es el caso, por ejemplo, si para estimar la opinión que de un profesor tienen los alumnos de una asignatura se visita un día la clase y se pasa la encuesta a los estudiantes que ese día asistieron a clase. De nuevo, puede que los alumnos presentes no sean representativos del alumnado de la asignatura (pueden ser los más aplicados, o los que no tienen la gripe, o a los que la asignatura no les coincide con otra). Veamos otros ejemplos de muestreo oportunista. Supongamos que queremos estudiar una característica de los animales de una determinada especie en un hábitat, y la medimos en los animales que capturamos. Estos ejemplares no tienen por qué ser representativos de la población: a lo mejor son los menos espabilados. O imaginad que tenéis una bolsa con bolas de diferentes tamaños. Si las removéis bien, las pequeñas tenderán a ir a parar al fondo y las grandes a quedar en la parte superior. Por lo tanto, si tomáis una muestra de la capa superior (que será lo más cómodo), no será representativa del total de la bolsa. La Figura 2.10 describe una muestra oportunista de nuestra urna: sus 15 primeras bolas. Aunque toda muestra de un mismo tamaño tiene la misma probabilidad de obtenerse por medio de un muestreo aleatorio sin reposición, es difícil de creer que esta muestra sea aleatoria; basta que calculéis cuál es la probabilidad de que en una muestra aleatoria de 15 bolas de nuestra urna todas tengan el mismo color: \\[ \\frac{40\\times 39\\times \\cdots\\times 26+60\\times 59\\times \\cdots\\times 46}{100\\times 99\\times \\cdots\\times 86}=2.1\\times 10^{-4} \\] Figura 2.10: Una muestra oportunista Las técnicas de estadística inferencial no se pueden aplicar a muestras no aleatorias, pero normalmente son las únicas que podemos conseguir. En este caso, lo que se suele hacer es describir en detalle las características de la muestra para justificar que, pese a no ser aleatoria, es representativa de la población y podría haber sido aleatoria. Por ejemplo, la muestra oportunista anterior de nuestra urna no es de ninguna manera representativa de su contenido por lo que refiere al color de las bolas. Muestreo polietápico En el ejemplo de los estudiantes de Primaria, la muestra final de estudiantes ha estado formada por todos los individuos de las aulas elegidas. Otra opción podría haber sido, tras seleccionar la muestra aleatoria de conglomerados, tomar de alguna manera una muestra aleatoria de cada uno de ellos. Por ejemplo, algunos estudios poblacionales a nivel estatal se realizan solamente en algunas provincias escogidas aleatoriamente, en las que luego se encuesta una muestra aleatoria de habitantes. Este sería un ejemplo de muestreo polietápico, en el que la muestra no se obtiene en un solo paso, sino mediante diversas elecciones sucesivas. La Figura 2.11 muestra un ejemplo sencillo de muestreo polietápico de nuestra urna: hemos elegido al azar 5 conglomerados (marcados en azul) y de cada uno de ellos hemos elegido 3 bolas al azar sin reposición. Figura 2.11: Una muestra polietápica Otro ejemplo enrevesado (pero real) de muestreo polietápico sería, para elegir una muestra de adolescentes de una ciudad grande, escoger en primer lugar 4 secciones censales al azar; a continuación, escoger al azar 10 manzanas de cada una de estas secciones censales y una esquina de cada manzana; finalmente, recorrer cada manzana en sentido horario a partir de la esquina seleccionada y visitar un portal de cada tres, entrevistando todos los habitantes de 13 a 19 años en las casas o fincas visitadas. En este proceso, hemos realizado tres muestreos aleatorios sin reposición (de secciones censales, de manzanas y de esquinas) y un muestreo sistemático (los portales). Si además los adolescentes que estudiamos al final no son todos los que viven en los portales seleccionados sino solo los que encontramos en casa el día que los visitamos, este muestreo oportunista significaría un cuarto paso en la formación de la muestra. Existen otros tipos de muestreo, solo hemos explicado los más comunes. En cualquier caso, lo importante es recordar que el estudio estadístico que se realice a posteriori deberá ser diferente según el tipo de muestreo usado. Por ejemplo, no se pueden usar las mismas técnicas para analizar una muestra aleatoria simple que una muestra por conglomerados. En este curso estudiaremos las propiedades de las diferentes técnicas de estimación solamente para el caso de muestreo aleatorio simple, es decir, al azar y con reposición, o al azar sin reposición si la población es muy, muy grande en comparación con la muestra. 2.2 Muestreo aleatorio con R Recordemos que un método de selección al azar de muestras de tamaño n (es decir, formadas por n individuos) de una cierta población produce muestras aleatorias simples (m.a.s.) cuando todas las muestras posibles de n individuos (con posibles repeticiones) tienen la misma probabilidad de ser elegidas. El tener una m.a.s. de una población junto con un tamaño muestral adecuado n nos asegurará que la estimación que hagamos sea muy probablemente correcta. La manera más sencilla de llevar a cabo un muestreo aleatorio simple es numerar todos los individuos de una población y sortearlos eligiendo números de uno en uno como si se tratase de una lotería, por ejemplo con algún generador de números aleatorios. Esto se puede llevar a cabo fácilmente con R. R dispone de un generador de muestras aleatorias de un vector. La función básica es sample(x, n, replace=...) donde: x es un vector o un número natural \\(x\\), en cuyo caso R entiende que representa el vector 1,2,…,\\(x\\); n es el tamaño de la muestra que deseamos extraer; el parámetro replace puede igualarse a TRUE, y será una muestra aleatoria con reposición, es decir, simple, o a FALSE, y será una muestra aleatoria sin reposición. Este último es su valor por defecto, por lo que no es necesario especificarlo si se quiere obtener una muestra sin reposición. Los dos primeros parámetros han de entrarse en este orden o igualados a los parámetros x y size, respectivamente. Así, por ejemplo, para obtener una m.a.s. de 15 números entre 1 y 100, podemos entrar: sample(100,15,replace=TRUE) ## [1] 48 5 17 72 84 99 72 40 6 27 7 25 99 92 69 Naturalmente, y como ya nos encontramos en la Lección 1 cuando generábamos vectores aleatorios con una distribución dada, cada ejecución de sample con los mismos parámetros puede dar lugar a muestras diferentes, y todas ellas tienen la misma probabilidad de aparecer: sample(100,15,replace=TRUE) ## [1] 7 84 46 43 15 95 95 53 63 18 41 33 4 62 57 sample(100,15,replace=TRUE) ## [1] 49 79 35 4 93 90 16 32 44 7 44 34 72 60 50 sample(100,15,replace=TRUE) ## [1] 55 9 45 96 55 13 32 68 67 2 35 85 52 57 3 Veamos cómo extraer una m.a.s de una tabla de datos. Recordemos el dataframe iris, que recoge medidas de pétalos y sépalos de 150 flores de tres especies de iris. str(iris) ## &#39;data.frame&#39;:\t150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Si queremos extraer una m.a.s. de 15 ejemplares (filas) de esta tabla de datos, podemos generar con sample una m.a.s. de índices de filas de la tabla (recordad que dim aplicado a un dataframe nos da un vector con sus dimensiones, es decir, sus números de filas y de columnas, en este orden; por lo tanto, dim(iris)[1] es el número de filas de iris): x=sample(dim(iris)[1],15,replace=TRUE) y a continuación crear un dataframe que contenga solo estas filas: muestra_iris=iris[x,] muestra_iris ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 46 4.8 3.0 1.4 0.3 setosa ## 133 6.4 2.8 5.6 2.2 virginica ## 85 5.4 3.0 4.5 1.5 versicolor ## 89 5.6 3.0 4.1 1.3 versicolor ## 91 5.5 2.6 4.4 1.2 versicolor ## 43 4.4 3.2 1.3 0.2 setosa ## 114 5.7 2.5 5.0 2.0 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 100 5.7 2.8 4.1 1.3 versicolor ## 102 5.8 2.7 5.1 1.9 virginica ## 32 5.4 3.4 1.5 0.4 setosa ## 107 4.9 2.5 4.5 1.7 virginica ## 115 5.8 2.8 5.1 2.4 virginica ## 66 6.7 3.1 4.4 1.4 versicolor Si solo quisiéramos una muestra aleatoria de longitudes de pétalos, podríamos aplicar directamente la función sample al vector correspondiente: muestra_long_pet=sample(iris$Petal.Length,15,replace=TRUE) muestra_long_pet ## [1] 6.6 5.0 4.5 1.6 3.6 4.0 1.3 4.6 1.6 3.6 1.5 1.5 1.6 3.8 6.1 El hecho de que funciones como sample o los generadores de vectores aleatorios con una cierta distribución de probabilidad fijada, como rnorm o rbinom, produzcan… pues eso, vectores aleatorios, puede tener inconvenientes a la hora de reproducir una simulación. R permite “fijar” el resultado de una función aleatoria con la instrucción set.seed. Sin entrar en detalles sobre cómo funcionan, los diferentes algoritmos que usa R para generar números aleatorios usan una semilla de aleatoriedad, que se modifica después de la ejecución del algoritmo, y por eso cada vez dan un resultado distinto. Pero, para una semilla fija, el algoritmo da el mismo resultado siempre. Lo que hace la función set.seed es igualar esta semilla al valor que le entramos. Si tras aplicar esta función a un número concreto ejecutamos una instrucción que genere un vector aleatorio de una longitud fija con una distribución fija, el resultado será siempre el mismo. Veamos un ejemplo de su efecto, generando muestras aleatorias simples de 10 longitudes de pétalos de flores iris con diferentes semillas de aleatoriedad: sample(iris$Petal.Length,10,replace=TRUE) ## [1] 6.7 5.1 3.3 1.6 4.0 1.5 1.5 1.5 1.4 1.6 set.seed(20) sample(iris$Petal.Length,10,replace=TRUE) ## [1] 4.5 5.0 5.8 4.3 1.4 1.9 4.8 1.3 4.5 5.7 set.seed(20) sample(iris$Petal.Length,10,replace=TRUE) ## [1] 4.5 5.0 5.8 4.3 1.4 1.9 4.8 1.3 4.5 5.7 sample(iris$Petal.Length,10,replace=TRUE) ## [1] 1.1 4.7 1.7 5.6 5.6 4.7 1.4 4.8 4.9 5.6 set.seed(10) sample(iris$Petal.Length,10,replace=TRUE) ## [1] 5.6 4.7 5.3 4.0 4.4 1.2 5.1 4.7 1.7 1.4 set.seed(10) sample(iris$Petal.Length,10,replace=TRUE) ## [1] 5.6 4.7 5.3 4.0 4.4 1.2 5.1 4.7 1.7 1.4 Ejecutado inmediatamente después de set.seed(20), sample(iris$Petal.Length,10,replace=TRUE) siempre da lo mismo. Y ejecutado después de set.seed(10), sample(iris$Petal.Length,10,replace=TRUE) vuelve a dar siempre lo mismo, pero diferente de con set.seed(20). La función set.seed no solo fija el resultado de la primera instrucción tras ella que genere un vector aleatorio, sino que, como fija la semilla de aleatoriedad y las funciones posteriores la modificarán de manera determinista, también fija los resultados de todas las instrucciones siguientes que generen vectores aleatorios. set.seed(100) sample(10,3) ## [1] 10 7 6 sample(10,3) ## [1] 3 9 2 sample(10,3) ## [1] 7 6 9 set.seed(100) sample(10,3) ## [1] 10 7 6 sample(10,3) ## [1] 3 9 2 sample(10,3) ## [1] 7 6 9 Si queréis volver a “reiniciar” la semilla de la aleatoriedad tras haber usado un set.seed, podéis usar set.seed(NULL). set.seed(100) sample(10,3) ## [1] 10 7 6 set.seed(NULL) sample(10,3) ## [1] 7 10 8 set.seed(100) sample(10,3) ## [1] 10 7 6 set.seed(NULL) sample(10,3) ## [1] 9 2 10 A veces querremos tomar diversas muestras aleatorias de una misma población y calcular algo sobre ellas. Para hacerlo podemos usar la función replicate. La sintaxis básica es replicate(n, instrucción) donde n es el número de repeticiones de la instrucción. Por ejemplo, para tomar 10 muestras aleatorias simples de 15 longitudes de pétalos de flores iris, podemos hacer: muestras=replicate(10, sample(iris$Petal.Length,15,replace=TRUE)) muestras ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 4.2 4.7 5.6 5.6 5.0 5.2 1.7 4.5 4.5 5.1 ## [2,] 5.2 3.9 1.3 4.7 5.2 1.4 4.5 5.1 4.9 1.2 ## [3,] 5.5 3.5 4.5 4.6 4.3 6.1 6.0 4.0 1.5 1.7 ## [4,] 1.3 5.3 5.1 5.1 1.7 4.4 3.8 6.1 3.8 1.5 ## [5,] 1.4 4.3 1.5 4.1 5.6 1.5 5.8 4.5 1.3 6.9 ## [6,] 1.4 5.5 5.8 6.1 1.5 5.1 6.1 5.1 1.5 5.0 ## [7,] 6.7 6.1 1.6 4.9 1.4 6.1 4.5 4.7 5.0 5.8 ## [8,] 1.3 4.5 1.5 1.4 6.3 1.4 5.0 3.8 5.2 4.7 ## [9,] 4.1 1.5 5.0 4.8 4.5 5.6 5.5 5.4 4.7 3.5 ## [10,] 5.0 4.3 3.7 4.6 5.6 4.7 6.0 5.1 5.1 5.6 ## [11,] 1.6 1.0 1.4 5.4 5.1 1.3 1.3 1.5 5.4 5.7 ## [12,] 1.4 5.5 4.4 1.5 3.9 4.5 1.5 1.4 1.5 4.2 ## [13,] 1.5 4.0 1.6 6.3 5.3 1.6 5.1 5.5 4.7 1.6 ## [14,] 1.4 6.0 1.9 1.6 1.6 4.5 5.0 5.1 5.2 1.4 ## [15,] 1.4 3.0 1.3 4.1 1.6 4.1 1.4 4.7 6.0 1.9 Observad que R ha organizado los 10 vectores generados con el replicate como columnas de una matriz. Si solo nos hubiera interesado calcular las medias, redondeadas a 2 cifras decimales, de 10 muestras aleatorias simples de 15 longitudes de pétalos de flores iris, podríamos haber hecho medias=replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2)) medias ## [1] 3.42 3.85 4.00 4.17 3.91 4.11 3.08 3.69 3.67 3.76 En este caso, como el resultado de la instrucción que iteramos es un solo número, los resultados del replicate forman un vector. ¿Y si quisiéramos la media y la desviación típica muestral de 10 muestras de estas? No podemos usar sin más dos replicate, como en rbind(replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2)), replicate(10,round(sd(sample(iris$Petal.Length,15,replace=TRUE)),2))) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 4.05 3.69 3.77 3.61 3.43 2.98 3.93 4.37 3.35 4.09 ## [2,] 1.75 1.35 1.71 1.79 1.76 1.69 1.86 1.72 2.02 1.64 porque es muy probable que el conjunto de muestras de las que hemos calculado la media en el primer replicate sea diferente del conjunto de muestras de las que hemos calculado la desviación típica en el segundo replicate. Lo más adecuado es definir una función que calcule un vector con estos dos valores (la función info del chunk siguiente), y luego usarla dentro de un único replicate: info=function(x){round(c(mean(x),sd(x)),2)} info_lp=replicate(10,info(sample(iris$Petal.Length,15,replace=TRUE))) info_lp ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 3.45 3.52 3.79 3.82 4.21 4.35 3.69 3.56 3.62 2.89 ## [2,] 1.71 1.83 1.72 1.64 1.63 1.93 1.93 1.85 1.72 1.85 En este último caso, R ha organizado la información obtenida como columnas de una matriz: la primera fila son las medias y la segunda las desviaciones típicas. Naturalmente, la función set.seed permite “fijar” el resultado de un replicate que incluya la generación de valores aleatorios: set.seed(1000) replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2)) ## [1] 3.25 3.85 4.35 3.24 4.25 4.36 3.39 3.77 3.75 3.90 set.seed(1000) replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2)) ## [1] 3.25 3.85 4.35 3.24 4.25 4.36 3.39 3.77 3.75 3.90 Un último comentario sobre la función sample. Aunque aquí la vamos a usar principalmente para tomar muestras aleatorias en las que todos los sujetos de la población tengan la misma probabilidad de ser escogidos, también podemos emplearla para obtener muestras en las que diferentes sujetos puedan tener probabilidades diferentes de salir. Estas probabilidades se especifican con el parámetro prob igualado a un vector de probabilidades (o de pesos proporcionales a probabilidades) de la misma longitud que el vector x al cual apliquemos sample. De esta manera, la primera entrada de prob representa la probabilidad del primer elemento de x, la segunda entrada de prob representa la probabilidad del segundo elemento de x, etc. Por ejemplo, si queremos tomar una muestra aleatoria de tamaño 10 del vector (1,2,3) de manera que cada elemento de este vector tenga probabilidad de ser escogido proporcional a su valor (es decir, el 2 tiene el doble de probabilidades de aparecer en la muestra que el 1, y el 3, el triple), podemos usar: sample(1:3,10,prob=1:3) ## Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when &#39;replace = FALSE&#39; ¡Ups! Para tomar una muestra de 10 elementos de una población de 3 sujetos, habrá que permitir repeticiones sample(1:3,10,replace=TRUE,prob=1:3) ## [1] 3 3 2 3 1 3 2 2 3 2 Para terminar esta lección, damos una función sencilla para efectuar muestreos sistemáticos aleatorios. El objetivo es, dado un vector de longitud n, obtener una muestra de tamaño n. Lo que haremos será tomar el cociente por exceso \\(k=\\lceil N/n\\rceil\\) de n entre n para determinar el período con el que tenemos que tomar los elementos de manera que todos los elementos puedan ser escogidos. A continuación elegimos al azar un elemento del vector con sample y a partir de él generamos una progresión aritmética de n elementos y paso \\(k\\), volviendo al inicio del vector si llegamos al final sin haber completado la muestra (lo que especificamos tomando los valores de la progresión aritmética módulo n). sist.sample=function(N,n){ k=ceiling(N/n) x0=sample(N,1) seq(x0,length.out=n,by=k)%%N } Por ejemplo, una muestra sistemática de 10 flores iris se podría obtener de la manera siguiente: x=sist.sample(dim(iris)[1],10) #Los índices de la muestra sistemática muestra_sist_iris=iris[x,] #La muestra de la tabla iris muestra_sist_iris Sepal.Length Sepal.Width Petal.Length Petal.Width Species 86 6.0 3.4 4.5 1.6 versicolor 101 6.3 3.3 6.0 2.5 virginica 116 6.4 3.2 5.3 2.3 virginica 131 7.4 2.8 6.1 1.9 virginica 146 6.7 3.0 5.2 2.3 virginica 11 5.4 3.7 1.5 0.2 setosa 26 5.0 3.0 1.6 0.2 setosa 41 5.0 3.5 1.3 0.3 setosa 56 5.7 2.8 4.5 1.3 versicolor 71 5.9 3.2 4.8 1.8 versicolor Como 150/10=15, podemos observar que los índices avanzan de 15 en 15 a partir del que ha sido escogido al azar en primer lugar. 2.3 Guía rápida pbirthday(n,N) calcula la probabilidad de que se produzca al menos una repetición en una m.a.s. de tamaño n tomada de una población de N individuos. qbirthday(p,N) calcula el tamaño mínimo que ha de tener una m.a.s. de una población de N individuos para que la probabilidad de una repetición sea como mínimo \\(p\\). sample(x, n) genera una muestra aleatoria de tamaño n del vector x. Si x es un número natural \\(x\\), representa el vector 1,2,…,\\(x\\). Dispone de los dos parámetros siguientes: replace, que igualado a TRUE produce muestras con reposición e igualado a FALSE (su valor por defecto) produce muestras sin reposición. prob, que permite especificar las probabilidades de aparición de los diferentes elementos de x (por defecto, son todas la misma). set.seed permite fijar la semilla de aleatoriedad. replicate(n,expresión) evalúa n veces la expresión, y organiza los resultados como las columnas de una matriz (o un vector, si el resultado de cada expresión es unidimensional). 2.4 Ejercicios Test (1) Queremos escoger 100 estudiantes de grado de la UIB para preguntarles cuántas horas semanales estudian. Como creemos que el tipo de estudio cursado influye en este dato, clasificamos los estudiantes según el centro (facultad o escuela) en el que están matriculados, y tomaremos una muestra al azar de cada centro, por sorteo a partir de la lista de todos los matriculados en ese centro y de manera que el tamaño de la muestra de cada centro sea proporcional al número de matriculados en el mismo. ¿De qué tipo de muestreo se tratará? Muestreo aleatorio simple Muestreo aleatorio estratificado Muestreo aleatorio sin reposición Muestreo aleatorio por conglomerados Muestreo aleatorio sistemático Ninguno de los anteriores (2) Con una sola instrucción, calculad la media de una muestra aleatoria sin reposición de 15 elementos escogidos de un vector numérico llamado \\(X\\). (3) Con una sola instrucción, extraed un subdataframe del dataframe iris formado por una muestra aleatoria sin reposición de 40 filas, y llamadlo muestra. Y antes de contestar, comprobad que funciona. (4) Con una sola instrucción, calculad un vector formado por las medias de 100 muestras aleatorias sin reposición de 20 elementos cada una escogidos de un vector numérico llamado \\(X\\) y llamadlo medias. Problemas (1) El bloque de código siguiente carga en un dataframe llamado DCR la tabla de datos datacrab.txt que se encuentra en el url https://raw.githubusercontent.com/AprendeR-UIB/Material/master/datacrab.txt y que contiene información sobre una muestra de cangrejos. library(RCurl) datos=getURL(&quot;https://raw.githubusercontent.com/AprendeR-UIB/Material/master/datacrab.txt&quot;) DCR=read.table(text=datos,header=TRUE) str(DCR) ## &#39;data.frame&#39;:\t173 obs. of 5 variables: ## $ input : int 3 4 2 4 4 3 2 4 3 4 ... ## $ color.spine: int 3 3 1 3 3 3 1 2 1 3 ... ## $ width : num 28.3 22.5 26 24.8 26 23.8 26.5 24.7 23.7 25.6 ... ## $ satell : int 8 0 9 0 4 0 0 0 0 0 ... ## $ weight : int 3050 1550 2300 2100 2600 2100 2350 1900 1950 2150 ... (a) Definid una función de parámetros N, n y s que tome N muestras aleatorias simples de n filas de este dataframe usando como semilla de aleatoriedad el número \\(s\\); a continuación, calcule las medias de los pesos de los individuos de cada una de estas muestras; y finalmente calcule la media y la desviación típica muestral del vector formado por estas medias. Tenéis que usar set.seed y replicate para definir la función. (b) Aplicadla a N=100, n=30 y tomando como \\(s\\) el número formado por las 5 primeras cifras de vuestro NIF o pasaporte. (c) ¿Qué valores predice el Teorema Central del Límite que se deberían obtener? ¿Habéis obtenido resultados similares a los predichos por dicho teorema? (2) Extraed una muestra aleatoria estratificada sin reposición de 15 flores de la tabla iris manteniendo en la muestra las proporciones de las diferentes especies en la tabla. Respuestas al test (1) b (2) mean(sample(X,15)) (También sería correcto sum(sample(X,15))/15. Y en ambos casos también sería correcto añadiendo dentro de la función sample el parámetro replace=FALSE, que hemos omitido porque es el valor por defecto de replace.) (3) muestra=iris[sample(dim(iris)[1],40),] (También sería correcto consultar antes el número de filas con str o tail, ver que son 150, y responder muestra=iris[sample(150,40),]. Hay otras respuestas correctas, no las damos para no liaros. Además, y como antes, también sería correcto añadir replace=FALSE.) (4) medias=replicate(100,mean(sample(X,20))) (¿Ya os hemos dicho que también sería correcto incluyendo replace=FALSE en el argumento de sample?) Soluciones sucintas de los problemas (1) (a) Una posible función: M=function(N,n,s){ set.seed(s) Muestras=replicate(N,mean(sample(DCR$weight,n,replace=TRUE))) Res=round(c(mean(Muestras),sd(Muestras)),2) attr(Res,&quot;names&quot;)=c(&quot;Media&quot;,&quot;Desv. Típica&quot;) Res } (b) M(100,30,42) ## Media Desv. Típica ## 2439.62 106.48 (c) El Teorema Central del Límite predice como media la de la población (que en este caso es el vector DCR$weight) round(mean(DCR$weight),2) ## [1] 2437.19 y como desviación típica la desviación típica de la población dividida por la raíz cuadrada del tamaño de las muestras round((sd(DCR$weight)*sqrt(length(DCR$weight)-1)/sqrt(length(DCR$weight)))/sqrt(30),2) ## [1] 105.04 (2) Vamos a dar una función mucho más general de la que necesitamos en este ejercicio. Sus parámetros son un dataframe DF, el factor Fact con el que clasificamos sus filas, el tamaño n de la muestra, y rep, un valor lógico que indique si tomamos las muestras con o sin reposición y que por defecto valdrá FALSE, es decir, sin reposición, como la definición por defecto de sample: Clust_sample=function(DF,Fact,n,rep=FALSE){ # Cargamos las variables del dataframe en el entorno global attach(DF) # Calculamos los tamaños de las muestras de cada estrato Tamaños=round(prop.table(table(Fact))*n) # Para cada nivel del factor, tomamos una muestra aleatoria # sin reposición de las filas de ese nivel y del tamaño que # hemos calculado Muestra=c() for(i in 1:length(levels(Fact))){ Muestra=c(Muestra,sample(which(Fact==levels(Fact)[i]),Tamaños[i],replace=rep)) } # Borramos las variables del dataframe del entorno global detach(DF) # Si la muestra ha quedado &quot;corta&quot; debido a redondeos al calcular # los tamaños de las submuestras, añadimos al azar tantas filas # como hagan falta. # Distinguimos si permitimos repeticiones o no if (rep==FALSE){ # Las nuevas filas han de ser diferentes de las anteriores Muestra=c(Muestra,sample((1:dim(DF)[1])[-Muestra],n-length(Muestra))) }else{ # Las nuevas filas pueden ser iguales a las anteriores Muestra=c(Muestra,sample((1:dim(DF)[1]),n-length(Muestra),replace=TRUE)) } sort(Muestra) } Apliquemos la función a la tabla iris: iris[Clust_sample(iris,Species,15),] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 13 4.8 3.0 1.4 0.1 setosa ## 15 5.8 4.0 1.2 0.2 setosa ## 18 5.1 3.5 1.4 0.3 setosa ## 23 4.6 3.6 1.0 0.2 setosa ## 28 5.2 3.5 1.5 0.2 setosa ## 56 5.7 2.8 4.5 1.3 versicolor ## 63 6.0 2.2 4.0 1.0 versicolor ## 65 5.6 2.9 3.6 1.3 versicolor ## 69 6.2 2.2 4.5 1.5 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 140 6.9 3.1 5.4 2.1 virginica Una construcción equivalente, sin usar un bucle for Clust_sample2=function(DF,Fact,n,rep=FALSE){ attach(DF) M=length(levels(Fact)) Tamaños=round(prop.table(table(Fact))*n) F=function(i){sample(which(Fact==levels(Fact)[i]),Tamaños[i],replace=rep)} Muestra=unlist(sapply(1:M,FUN=F)) detach(DF) if (rep==FALSE){ Muestra=c(Muestra,sample((1:dim(DF)[1])[-Muestra],n-length(Muestra))) }else{ Muestra=c(Muestra,sample((1:dim(DF)[1]),n-length(Muestra),replace=TRUE)) } sort(Muestra) } iris[Clust_sample2(iris,Species,15),] Sepal.Length Sepal.Width Petal.Length Petal.Width Species 13 4.8 3.0 1.4 0.1 setosa 18 5.1 3.5 1.4 0.3 setosa 25 4.8 3.4 1.9 0.2 setosa 29 5.2 3.4 1.4 0.2 setosa 44 5.0 3.5 1.6 0.6 setosa 51 7.0 3.2 4.7 1.4 versicolor 57 6.3 3.3 4.7 1.6 versicolor 77 6.8 2.8 4.8 1.4 versicolor 88 6.3 2.3 4.4 1.3 versicolor 100 5.7 2.8 4.1 1.3 versicolor 121 6.9 3.2 5.7 2.3 virginica 123 7.7 2.8 6.7 2.0 virginica 137 6.3 3.4 5.6 2.4 virginica 139 6.0 3.0 4.8 1.8 virginica 148 6.5 3.0 5.2 2.0 virginica Por ejemplo, podríamos usar esta función para extraer una muestra aleatoria estratificada con reposición de 15 cangrejos de la tabla datacrab.txt usada en el problema anterior usando como estratos los valores de la variable color.spine. prop.table(table(DCR$color.spine)) ## ## 1 2 3 ## 0.2138728 0.0867052 0.6994220 DCR$color.spine=as.factor(DCR$color.spine) Muestra=DCR[Clust_sample(DCR,color.spine,15,TRUE),] Muestra ## input color.spine width satell weight ## 50 3 1 30.3 3 3600 ## 62 4 3 24.5 5 2050 ## 64 3 3 26.0 5 2150 ## 67 3 3 29.0 10 3200 ## 74 3 3 25.4 6 2250 ## 81 4 2 24.5 0 2250 ## 82 5 3 27.5 0 2900 ## 116 5 3 23.7 0 1800 ## 137 3 1 28.4 5 3100 ## 147 3 1 31.7 4 3725 ## 151 3 3 27.6 4 2850 ## 152 3 3 26.2 0 2300 ## 156 3 3 24.7 4 1950 ## 162 3 3 26.0 3 2275 ## 165 3 3 26.5 7 2750 prop.table(table(Muestra$color.spine)) ## ## 1 2 3 ## 0.20000000 0.06666667 0.73333333 "],
["chap-estimacion.html", "Lección 3 Estimación puntual 3.1 Estimación máximo verosímil 3.2 Guía rápida 3.3 Ejercicios", " Lección 3 Estimación puntual En un estudio inferencial, una vez tomada la muestra y obtenidos los datos sobre sus miembros, el siguiente paso es inferir, es decir, deducir información sobre la población a partir de estos datos. Dicha información se puede deducir de dos formas: Suponiendo que conocemos el modelo al que se ajusta la población: es decir, suponiendo que conocemos el tipo de distribución de la variable aleatoria que modela la característica de la población en la que estamos interesados, pero desconocemos uno o varios parámetros de los que depende dicha distribución (observad que si lo sabemos todo sobre esta distribución, ya no hace falta tomar muestras para inferir algo sobre ella). Así, podemos saber (o suponer) que las longitudes de los ejemplares adultos de una cierta especie se distribuyen según una variable aleatoria normal, pero desconocer sus parámetros \\(\\mu\\) (media) y \\(\\sigma\\) (desviación típica), y usar este conocimiento para inferir información sobre dichas longitudes a partir de las de una muestra: por ejemplo, para estimar con un cierto margen de error su longitud media. Si estamos en este caso, hablaremos de estimación paramétrica. Suponiendo que desconocemos qué tipo de distribución tiene la variable aleatoria que modela la característica que nos interesa (aunque a veces necesitaremos saber algo de esta distribución; por ejemplo, si es simétrica o no). En este caso, hablaremos de estimación no paramétrica. En ambos casos, existen tres vías para obtener información sobre los parámetros de la distribución (conocida o desconocida) de la variable aleatoria que nos interesa: Estimación puntual. Se trata de obtener expresiones matemáticas, llamadas estimadores puntuales, que aplicadas a los valores de una muestra nos dan una aproximación (el término exacto es una estimación) del valor de dicho parámetro para la población. A modo de ejemplo, la media aritmética de los datos \\(x_1,\\ldots,x_n\\) de una muestra aleatoria, \\[ \\overline{x}=\\frac{x_1+\\cdots +x_n}{n}, \\] es un estimador del valor medio (valor esperado, esperanza) de la variable aleatoria de la que hemos extraído la muestra. Estimación por intervalos de confianza. Se trata de obtener intervalos que contengan con probabilidad alta el parámetro objeto de estudio. Trataremos este tema en la Lección 4. Contraste de hipótesis. Grosso modo, se establecen dos hipótesis opuestas sobre el parámetro o, más en general, sobre la distribución de la variable aleatoria, y se contrastan para intentar decidir cuál es la verdadera. Los estudiaremos en próximas lecciones. En esta lección hablaremos de la estimación puntual. Para empezar, es obvio que no toda fórmula matemática sirve para estimar de manera sensata el valor de un parámetro. Por ejemplo, si queréis estimar la media de las alturas de los habitantes de una población y disponéis de una muestra aleatoria de las mismas, no tomáis la raíz cuadrada de la altura máxima en la muestra como estimación de la altura media de la población, ¿verdad? Lo que habéis hecho toda la vida, y seguiréis haciendo en este curso, ha sido calcular la media de las alturas en la muestra y dar ese valor como estimación de la altura media poblacional. Y es lo correcto, porque la media muestral es siempre un estimador insesgado de la media poblacional y muy a menudo es además su estimador máximo verosímil, Veamos qué significan estas propiedades. Insesgado: Los valores de un estimador sobre muestras aleatorias de una población forman una variable aleatoria con una distribución de probabilidad propia, llamada genéricamente muestral. Decimos entonces que un estimador es insesgado cuando su valor esperado (o sea, la esperanza de su distribución muestral) coincide con el valor del parámetro poblacional que se quiere estimar. Por ejemplo, si se toman muestras aleatorias con o sin reposición, la media muestral es siempre un estimador insesgado del valor medio poblacional: su valor esperado es el valor medio poblacional. Máximo verosímil: Cada muestra aleatoria de una población tiene una probabilidad de obtenerse que no solo depende de la muestra, sino también de la distribución de probabilidad de la variable aleatoria poblacional. Si la distribución poblacional es de un tipo concreto (Bernoulli, normal, …), esta probabilidad depende de sus parámetros. Decimos entonces que un estimador es máximo verosímil cuando el resultado que da sobre cada muestra aleatoria es el valor del parámetro poblacional que maximiza la probabilidad de obtenerla. Por ejemplo, si lanzamos una moneda al aire \\(n\\) veces y calculamos la proporción de veces que obtenemos cara, esa proporción muestral \\(\\widehat{p}\\) es el estimador máximo verosímil de la probabilidad \\(p\\) de obtener cara con esa moneda. Esto quiere decir que, de entre todas las distribuciones binomiales \\(B(n,p)\\) que pueden modelar el número de caras que obtenemos al lanzar \\(n\\) veces nuestra moneda, aquella que asigna mayor probabilidad al número de caras que hemos obtenido es la que tiene como parámetro \\(p\\) la frecuencia relativa de caras \\(\\widehat{p}\\) que hemos observado. Para algunas distribuciones, el método de estimación por máxima verosimilitud de sus parámetros da lugar a fórmulas cerradas más o menos sencillas, pero en otros casos nos tenemos que conformar con un valor aproximado obtenido mediante algún método numérico. 3.1 Estimación máximo verosímil A continuación recordamos una lista de los estimadores máximo verosímiles de los parámetros de las distribuciones más comunes a partir de una muestra aleatoria simple: Para la familia Bernoulli, el estimador máximo verosímil del parámetro \\(p\\) es la proporción muestral de éxitos \\(\\widehat{p}\\). Este estimador es además insesgado. Para la familia Poisson, el estimador máximo verosímil del parámetro \\(\\lambda\\) es la media muestral \\(\\overline{X}\\). Este estimador es de nuevo insesgado. Para la familia geométrica, el estimador máximo verosímil del parámetro \\(p\\) es \\({1}/{\\overline{X}}\\). Este estimador es sesgado. Para la familia exponencial, el estimador máximo verosímil del parámetro \\(\\lambda\\) también es \\({1}/{\\overline{X}}\\). Este estimador también es sesgado. Para la familia normal, los estimadores máximo verosímiles de la media \\(\\mu\\), la desviación típica \\(\\sigma\\) y la varianza \\(\\sigma^2\\) son, respectivamente, la media muestral \\(\\overline{X}\\), la desviación típica “verdadera” \\(S_X\\) y la varianza “verdadera” \\(S_X^2\\). Además, \\(\\overline{X}\\) es un estimador insesgado de \\(\\mu\\). La varianza verdadera \\(S_X^2\\) no es un estimador insesgado de \\(\\sigma^2\\), pero sí que lo es la varianza muestral \\(\\widetilde{S}_X^2\\). Y ninguna de las dos desviaciones típicas, ni la “verdadera” \\(S_X\\) ni la muestral \\(\\widetilde{S}_X\\), es un estimador insesgado de \\(\\sigma\\); si necesitáis un estimador insesgado de la desviación típica de una variable aleatoria normal a partir de una muestra aleatoria simple, lo podéis encontrar en la correspondiente entrada de la Wikipedia. No obstante, el beneficio de usar este estimador insesgado no suele compensar lo complicado de su cálculo. Cuando se estima algún parámetro de una distribución a partir de una muestra, es conveniente aportar el error típico, o estándar, como medida de la finura de la estimación. Recordemos que el error típico de un estimador es la desviación típica de su distribución muestral, y que el error típico de una estimación a partir de una muestra es la estimación del error típico del estimador usando dicha muestra. Veamos un ejemplo sencillo. Ejemplo 3.1 Supongamos que tenemos una muestra aleatoria simple de tamaño \\(n\\) de una variable \\(X\\) que sigue una distribución Bernoulli de probabilidad poblacional \\(p\\) desconocida que queremos estimar. Por ejemplo, puede ser que tengamos una moneda posiblemente trucada, la hayamos lanzado 100 veces al aire y hayamos anotado los resultados (1, cara, 0, cruz), y a partir de este experimento queramos estimar la probabilidad de sacar cara con esta moneda. O que hayamos anotado para 100 individuos de una población elegidos al azar si tienen o no una determinada enfermedad (1 significa que sí, 0 que no) y a partir de esta muestra deseemos estimar la prevalencia de la enfermedad en la población, es decir, la proporción real de enfermos, que coincide con la probabilidad de que un individuo elegido al azar tenga dicha enfermedad. Tomemos, para fijar ideas, la siguiente muestra de tamaño 100: x=c(0,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,1,0,1,0,0,0, 0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0, 1,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0) En este caso, podemos estimar \\(p\\) mediante la proporción muestral de éxitos \\(\\widehat{p}\\), que coincide con la media muestral. El error típico de este estimador es \\(\\sqrt{p(1-p)/n}\\), y el error típico de una estimación concreta es \\(\\sqrt{\\widehat{p}(1-\\widehat{p})/n}\\). Por lo tanto, a mano podemos estimar \\(p\\) y calcular el error típico de dicha estimación de la manera siguiente: n=length(x) #Tamaño de la muestra estim.p=mean(x) #Proporción muestral estim.p ## [1] 0.22 error.tip.p=sqrt(estim.p*(1-estim.p)/n) #Error típico de la estimación error.tip.p ## [1] 0.04142463 De esta manera, estimamos que \\(p\\)=0.22 con un error típico de 0.04. Con R podemos estimar un parámetro de una distribución por el método de máxima verosimilitud a partir de una muestra y además obtener el error típico de dicha estimación usando la función fitdistr del paquete MASS. Esta función calcula los estimadores máximo verosímiles de los parámetros de la mayoría de las familias de distribuciones disponibles en R. Su sintaxis básica es fitdistr(x, densfun=..., start=...) donde x es la muestra, un vector numérico. El valor de densfun ha de ser el nombre de la familia de distribuciones; se tiene que entrar entre comillas y puede tomar, entre otros, los valores siguientes: \"chi-squared\", \"exponential\", \"f\", \"geometric\", \"lognormal\", \"normal\" y \"poisson\". La lista de distribuciones a las que se puede aplicar, que podéis consultar en la Ayuda de la función, no incluye la Bernoulli ni la binomial. Si fitdistr no dispone de una fórmula cerrada para el estimador máximo verosímil de algún parámetro, usa un algoritmo numérico para aproximarlo que requiere de un valor inicial para arrancar. Este valor (o valores) se puede especificar igualando el parámetro start a una list con cada parámetro a estimar igualado a un valor inicial. Para algunas distribuciones, como la \"t\", fitdistr sabe tomar valores iniciales razonables, y no es necesario especificar el parámetro start. Pero para otras distribuciones, como por ejemplo la \"chi-squared\", es obligatorio especificarlo. Para las distribuciones que disponen de fórmula cerrada, como la \"normal\" o la \"poisson\", se tiene que omitir el parámetro start. Como no podemos usar la función fitdistr para estimar el parámetro \\(p\\) de una Bernoulli (los autores del paquete debieron de considerar que era más fácil estimarlo directamente), vamos a usarla en otro ejemplo. Ejemplo 3.2 Consideremos la siguiente muestra y de 100 valores generados con distribución de Poisson de parámetro \\(\\lambda=10\\): set.seed(100) y=rpois(100,10) set.seed(NULL) y ## [1] 8 10 9 12 10 11 8 12 7 11 11 12 7 10 9 8 11 7 6 12 10 12 7 ## [24] 7 10 6 7 15 9 9 7 8 15 11 12 5 14 4 7 14 8 14 10 4 9 8 ## [47] 11 11 10 12 7 14 7 9 10 3 10 7 9 21 14 6 13 3 10 6 3 13 9 ## [70] 12 8 11 10 11 11 8 6 17 7 8 10 12 15 12 13 10 9 12 8 11 12 4 ## [93] 10 8 5 8 8 10 8 11 Vamos a estimar el parámetro \\(\\lambda\\) de una distribución Poisson que haya generado este vector: library(MASS) fitdistr(y, densfun=&quot;poisson&quot;) ## lambda ## 9.5600000 ## (0.3091925) El resultado dice que el valor estimado de \\(\\lambda\\) es 9.56, con un error típico estimado de 0.31. Veámoslo directamente: el estimador máximo verosímil de \\(\\lambda\\) es la media aritmética \\(\\overline{X}\\) y el error típico de este estimador es \\(\\sqrt{\\lambda}/\\sqrt{n}\\) (recordad que la desviación típica de una Poisson de parámetro \\(\\lambda\\) es \\(\\sqrt{\\lambda}\\) y que el error típico de la media muestral es la desviación típica poblacional dividida por la raíz cuadrada del tamaño de la muestra), por lo que el error típico de una estimación es \\(\\sqrt{\\overline{X}}/\\sqrt{n}\\). mean(y) ## [1] 9.56 sqrt(mean(y)/length(y)) ## [1] 0.3091925 Ejemplo 3.3 También podemos estimar la media y la desviación típica de una variable normal que hubiera producido la muestra del ejemplo anterior: fitdistr(y, densfun=&quot;normal&quot;) ## mean sd ## 9.5600000 3.0832450 ## (0.3083245) (0.2180183) Observad que la estimación de la desviación típica que nos da fitdistr es la desviación típica “verdadera” \\(S_X\\) (que es su estimador máximo verosímil) y no la muestral \\(\\widetilde{S}_X\\): sd(y) ## [1] 3.098778 sqrt((length(y)-1)/length(y))*sd(y) ## [1] 3.083245 Y que, consistentemente, da como error típico de la estimación de la media \\(S_X/\\sqrt{n}\\) en vez de \\(\\widetilde{S}_X/\\sqrt{n}\\). Ejemplo 3.4 Vamos a estimar ahora el número de grados de libertad de una t de Student que hubiera producido la muestra del Ejemplo 3.2. fitdistr(y, densfun=&quot;t&quot;) ## m s df ## 9.5085516 2.7517475 9.9229027 ## (0.2997949) (0.3072053) (8.4890355) ¡Vaya!, aparte del número de grados de libertad, df, han aparecido parámetros que no esperábamos. Los parámetros m y s son los parámetros de posición, \\(\\mu\\), y de escala, \\(\\sigma\\), respectivamente, que definen una familia más general de distribuciones t de Student (si os interesa, consultad esta entrada de la Wikipedia). Las que usamos en este curso tienen \\(\\mu=0\\) y \\(\\sigma=1\\). ¿Cómo podríamos estimar los grados de libertad de una t de Student de las nuestras? Especificando dentro de fitdistr los valores de los parámetros que queremos que tomen un valor concreto: en este caso, añadiendo m=0 y s=1. fitdistr(y, densfun=&quot;t&quot;, m=0, s=1) ## Error in fitdistr(y, densfun = &quot;t&quot;, m = 0, s = 1): &#39;start&#39; must be a named list Ahora R nos pide que demos un valor inicial al número de grados de libertad, df, para poder arrancar el algoritmo numérico que usará. Vamos a inicializarlo a 1, y de paso veremos cómo se usa este parámetro: fitdistr(y, densfun=&quot;t&quot;, m=0, s=1, start=list(df=1)) ## Warning in stats::optim(x = c(8L, 10L, 9L, 12L, 10L, 11L, 8L, 12L, 7L, 11L, : one-dimensional optimization by Nelder-Mead is unreliable: ## use &quot;Brent&quot; or optimize() directly ## Warning in dt((x - m)/s, df, log = TRUE): NaNs produced ## df ## 0.37265625 ## (0.04277075) Obtenemos un número estimado de grados de libertad de la t de Student de aproximadamente 0.37 grados de libertad (sí, los grados de libertad de una t de Student pueden ser un número real positivo cualquiera). Por otro lado, R nos avisa de que el resultado es poco de fiar, pero tampoco nos importa mucho, porque el objetivo era mostrar un ejemplo de cómo fijar valores de parámetros, igualándolos a dichos valores, y cómo especificar el parámetro start, como una list donde asignamos a cada parámetro un valor inicial. El resultado de fitdistr es una list, y por lo tanto el valor de cada estimador y su error típico se pueden obtener con los sufijos adecuados. En concreto, los valores estimados forman la componente estimate y los errores típicos la componente sd. Para obtenerlos directamente, basta usar los sufijos $estimate y $sd, respectivamente: fitdistr(y,&quot;poisson&quot;)$estimate #Estimación de lambda ## lambda ## 9.56 fitdistr(y,&quot;poisson&quot;)$sd #Error típico ## lambda ## 0.3091925 fitdistr(y,&quot;normal&quot;)$estimate #Estimaciones ## mean sd ## 9.560000 3.083245 fitdistr(y,&quot;normal&quot;)$estimate[1] #Estimación de mu ## mean ## 9.56 fitdistr(y,&quot;normal&quot;)$estimate[2] #Estimación de sigma ## sd ## 3.083245 3.2 Guía rápida fitdistr, del paquete MASS, sirve para calcular los estimadores máximo verosímiles de los parámetros de una distribución a partir de una muestra. El resultado es una list que incluye los objetos estimate (los valores estimados) y sd (los errores típicos de las estimaciones). Sus parámetros principales son: densfun: el nombre de la familia de distribuciones, entre comillas. start: permite fijar el valor inicial del algoritmo numérico para calcular el estimador, si la función lo requiere. 3.3 Ejercicios Test (1) Las distribuciones de Weibull tienen dos parámetros: forma, shape, y escala, scale. Supongamos que los datos siguientes siguen una distribución de Weibull: 2.46, 2.28, 1.7, 0.62, 0.87, 2.81, 2.35, 2.08, 2.11, 1.72. Calculad el estimador máximo verosímil del parámetro de escala de esta distribución, redondeado a 3 cifras decimales. Tenéis que dar el resultado (sin ceros innecesarios a la derecha), no cómo lo habéis calculado. (2) Generad, con semilla de aleatoriedad igual a 42, una secuencia aleatoria de 100 valores con distribución geométrica Ge(0.6). A continuación estimad por máxima verosimilitud el parámetro \\(p\\) de una distribución geométrica que haya generado dicha muestra y dad como respuesta a esta pregunta el error típico de esta estimación redondeado a 3 cifras decimales. Tenéis que dar el resultado (sin ceros innecesarios a la derecha), no cómo lo habéis calculado. Problemas (1) Sea \\(X\\) una variable aleatoria normal \\(N(\\mu,\\sigma)\\). Suponemos que recordáis que si tomamos muestras aleatorias simples de \\(X\\) de tamaño n y calculamos su media aritmética \\(\\overline{X}\\) y su desviación típica muestral \\(\\widetilde{S}_X\\), la variable aleatoria \\[ T=\\frac{\\overline{X}-\\mu}{\\widetilde{S}_X/\\sqrt{n}} \\] sigue una distribución t de Student con n-1 grados de libertad. Vamos a mirar de confirmarlo con una simulación. (a) Construid un vector formado por los valores de \\(T\\) para 5000 muestras aleatorias simples de tamaño 10 de una variable normal estándar. A continuación, dad la estimación máximo verosímil de los tres parámetros de una variable t de Student que haya generado dicho vector y mirad si el resultado se acerca a lo predicho por la teoría (recordad del Ejemplo 3.4 que nuestras t de Student tienen los parámetros \\(\\mu=0\\) y \\(\\sigma=1\\)). (b) Repetid el apartado anterior a partir de una variable \\(N(5,2)\\) y muestras de tamaño 25. Respuestas al test (1) 2.116 Nosotros lo hemos calculado con x=c(2.46, 2.28, 1.7, 0.62, 0.87, 2.81, 2.35, 2.08, 2.11, 1.72) round(fitdistr(x,&quot;weibull&quot;)$estimate,3) ## shape scale ## 3.432 2.116 (2) 0.038 Nosotros lo hemos calculado con set.seed(42) x=rgeom(100,0.6) round(fitdistr(x,&quot;geometric&quot;)$sd,3) ## prob ## 0.038 Soluciones sucintas de los problemas (1) (a) Construimos el vector con el código siguiente (fijamos la semilla de aleatoriedad para que sea reproducible): set.seed(100) Est.T=function(x,mu){(mean(x)-mu)/(sd(x)/sqrt(length(x)))} Simulación.10=replicate(5000,Est.T(rnorm(10),0)) Ahora estimamos los parámetros con el código siguiente: fitdistr(Simulación.10,densfun=&quot;t&quot;)$estimate ## m s df ## -0.003707446 0.997435397 8.352281013 (b) Simulación.25=replicate(5000,Est.T(rnorm(25,5,2),5)) fitdistr(Simulación.25,densfun=&quot;t&quot;)$estimate ## m s df ## 0.03191577 1.00808360 23.77363044 "],
["chap-IC.html", "Lección 4 Intervalos de confianza 4.1 Intervalo de confianza para la media basado en la t de Student 4.2 Intervalos de confianza para la proporción poblacional 4.3 Intervalo de confianza para la varianza de una población normal 4.4 Bootstrap 4.5 Guía rápida 4.6 Ejercicios", " Lección 4 Intervalos de confianza En esta lección explicamos cómo calcular con R algunos intervalos de confianza básicos. Recordad que un intervalo de confianza del \\(q\\times 100\\%\\) (con \\(q\\) entre 0 y 1) para un parámetro poblacional (la media, la desviación típica, la probabilidad de éxito de una variable Bernoulli, …) es un intervalo obtenido aplicando a una muestra aleatoria simple una fórmula que garantiza (si se cumplen una serie de condiciones sobre la distribución de la variable aleatoria poblacional que en cada caso dependen del parámetro y de la fórmula) que el \\(q\\times 100\\%\\) de las veces que la aplicáramos a una muestra aleatoria simple de la misma población, el intervalo resultante contendría el parámetro poblacional que queremos estimar. Esto es lo que significa lo de “confianza del \\(q\\times 100\\%\\)”: que confiamos en que nuestra muestra pertenece al \\(q\\times 100\\%\\) de las muestras (aleatorias simples) en las que la fórmula acierta y da un intervalo que contiene el parámetro deseado. Algunas de las funciones que aparecen en esta lección volverán a salir en la próxima, ya que aunque calculan intervalos de confianza, su función principal es en realidad efectuar contrastes de hipótesis. 4.1 Intervalo de confianza para la media basado en la t de Student Supongamos que queremos estimar a partir de una m.a.s. la media \\(\\mu\\) de una variable \\(X\\) que sigue una distribución normal o de una variable cualquiera pero tomando una muestra grande (por fijar una cota, de tamaño 40 o mayor). En esta situación, si \\(\\overline{X}\\), \\(\\widetilde{S}_{X}\\) y \\(n\\) son, respectivamente, la media muestral, la desviación típica muestral y el tamaño de la muestra, un intervalo de confianza del \\(q\\times 100\\%\\) para \\(\\mu\\) es \\[\\begin{equation} \\overline{X}\\pm t_{n-1,(1+q)/2} \\cdot \\frac{\\widetilde{S}_{X}}{\\sqrt{n}} \\tag{4.1} \\end{equation}\\] donde \\(t_{n-1,(1+q)/2}\\) es el cuantil de orden \\((1+q)/2\\) de una variable aleatoria con distribución t de Student con \\(n-1\\) grados de libertad. Fijaos en que \\(\\widetilde{S}_{X}/\\sqrt{n}\\) es el error típico de la estimación de la media. A la hora de calcular este intervalo de confianza, tenemos dos posibles situaciones. Una, típica de ejercicios, es cuando de la muestra sólo conocemos su media muestral \\(\\overline{X}\\), su desviación típica muestral \\(\\widetilde{S}_X\\) y su tamaño \\(n\\). Si los denotamos por x.b, sdm y n, respectivamente, y denotamos el nivel de confianza en tanto por uno \\(q\\) por q, podemos calcular los extremos de este intervalo de confianza con la expresión siguiente: x.b+(qt((1+q)/2,n-1)*sdm/sqrt(n))*c(-1,1) Ahora bien, “en la vida real” lo usual es disponer de un vector numérico X con los valores de la muestra. En este caso, podemos usar la función t.test de R, que, entre otra información, calcula estos intervalos de confianza para \\(\\mu\\). Si solo nos interesa el intervalo de confianza, podemos usar la sintaxis siguiente: t.test(X,conf.level=...)$conf.int donde tenemos que igualar el parámetro conf.level al nivel de confianza \\(q\\) en tanto por uno. Si \\(q=0.95\\), no hace falta entrarlo, porque es su valor por defecto. Ejemplo 4.1 Tenemos una muestra de pesos en gramos de 28 recién nacidos con luxación severa de cadera: pesos=c(2466,3941,2807,3118,3175,3515,3317,3742,3062,3033,2353,3515,3260,2892, 4423,3572,2750,3459,3374,3062,3205,2608,3118,2637,3438,2722,2863,3513) Vamos a suponer que nuestra muestra es aleatoria simple y que los pesos al nacer de los bebés con esta patología siguen una distribución normal. A partir de esta muestra, queremos calcular un intervalo de confianza del 95% para el peso medio de un recién nacido con luxación severa de cadera, y ver si contiene el peso medio de la población global de recién nacidos, que es de unos 3400 g. Como suponemos que la variable aleatoria poblacional es normal, para calcular un intervalo de confianza del 95% para su valor medio vamos a usar la fórmula basada en la distribución t de Student, y por lo tanto la función t.test: t.test(pesos)$conf.int ## [1] 2997.849 3355.008 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 El intervalo que obtenemos es [2997.8, 3355] y está completamente a la izquierda del peso medio global de 3400 g, por lo que tenemos evidencia (a un 95% de confianza) de que los niños con luxación severa de cadera pesan de media al nacer por debajo de la media global. La apostilla entre paréntesis “a un 95% de confianza” aquí significa que hemos basado esta conclusión en un intervalo obtenido con una fórmula que acierta con una probabilidad del 95%, en el sentido de que el 95% de las ocasiones que aplicamos esta fórmula a una m.a.s. de una variable aleatoria normal, produce un intervalo que contiene la media de esta variable. Observad que el resultado de t.test(pesos)$conf.int tiene un atributo, conf.level, que indica su nivel de confianza. En principio este atributo no molesta para nada en cálculos posteriores con los extremos de este intervalo de confianza, pero si os molesta, lo podéis quitar igualándolo a NULL. IC.lux=t.test(pesos)$conf.int attr(IC.lux,&quot;conf.level&quot;)=NULL IC.lux ## [1] 2997.849 3355.008 Veamos cómo podríamos haber obtenido este intervalo directamente con la fórmula (4.1): x.b=mean(pesos) sdm=sd(pesos) n=length(pesos) q=0.95 x.b+(qt((1+q)/2,n-1)*sdm/sqrt(n))*c(-1,1) ## [1] 2997.849 3355.008 Como podéis ver, coincide con el intervalo obtenido con la función t.test. Ejemplo 4.2 Vamos a comprobar con un experimento esto de la “confianza” de los intervalos de confianza, y en concreto de la fórmula (4.1). Vamos a generar al azar una Población de 10,000,000 “individuos” con distribución normal estándard. Vamos a tomar 200 muestras aleatorias simples de tamaño 50 de esta población y calcularemos el intervalo de confianza para la media poblacional usando dicha fórmula. Finalmente, contaremos cuántos de estos intervalos de confianza contienen la media de la Población. Fijaremos la semilla de aleatoriedad para que el experimento sea reproducible y podáis comprobar que no hacemos trampa. En otras simulaciones habríamos obtenido resultados mejores o peores, es lo que tienen las simulaciones aleatorias. set.seed(42) Poblacion=rnorm(10^7) #La población mu=mean(Poblacion) # La media poblacional M=replicate(200, sample(Poblacion,50,replace=TRUE)) # Las muestras dim(M) ## [1] 50 200 Tenemos una matriz M de 200 columnas y 50 filas, donde cada columna es una m.a.s. de nuestra población. Vamos a aplicar a cada una de estas muestras la función t.test para calcular un intervalo de confianza del 95% y luego contaremos los aciertos, es decir, cuántos de ellos contienen la media poblacional IC.t=function(X){t.test(X)$conf.int} ICs=apply(M,FUN=IC.t,MARGIN=2) Aciertos=length(which((mu&gt;=ICs[1,]) &amp; (mu&lt;=ICs[2,]))) Aciertos ## [1] 189 Hemos acertado 189 veces, es decir, un 94.5% de los intervalos obtenidos contienen la media poblacional. No hemos quedado muy lejos del 95% esperado. Para visualizar mejor los aciertos, vamos a dibujar los intervalos apilados en un gráfico, donde aparecerán en azul claro los que aciertan y en rojo los que no aciertan. plot(1,type=&quot;n&quot;,xlim=c(-0.8,0.8),ylim=c(0,200),xlab=&quot;Valores&quot;,ylab=&quot;Repeticiones&quot;,main=&quot;&quot;) seg.int=function(i){ color=&quot;light blue&quot;; if((mu&lt;ICs[1,i]) | (mu&gt;ICs[2,i])){color = &quot;red&quot;} segments(ICs[1,i],i,ICs[2,i],i,col=color,lwd=2) } sapply(1:200,FUN=seg.int) abline(v=mu,lwd=2) Figura 4.1: Aciertos y errores en 200 Intervalos de confianza al 95% Fijaos en que los errores no se distribuyen por igual a los dos lados, hay muchos más intervalos que dejan la media poblacional a su izquierda que a su derecha, mientras que, en teoría, tendríamos que esperar que en la mitad de los errores la media poblacional estuviera a la izquierda del intervalo calculado y en la otra mitad a la derecha. Cosas de la aleatoriedad. 4.2 Intervalos de confianza para la proporción poblacional En esta sección consideramos el caso en que la población objeto de estudio sigue una distribución Bernoulli y queremos estimar su probabilidad de éxito (o proporción poblacional) \\(p\\). Para ello, tomamos una muestra aleatoria simple de tamaño \\(n\\) y número de éxitos \\(x\\), y, por lo tanto, de proporción muestral de éxitos \\(\\widehat{p}_X=x/n\\). El método “exacto” de Clopper-Pearson para calcular un intervalo de confianza del \\(q\\times 100\\%\\) para \\(p\\) se basa en el hecho de que, en estas condiciones, el valor de \\(x\\) sigue una distribución binomial \\(B(n,p)\\). Este método se puede usar siempre, sin ninguna restricción sobre la muestra, y consiste básicamente en encontrar los valores \\(p_0\\) y \\(p_1\\) tales que \\[ \\begin{array}{l} \\displaystyle \\sum_{k=x}^n\\binom{n}{k}p_0^k(1-p_0)^{n-k}=(1-q)/2\\\\ \\displaystyle\\sum_{k=0}^x\\binom{n}{k}p_1^k(1-p_1)^{n-k}=(1-q)/2 \\end{array} \\] y dar el intervalo \\([p_0,p_1]\\). Para calcular este intervalo se puede usar la función binom.exact del paquete epitools. Su sintaxis es binom.exact(x,n,conf.level) donde x y n representan, respectivamente, el número de éxitos y el tamaño de la muestra, y conf.level es \\(q\\), el nivel de confianza en tanto por uno. El valor por defecto de conf.level es 0.95. Ejemplo 4.3 Supongamos que, de una muestra de 15 enfermos tratados con un cierto medicamento, solo 1 ha desarrollado taquicardia. Queremos conocer un intervalo de confianza del 95% para la proporción de enfermos tratados con este medicamento que presentan este efecto adverso. Tenemos una población Bernoulli, formada por los enfermos tratados con el medicamento en cuestión, donde los éxitos son los enfermos que desarrollan taquicardia. La fracción de éstos es la fracción poblacional \\(p\\) para la que queremos calcular el intervalo de confianza del 95%. Para ello cargamos el paquete epitools y usamos binom.exact: library(epitools) binom.exact(1,15) ## x n proportion lower upper conf.level ## 1 1 15 0.06666667 0.00168643 0.3194846 0.95 El resultado de la función binom.exact es un data frame; el intervalo de confianza deseado está formado por los números en las columnas lower (extremo inferior) y upper (extremo superior): binom.exact(1,15)$lower ## [1] 0.00168643 binom.exact(1,15)$upper ## [1] 0.3194846 Hemos obtenido el intervalo de confianza [0.002,0.319]: podemos afirmar con un nivel de confianza del 95% que el porcentaje de enfermos tratados con este medicamento que presentan este efecto adverso está entre el 0.2% y el 31.9%. Supongamos ahora que el tamaño \\(n\\) de la muestra aleatoria simple es grande; de nuevo, digamos que \\(n\\geq 40\\). En esta situación, podemos usar el Método de Wilson para aproximar, a partir del Teorema Central del Límite, un intervalo de confianza para el parámetro \\(p\\) al nivel de confianza \\(q\\times 100\\%\\), mediante la fórmula \\[ \\frac{\\widehat{p}_{X}+\\frac{z_{(1+q)/2}^2}{2n}\\pm z_{(1+q)/2}\\sqrt{\\frac{\\widehat{p}_{X}(1-\\widehat{p}_{X})}{n}+\\frac{z_{(1+q)/2}^2}{4n^2}}}{1+\\frac{z_{(1+q)/2}^2}{n}} \\] donde \\(z_{(1+q)/2}\\) es el cuantil de orden \\((1+q)/2\\) de una variable aleatoria normal estándar. Para calcular este intervalo se puede usar la función binom.wilson del paquete epitools. Su sintaxis es binom.wilson(x,n,conf.level) con los mismos parámetros que binom.exact. Ejemplo 4.4 Supongamos que tratamos 45 ratones con un agente químico, y 10 de ellos desarrollan un determinado cáncer de piel. Queremos calcular un intervalo de confianza al 90% para la proporción \\(p\\) de ratones que desarrollan este cáncer de piel al ser tratados con este agente químico. Como 45 es relativamente grande, usaremos el método de Wilson. Para comparar los resultados, usaremos también el método exacto. Fijaos que, en este ejemplo, \\(q=0.9\\). binom.wilson(10,45,0.9) ## x n proportion lower upper conf.level ## 1 10 45 0.2222222 0.1377238 0.3382281 0.9 binom.exact(10,45,0.9) ## x n proportion lower upper conf.level ## 1 10 45 0.2222222 0.1258186 0.3477285 0.9 Con el método de Wilson obtenemos el intervalo [0.138,0.338] y con el método de Clopper-Pearson, el intervalo [0.126,0.348], un poco más ancho: hay una diferencia en los extremos de alrededor de un punto porcentual. Supongamos finalmente que la muestra aleatoria simple es considerablemente más grande que la usada en el método de Wilson y que, además, la proporción muestral de éxitos \\(\\widehat{p}_{X}\\) está alejada de 0 y de 1. Una posible manera de formalizar estas condiciones es requerir que \\(n\\geq 100\\) y que \\(n\\widehat{p}_{X}\\geq 10\\) y \\(n(1-\\widehat{p}_{X})\\geq 10\\); observad que estas dos últimas condiciones son equivalentes a que tanto el número de éxitos como el número de fracasos en la muestra sean como mínimo 10. En este caso, se puede usar la fórmula de Laplace, que simplifica la de Wilson (aunque, en realidad, la precede en más de 100 años) suponiendo que, si n es grande y \\(\\widehat{p}_{X}(1-\\widehat{p}_{X})/n\\) no es muy cercano a 0, podemos igualar \\(z_{(1+q)/2}^2/(2n)\\) a 0: un intervalo de confianza del parámetro \\(p\\) al nivel de confianza \\(q\\times 100\\%\\) viene dado aproximadamente por la fórmula \\[ \\widehat{p}_{X}\\pm z_{(1+q)/2}\\sqrt{\\frac{\\widehat{p}_{X} (1-\\widehat{p}_{X})}{n}} \\] Esta fórmula está implementada en la función binom.approx del paquete epitools, de uso similar al de las dos funciones anteriores. Ejemplo 4.5 En una muestra aleatoria de 500 familias con niños en edad escolar de una determinada ciudad se ha observado que 340 introducen fruta de forma diaria en la dieta de sus hijos. A partir de este dato, queremos encontrar un intervalo de confianza del 95% para la proporción real de familias de esta ciudad con niños en edad escolar que incorporan fruta fresca de forma diaria en la dieta de sus hijos. Tenemos una población Bernoulli donde los éxitos son las familias que aportan fruta de forma diaria a la dieta de sus hijos, y la fracción de estas familias en el total de la población es la proporción poblacional \\(p\\) para la que queremos calcular el intervalo de confianza. Como \\(n\\) es muy grande y los números de éxitos y fracasos también lo son, podemos emplear el método de Laplace. binom.approx(340,500) ## x n proportion lower upper conf.level ## 1 340 500 0.68 0.6391123 0.7208877 0.95 Por lo tanto, según la fórmula de Laplace, un intervalo de confianza al 95% para la proporción poblacional es [0.639,0.721]. ¿Qué hubiéramos obtenido con los otros dos métodos? binom.wilson(340,500) ## x n proportion lower upper conf.level ## 1 340 500 0.68 0.637873 0.7193822 0.95 binom.exact(340,500) ## x n proportion lower upper conf.level ## 1 340 500 0.68 0.6371369 0.7207188 0.95 Como podéis ver, los resultados son muy parecidos, con diferencias de unas pocas milésimas. 4.3 Intervalo de confianza para la varianza de una población normal Supongamos ahora que queremos estimar la varianza \\(\\sigma^2\\), o la desviación típica \\(\\sigma\\), de una población que sigue una distribución normal. Tomamos una muestra aleatoria simple de tamaño \\(n\\), y sea \\(\\widetilde{S}_{X}\\) su desviación típica muestral. En esta situación, un intervalo de confianza del \\(q\\times 100\\%\\) para la varianza \\(\\sigma^2\\) es \\[ \\left[ \\frac{(n-1)\\widetilde{S}_{X}^2}{\\chi_{n-1,(1+q)/2}^2},\\ \\frac{(n-1)\\widetilde{S}_{X}^2}{\\chi_{n-1,(1-q)/2}^2}\\right], \\] donde \\(\\chi_{n-1,(1-q)/2}^2\\) y \\(\\chi_{n-1,(1+q)/2}^2\\) son, respectivamente, los cuantiles de orden \\((1-q)/2\\) y \\((1+q)/2\\) de una variable aleatoria que sigue una distribución \\(\\chi^2\\) con \\(n-1\\) grados de libertad. Si conocéis la varianza muestral \\(\\widetilde{S}_{X}^2\\), que denotaremos por varm, podéis calcular este intervalo de confianza para la varianza con la fórmula siguiente (donde qindica el nivel de confianza en tanto por uno \\(q\\)): (n-1)*varm*c(1/qchisq((1+q)/2,n-1),1/qchisq((1-q)/2,n-1)) Si, en cambio, disponéis de la muestra, podéis calcular este intervalo de confianza con la función varTest del paquete EnvStats. La sintaxis es similar a la usada con t.test: varTest(X,conf.level)$conf.int donde X es el vector que contiene la muestra y conf.level el nivel de confianza, que por defecto es igual a 0.95. Ejemplo 4.6 Un índice de calidad de un reactivo químico es el tiempo que tarda en actuar. Se supone que la distribución de este tiempo de actuación del reactivo es aproximadamente normal. Se realizaron 30 pruebas independientes, que forman una muestra aleatoria simple, en las que se midió el tiempo de actuación del reactivo. Los tiempos obtenidos fueron reactivo = c(12,13,13,14,14,14,15,15,16,17,17,18,18,19,19,25,25,26,27,30,33,34,35, 40,40,51,51,58,59,83) Queremos usar estos datos para calcular un intervalo de confianza del 95% para la desviación típica de este tiempo de actuación. El siguiente código calcula un intervalo de confianza al 95% para la varianza a partir del vector reactivo: library(EnvStats) varTest(reactivo)$conf.int ## LCL UCL ## 191.2627 544.9572 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Este intervalo de confianza es para la varianza. Como la desviación típica es la raíz cuadrada de la varianza, para obtener un intervalo de confianza al 95% para la desviación típica, tenemos que tomar la raíz cuadrada de este intervalo para la varianza: sqrt(varTest(reactivo)$conf.int) ## LCL UCL ## 13.82977 23.34432 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Por lo tanto un intervalo de confianza del 95% para la desviación típica poblacional es [13.83, 23.34]. De nuevo, si os molesta, podéis eliminar el atributo conf.level igualándolo a NULL. 4.4 Bootstrap Cuando no tiene sentido usar un método paramétrico como los explicados en las secciones anteriores para calcular un intervalo de confianza porque no se satisfacen las condiciones teóricas que garantizan que el intervalo obtenido contiene el 95% de las veces el parámetro poblacional deseado, podemos recurrir a un método no paramétrico. El más utilizado es el bootstrap, que básicamente consiste en: Remuestrear la muestra: tomar muchas muestras aleatorias simples de la muestra de la que disponemos, cada una de ellas del mismo tamaño que la muestra original (pero simples, es decir, con reposición). Calcular el estimador sobre cada una de estas submuestras. Organizar los resultados en un vector. Usar este vector para calcular un intervalo de confianza. La manera más sencilla de llevar a cabo el cálculo final del intervalo de confianza es el llamado método de los percentiles, en el que se toman como extremos del intervalo de confianza del \\(q\\times 100\\%\\) los cuantiles de orden \\((1-q)/2\\) y \\((1+q)/2\\) del vector de estimadores, pero hay muchos otros métodos; encontraréis algunos en esta entrada de la Wikipedia. Ejemplo 4.7 Volvamos a la muestra de pesos del Ejemplo 4.1, pero supongamos ahora que la variable aleatoria poblacional de la que la hemos extraído no es normal (o que no queremos suponer que lo sea). Vamos a usar el método bootstrap de los percentiles para calcular un intervalo de confianza del 95% para el peso medio poblacional. Para ello, vamos a general 1000 muestras aleatorias simples de la muestra, todas ellas del mismo tamaño que la muestra, calcularemos la media de cada una de estas muestras simples, construiremos un vector con estas medias muestrales, y daremos como extremos del intervalo de confianza los cuantiles de orden 0.025 y 0.975 del vector así obtenido. set.seed(42) n=length(pesos) X=replicate(1000,mean(sample(pesos,n,replace=TRUE))) IC.boot=c(quantile(X,0.025),quantile(X,0.975)) round(IC.boot,1) ## 2.5% 97.5% ## 3027.3 3350.6 El intervalo obtenido en este caso es [3027.3, 3350.6]; como se trata de un método basado en una simulación aleatoria, seguramente con otra semilla de aleatoriedad daría un intervalo diferente. Para comparar, recordad que el intervalo de confianza obtenido con la fórmula basada en la t de Student ha sido [2997.8, 3355]. El paquete boot dispone de la función boot para llevar a cabo simulaciones bootstrap. Aplicando luego la función boot.ci al resultado de la función boot obtenemos diversos intervalos de confianza basados en el enfoque bootstrap. La sintaxis básica de la función boot es boot(X,estadístico,R) donde: X es el vector que forma la muestra de la que disponemos R es el número de muestras que queremos extraer de la muestra original El estadístico es la función que calcula el estadístico deseado de la submuestra, y tiene que tener dos parámetros: el primero representa la muestra original X y el segundo representa el vector de índices de una m.a.s. de X. Por ejemplo, si vamos a usar la función boot para efectuar una simulación bootstrap de medias muestrales, podemos tomar como estadístico la función: media.boot=function(X,índices){mean(X[índices])} Por otro lado, el nivel de confianza se especifica en la función boot.ci mediante el parámetro conf (no conf.level, como hasta ahora), cuyo valor por defecto es, eso sí, el de siempre: 0.95. A modo de ejemplo, vamos a usar las funciones del paquete boot para calcular un intervalo de confianza del 95% para la media de la variable aleatoria que ha producido el vector pesos. library(boot) set.seed(42) simulacion=boot(pesos,media.boot,1000) El resultado simulacion de esta última instrucción es una list que incluye, en su componente t, el vector de 1000 medias muestrales obtenido mediante la simulación; sus 10 primeros valores son: simulacion$t[1:10] ## [1] 3189.14 3237.11 3301.32 3103.00 3154.11 3294.11 3347.25 3239.93 ## [9] 3304.07 3217.11 Calculemos ahora el intervalo de confianza deseado: boot.ci(simulacion) ## Warning in boot.ci(simulacion): bootstrap variances needed for studentized ## intervals ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = simulacion) ## ## Intervals : ## Level Normal Basic ## 95% (3010, 3341 ) (3011, 3345 ) ## ## Level Percentile BCa ## 95% (3008, 3342 ) (3009, 3344 ) ## Calculations and Intervals on Original Scale Obtenemos cuatro intervalos de confianza para \\(\\mu\\), calculados con cuatro métodos a partir de la simulación realizada (y un aviso de que no ha podido calcular un quinto intervalo). Cada uno de estos intervalos es un objeto de una list y por lo tanto se puede obtener con el sufijo adecuado, que podréis deducir del resultado de aplicar str al resultado de una función boot.ci. El intervalo Percentile es el calculado con el método de los percentiles que hemos explicado antes, y se obtiene con el sufijo $percent[4:5] (no ha dado lo mismo que antes, pese a usar la misma semilla de aleatoriedad, porque el procedimiento usado por la función boot.ci par calcular el intervalo de confianza no es exactamente el que hemos explicado). No vamos a entrar en detalle sobre los métodos que usa para calcular el resto de intervalos, en realidad todos tienen ventajas e inconvenientes. Ejemplo 4.8 ¿Realmente funciona el enfoque bootstrap? Vamos a retomar el experimento realizado en el Ejemplo 4.2, donde construimos una matriz M cuyas columnas son 200 muestras aleatorias simples de tamaño 50 de una población que sigue una distribución normal estándard. En dicho ejemplo calculamos para cada una de estas muestras el intervalo de confianza del 95% para la media poblacional usando la fórmula (4.1), que es la recomendada por la teoría en este caso. De los 200 intervalos calculados, 189 contuvieron la media poblacional, lo que representa un 94.5% de aciertos. Ahora vamos a calcular para cada una de estas muestras un intervalo de confianza del 95% por el método bootstrap de los percentiles y compararemos las tasas de aciertos. Para mayor seguridad, vamos a volver a generar en las mismas condiciones las 200 muestras de la población, no sea que a lo largo de la lección hayamos modificado inadvertidamente el contenido de la matriz M (y así de paso fijamos la semilla de aleatoriedad). set.seed(42) Poblacion=rnorm(10^7) #La población mu=mean(Poblacion) # La media poblacional M=replicate(200, sample(Poblacion,50,replace=TRUE)) # Las muestras IC.b=function(X){boot.ci(boot(X,media.boot,1000))$percent[4:5]} ICs.bootstrap=apply(M,FUN=IC.b,MARGIN=2) Aciertos.bootstrap=length(which((mu&gt;=ICs.bootstrap[1,]) &amp; (mu&lt;=ICs.bootstrap[2,]))) Aciertos.bootstrap ## [1] 186 Con el bootstrap, hemos acertado en 186 ocasiones, lo que supone un 93% de aciertos. 4.5 Guía rápida t.test(X, conf.level=...)$conf.int calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la media poblacional usando la fórmula basada en la t de Student aplicada a la muestra X. binom.exact(x,n,conf.level=...), del paquete epitools, calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la proporción poblacional aplicando el método de Clopper-Pearson a una muestra de tamaño n con x éxitos. binom.wilson(x,n,conf.level=...), del paquete epitools, calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la proporción poblacional aplicando el método de Wilson a una muestra de tamaño n con x éxitos. binom.approx(x,n,conf.level=...), del paquete epitools, calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la proporción poblacional aplicando la fórmula de Laplace a una muestra de tamaño n con x éxitos. varTest(X,conf.level=...)$conf.int, del paquete EnvStats, calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la varianza poblacional usando la fórmula basada en la ji cuadrado aplicada a la muestra X. boot(X,E,R), del paquete boot, lleva a cabo una simulación bootstrap, tomando R submuestras del vector X y calculando sobre ellas el estadístico representado por la función E. boot.ci, del paquete boot, aplicado al resultado de una función boot, calcula diversos intervalos de confianza a partir del resultado de la simulación efectuada con boot. El nivel de confianza se especifica con el parámetro conf. 4.6 Ejercicios Test (1) Tomad la muestra de todas las longitudes de pétalos de flores Iris setosa contenida en la tabla de datos iris y usadla para calcular un intervalo de confianza del 95% para el valor medio de las longitudes de pétalos de esta especie de flores usando la fórmula basada en la t de Student. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 2 cifras decimales (sin ceros innecesarios a la derecha). (2) Tenemos una población de media \\(\\mu\\) desconocida. Tomamos una muestra aleatoria simple de tamaño 80 y obtenemos una media muestral de 6.2 y una desviación típica muestral de 1.2. Usad estos datos y la fórmula del intervalo de confianza para la media basado en la t de Student para calcular un intervalo de confianza al 95% para \\(\\mu\\). Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 2 cifras decimales (sin ceros innecesarios a la derecha). (3) Tenemos una población Bernoulli de proporción poblacional \\(p\\) desconocida. Tomamos una muestra aleatoria simple de 80 individuos y obtenemos una proporción muestral de 35% de éxitos. Calculad un intervalo de confianza para \\(p\\) a un nivel de confianza del 95% usando el método de Wilson. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 3 cifras decimales (sin ceros innecesarios a la derecha). (4) Tomad la muestra de todas las longitudes de pétalos de flores Iris setosa contenida en la tabla de datos iris y usadla para calcular un intervalo de confianza del 95% para la varianza de las longitudes de pétalos de esta especie de flores. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 3 cifras decimales (sin ceros innecesarios a la derecha). Problemas (1) En la tabla de datos que encontraréis en el url https://www.dropbox.com/s/qcok2f7u51teqr6/pacientes.txt?raw=1 se ha recogido información acerca de un conjunto de pacientes que se han sometido a una revisión médica voluntaria. Sus variables son: SEXO: el sexo EDAD: la edad, en años ALTURA: la altura, en cm PESO: el peso, en Kg HIPERT: si es hipertenso o no IMC: el índice de masa corporal CARDIOPA: si sufre alguna cardiopatía o no A partir de estos datos: (a) Calculad un intervalo de confianza del 95% para el IMC medio de las mujeres usando la fórmula basada en la t de Student. (b) Calculad un intervalo de confianza del 95% para la desviación típica del IMC de las mujeres usando la fórmula basada en la \\(\\chi^2\\). (c) Calculad un intervalo de confianza del 95% para la proporción de hombres hipertensos que sufren cardiopatías. Usad todos los métodos que tenga sentido usar y comparad los resultados. (d) Calculad un intervalo de confianza del 95% para la desviación típica del IMC de las mujeres de 50 años o más usando la técnica bootstrap de los percentiles y 1000 muestras. Respuestas al test (1) 1.41,1.51 Nosotros lo hemos calculado con round(t.test(iris[iris$Species==&quot;setosa&quot;,&quot;Petal.Length&quot;])$conf.int,2) ## [1] 1.41 1.51 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 (2) 5.933,6.467 Nosotros lo hemos calculado con x=6.2 n=80 sdm=1.2 conf.level=0.95 round(x+(qt((1+conf.level)/2,n-1)*sdm/sqrt(n))*c(-1,1),3) ## [1] 5.933 6.467 (3) 0.255,0.459 Nosotros lo hemos calculado con round(binom.wilson(0.35*80,80),3) ## x n proportion lower upper conf.level ## 1 28 80 0.35 0.255 0.459 0.95 (4) 0.021,0.047 Nosotros lo hemos calculado con round(varTest(iris[iris$Species==&quot;setosa&quot;,&quot;Petal.Length&quot;])$conf.int,3) ## LCL UCL ## 0.021 0.047 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Soluciones sucintas de los problemas (1) Cargamos la tabla de datos: Datos=read.table(&quot;https://www.dropbox.com/s/qcok2f7u51teqr6/pacientes.txt?raw=1&quot;, header=TRUE) str(Datos) ## &#39;data.frame&#39;:\t81 obs. of 7 variables: ## $ SEXO : Factor w/ 2 levels &quot;Hombre&quot;,&quot;Mujer&quot;: 1 1 2 1 1 2 1 1 1 1 ... ## $ EDAD : int 47 51 54 50 52 53 49 52 50 48 ... ## $ ALTURA : int 170 170 148 170 165 151 170 168 170 166 ... ## $ PESO : int 80 73 46 74 67 51 73 72 73 73 ... ## $ HIPERT : Factor w/ 2 levels &quot;Hipertenso&quot;,&quot;Normotenso&quot;: 2 1 1 1 2 1 1 2 2 2 ... ## $ IMC : num 27.7 25.3 21 25.6 24.6 ... ## $ CARDIOPA: Factor w/ 2 levels &quot;Con cardiopatia&quot;,..: 2 1 2 2 2 2 2 2 1 1 ... (a) t.test(Datos[Datos$SEXO==&quot;Mujer&quot;,&quot;IMC&quot;])$conf.int ## [1] 22.3313 22.9053 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 (b) sqrt(varTest(Datos[Datos$SEXO==&quot;Mujer&quot;,&quot;IMC&quot;])$conf.int) ## LCL UCL ## 0.687995 1.106481 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 (c) Veamos el tamaño de la muestra HH=Datos[Datos$SEXO==&quot;Hombre&quot; &amp; Datos$CARDIOPA==&quot;Con cardiopatia&quot;, &quot;HIPERT&quot;] length(HH) ## [1] 12 table(HH) ## HH ## Hipertenso Normotenso ## 4 8 Con un tamaño tan pequeño, solo tiene sentido calcular el intervalo de Clopper-Pearson: binom.exact(table(HH)[1],length(HH)) ## x n proportion lower upper conf.level ## Hipertenso 4 12 0.333333 0.0992461 0.651124 0.95 (d) “A mano” Muestra=Datos[Datos$SEXO==&quot;Mujer&quot; &amp; Datos$EDAD&gt;=50, &quot;IMC&quot;] n=length(Muestra) X=replicate(1000,sd(sample(Muestra,n,replace=TRUE))) c(quantile(X,0.025),quantile(X,0.975)) ## 2.5% 97.5% ## 0.618456 0.996212 Usando las funciones del paquete boot: sd.boot=function(X,índices){sd(X[índices])} simulacion=boot(Muestra,sd.boot,1000) boot.ci(simulacion)$percent[4:5] ## [1] 0.6252360 0.9977014 "],
["chap-contrastes.html", "Lección 5 Contrastes de hipótesis 5.1 Contrastes para medias 5.2 Contrastes para varianzas 5.3 Contrastes para proporciones 5.4 Cálculo de la potencia de un contraste 5.5 Guía rápida 5.6 Ejercicios", " Lección 5 Contrastes de hipótesis En esta lección explicamos algunas instrucciones de R que permiten llevar a cabo contrastes de hipótesis sobre parámetros poblacionales. Antes de empezar, repasemos el vocabulario básico relacionado con los contrastes de hipótesis: Hipótesis alternativa, \\(H_1\\): Aquella de la que buscamos evidencia en nuestro estudio. Hipótesis nula, \\(H_0\\): La hipótesis que estamos dispuestos a aceptar si no encontramos evidencia suficiente de la hipótesis alternativa. Suele plantearse en términos de “no hay diferencia”. Contraste bilateral: Contraste en el que la hipótesis alternativa viene definida por un \\(\\neq\\): que un parámetro sea diferente de un valor dado, que un parámetro sobre una población sea diferente del mismo parámetro sobre otra población, … Contraste unilateral: Contraste en el que la hipótesis alternativa viene definida por un \\(&gt;\\) o un \\(&lt;\\): que un parámetro sea mayor que un valor dado, que un parámetro sobre una población sea menor que el mismo parámetro sobre otra población, … Error de tipo I, o Falso positivo: Concluir que la hipótesis alternativa es verdadera cuando en realidad es falsa. Nivel de significación, \\(\\alpha\\): La probabilidad de cometer un error de tipo I. Nivel de confianza, \\(1-\\alpha\\): La probabilidad de no cometer un error de tipo I. Error de tipo II, o Falso negativo: Concluir que la hipótesis alternativa es falsa cuando en realidad es verdadera. Potencia, \\(1-\\beta\\): La probabilidad de no cometer un error de tipo II. Estadístico de contraste: El valor que se calcula a partir de la muestra obtenida en el estudio y que se usará para tomar la decisión en el contraste planteado. p-valor: La probabilidad de que, si la hipótesis nula es verdadera, el estadístico de contraste tome un valor tan o más extremo en el sentido de la hipótesis alternativa que el obtenido en el estudio Intervalo de confianza al nivel de confianza \\(1-\\alpha\\): Un intervalo en el que el parámetro poblacional que contrastamos tiene probabilidad \\(1-\\alpha\\) de pertenecer en el sentido de los intervalos de confianza de la Lección 4: es decir, porque se ha obtenido con un procedimiento que produce intervalos que, en el \\((1-\\alpha)\\times 100\\%\\) de las ocasiones que lo aplicamos a muestras aleatorias simples, contiene el parámetro poblacional. Está formado por los valores del parámetro poblacional que, si fueran los que contrastáramos en nuestro contraste, producirían un p-valor como mínimo \\(\\alpha\\). Los intervalos de confianza de los contrastes bilaterales coinciden con los definidos en la Lección 4. Regla de rechazo: Rechazamos la hipótesis nula en favor de la alternativa con un nivel de significación \\(\\alpha\\) dado cuando se da alguna de las dos condiciones siguientes (que son equivalentes, es decir, se dan las dos o ninguna): El p-valor és menor que el nivel de significación. El valor contrastado del parámetro poblacional no pertenece al intervalo de confianza del nivel \\(1-\\alpha\\). Cuando no rechazamos la hipótesis nula en favor de la alternativa, aceptamos la hioótesis nula. Ejemplo 5.1 Tenemos una moneda y creemos que está trucada a favor de Cara, es decir, que al lanzarla al aire produce más caras que cruces. Para intentar decidir si esto es cierto o no, lanzamos la moneda al aire 20 veces y obtenemos 15 caras. Llamemos \\(p\\) a la probabilidad de obtener cara al lanzar al aire esta moneda. Entonces: La hipótesis alternativa, de la que buscamos evidencia, es que la moneda está trucada a favor de cara: \\(H_1: p&gt;0.5\\). La hipótesis nula, que aceptaremos por defecto si no encontramos evidencia de que la alternativa sea verdadera, es que la moneda no está trucada: \\(H_0: p=0.5\\). Cometeríamos un error de tipo I si la moneda fuera honrada y nosotros concluyéramos que está trucada. Cometeríamos un error de tipo II si la moneda estuviera trucada y nosotros concluyéramos que no lo está. El estadístico de contraste en este experimento es simplemente el número de caras en un serie de lanzamientos de la moneda. El p-valor es la probabilidad de obtener 15 o más caras al lanzar al aire 20 veces la moneda si fuera verdad que \\(p=0.5\\). Como en este caso el número de caras seguiría una distribución binomial \\(B(20,0.5)\\), podemos calcular fácilmente esta probabilidad: 1-pbinom(14,20,0.5) ## [1] 0.0206947 Por lo tanto, el p-valor es 0.021. El intervalo de confianza del 95% de este contraste está formado por los valores de \\(p\\) para los que la probabilidad de obtener 15 o más caras al lanzar al aire 20 veces la moneda es mayor o igual que el 5%, y es [0.54442, 1]. En efecto, fijaos que 1-pbinom(14,20,0.54441) ## [1] 0.049993 1-pbinom(14,20,0.54442) ## [1] 0.0500022 Y bueno, tras todo este vocabulario, ¿cuál sería la conclusión? El p-valor obtenido significa que si la moneda no estuviera trucada, la probabilidad de obtener el número de caras que hemos obtenido o más es muy pequeña, lo que hace difícil de creer que la moneda no esté trucada. En particular, si trabajamos con un nivel de significación del 5%, como el p-valor es más pequeño que 0.05, rechazamos la hipótesis nula. Equivalentemente, como el intervalo de confianza del 95% para la \\(p\\) está completamente a la derecha del valor que contrastamos, 0.5, con este nivel de confianza hemos de concluir que \\(p&gt;0.5\\). En resumen, aceptando una probabilidad de error de tipo I (de concluir que una moneda honrada está trucada) del 5%, rechazamos la hipótesis nula e inferimos que la moneda está trucada a favor de cara. Figura 5.1: “Hipótesis Nula” (http://imgs.xkcd.com/comics/null_hypothesis.png (CC-BY-NC 2.5)) 5.1 Contrastes para medias El test t El test t para contrastar una o dos medias basado en la t de Student está implementado en la función t.test. Este test usa diferentes estadísticos según que el contraste sea de una media o de dos; en este último caso, según se usen muestras emparejadas o independientes; y en este último caso, según las poblaciones tengan varianzas iguales o diferentes. Aunque este test solo es exacto (en el sentido de que da la conclusión con el nivel de significación requerido) cuando las poblaciones involucradas siguen distribuciones normales, el Teorema Central del Límite garantiza que también da resultados aproximadamente correctos cuando las muestras son grandes, aunque las poblaciones no sean normales, por lo que en esta situación también se recomienda su uso. Así pues, aunque en la práctica el test t se use como test “de talla única” para contrastar una o dos medias en cualquier situación, hay que tener claro que su resultado es fiable tan solo: cuando las variables poblacionales involucradas son (aproximadamente) normales, o cuando todas las muestras usadas son grandes. Al final de esta sección explicamos las funciones asociadas a algunos contrastes no paramétricos que pueden usarse cuando estas condiciones no se cumplen. La sintaxis básica de la función t.test es t.test(x, y, mu=..., alternative=..., conf.level=..., paired=..., var.equal=...) donde: x es el vector de datos que forma la muestra que analizamos. y es un vector opcional; si lo entramos, R entiende que estamos realizando un contraste de dos medias, con hipótesis nula la igualdad de estas medias. Podemos sustituir los vectores x e y por una fórmula variable1~variable2 que indique que separamos la variable numérica variable1 en dos vectores definidos por los niveles de un factor variable2 de dos niveles (o de otra variable asimilable a un factor de dos niveles, como por ejemplo una variable numérica que solo tome dos valores diferentes). Con esta construcción, R tomará estos vectores en el orden natural de los niveles de variable2: x será el vector correspondiente al primer nivel e y el correspondiente al segundo. Hay que tener esto en cuenta a la hora de especificar la hipótesis alternativa si es unilateral. Si las dos variables de la fórmula son columnas de un dataframe, se puede usar el parámetro data=... para indicarlo. Solamente tenemos que especificar el parámetro mu si hemos entrado una sola muestra, y en este caso lo hemos de igualar al valor \\(\\mu_0\\) que queremos contrastar, de manera que la hipótesis nula será \\(H_0: \\mu=\\mu_0\\). El parámetro alternative puede tomar tres valores: \"two.sided\", para contrastes bilaterales, y \"less\" y \"greater\", para contrastes unilaterales. En esta función, y en todas las que explicamos en esta lección, su valor por defecto, que no hace falta especificar, es \"two.sided\". El significado de estos valores depende del tipo de test que efectuemos: Si el test es de una sola muestra, \"two.sided\" representa la hipótesis alternativa \\(H_1: \\mu\\neq \\mu_0\\), \"less\" corresponde a \\(H_1: \\mu&lt; \\mu_0\\), y \"greater\" corresponde a \\(H_1: \\mu&gt; \\mu_0\\). Si hemos entrado dos muestras y llamamos \\(\\mu_x\\) y \\(\\mu_y\\) a las medias de las poblaciones de las que hemos extraído las muestras \\(x\\) e \\(y\\), respectivamente, entonces \"two.sided\" representa la hipótesis alternativa \\(H_1: \\mu_x \\neq \\mu_y\\); \"less\" indica que la hipótesis alternativa es \\(H_1: \\mu_x&lt; \\mu_y\\); y \"greater\", que la hipótesis alternativa es \\(H_1: \\mu_x&gt; \\mu_y\\). El valor del parámetro conf.level es el nivel de confianza \\(1-\\alpha\\). En esta función, y en todas las que explicamos en esta lección, su valor por defecto, que no es necesario especificar, es 0.95, que corresponde a un nivel de confianza del 95%, es decir, a un nivel de significación \\(\\alpha=0.05\\). El parámetro paired solo lo tenemos que especificar si llevamos a cabo un contraste de dos medias. En este caso, con paired=TRUE indicamos que las muestras son emparejadas, y con paired=FALSE (que es su valor por defecto) que son independientes. Si se trata de muestras emparejadas, los vectores x e y tienen que tener la misma longitud, naturalmente. El parámetro var.equal solo lo tenemos que especificar si llevamos a cabo un contraste de dos medias usando muestras independientes, y en este caso sirve para indicar si queremos considerar las dos varianzas poblacionales iguales (igualándolo a TRUE) o diferentes (igualándolo a FALSE, que es su valor por defecto). La función t.test tiene otro parámetro que queremos destacar, que es común a la mayoría de las funciones de estadística inferencial y análisis de datos. Se trata del parámetro na.action, que sirve para especificar qué queremos hacer con los valores NA. Su valor por defecto es na.omit, que elimina las entradas NA de los vectores (o los pares que contengan algún NA, en el caso de muestras emparejadas). Por ahora, esta opción por defecto es la adecuada, por lo que no hace falta usar este parámetro, pero conviene saber que hay alternativas. Las más útiles son: na.fail, que hace que la ejecución pare si hay algún NA en los vectores, y na.pass, que no hace nada con los NA y permite que las operaciones internas de la función sigan su curso y los manejen como les corresponda. Veamos varios ejemplos de uso de esta función. Ejemplo 5.2 Consideremos el siguiente vector de longitud 25: x=c(2.2,2.66,2.74,3.41,2.46,2.96,3.34,2.16,2.46,2.71,2.04,3.74,3.24,3.92,2.38,2.82,2.2, 2.42,2.82,2.84,4.22,3.64,1.77,3.44,1.53) Supongamos que esta muestra ha sido extraída de una población normal. Postulamos que el valor medio \\(\\mu\\) de la población no es 2. Para confirmarlo, vamos realizar el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu=2\\\\ H_{1}:\\mu\\neq 2 \\end{array}\\right. \\] con nivel de significación \\(\\alpha=0.05\\): t.test(x, mu=2, alternative=&quot;two.sided&quot;, conf.level=0.95) ## ## One Sample t-test ## ## data: x ## t = 5.912, df = 24, p-value = 4.23e-06 ## alternative hypothesis: true mean is not equal to 2 ## 95 percent confidence interval: ## 2.52384 3.08576 ## sample estimates: ## mean of x ## 2.8048 (Como los parámetros alternative=\"two.sided\" y conf.level=0.95 eran los que toma R por defecto, en realidad no hacía falta especificarlos.) Observad la información que obtenemos con esta instrucción: Información sobre la muestra \\(x\\): su media muestral (mean of x) \\(\\overline{x}\\), que vale 2.8048. La hipótesis alternativa (alternative hypothesis), en este caso true mean is not equal to 2: la media verdadera, o poblacional, \\(\\mu\\) es diferente de 2. El valor t que toma el estadístico de contraste, \\(T=\\frac{\\overline{X}-\\mu_0}{\\widetilde{S}_X/\\sqrt{n}}\\), sobre la muestra, en este caso 5.912, y los grados de libertad df (degrees of freedom) de su distribución t de Student cuando la hipótesis nula es verdadera, df =24. El p-valor (p-value) de nuestro test, en este caso p-value = 4.232e-06, es decir, \\(4.232\\times 10^{-6}\\). Un intervalo de confianza del \\((1-\\alpha)\\times 100\\%\\) (en nuestro caso, 95 percent confidence interval) para la \\(\\mu\\): en este ejemplo, [2.523844, 3.085756]. Lo único que no nos dice directamente es si tenemos que rechazar o no la hipótesis nula, pero esto lo deducimos del p-valor: como es más pequeño que el nivel de significación (de hecho, es muy pequeño), podemos rechazar la hipótesis nula, \\(\\mu=2\\), en favor de la alternativa, \\(\\mu \\neq 2\\). Es decir, hay evidencia estadísticamente significativa de que \\(\\mu \\neq 2\\). Otra manera de decidir si rechazamos o no la hipótesis nula es mirar si el valor que contrastamos pertenece al intervalo de confianza del contraste. Puesto que \\(2 \\notin [2.523844, 3.085756]\\), podemos rechazar la hipótesis nula en favor de la alternativa y concluir que \\(\\mu\\neq 2\\). Hagamos ahora el test cambiando la hipótesis alternativa por \\(H_{1}:\\mu&lt; 3\\), es decir, \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu=3\\\\ H_{1}:\\mu&lt; 3 \\end{array}\\right. \\] y tomando como nivel de significación \\(\\alpha=0.1\\): t.test(x, mu=3, alternative=&quot;less&quot;, conf.level=0.9) ## ## One Sample t-test ## ## data: x ## t = -1.434, df = 24, p-value = 0.0822 ## alternative hypothesis: true mean is less than 3 ## 90 percent confidence interval: ## -Inf 2.9842 ## sample estimates: ## mean of x ## 2.8048 En este caso, el p-valor es 0.082, por lo que podemos rechazar la hipótesis nula con un nivel de significación del 10% y concluir, con este nivel de significación (es decir, asumiendo esta probabilidad de equivocarnos), que \\(\\mu&lt;3\\); pero fijaos en que con un nivel de significación del 5% no podríamos rechazar la hipótesis nula. El intervalo de confianza del 90% es ahora \\((-\\infty,2.984]\\) (Inf representa \\(\\infty\\)). Que no contenga el 3 (aunque por muy poco) también indica que podemos rechazar la hipótesis nula \\(\\mu=3\\) en favor de la alternativa \\(\\mu&lt; 3\\) con este nivel de confianza. El p-valor y el intervalo de confianza se pueden obtener directamente, añadiendo a la instrucción t.test los sufijos $p.value o $conf.int, respectivamente. Esperamos que recordéis que en la lección anterior ya usábamos la construcción t.test(...)$conf.int para calcular un intervalo de confianza para la media usando la fórmula basada en la t de Student. Es lo correcto, puesto que el intervalo de confianza de un test t bilateral es el que explicamos entonces y no depende para nada de la hipótesis nula (por eso no especificábamos la \\(\\mu\\), y dejábamos que la función tomase su valor por defecto, 0). Pero ahora podemos calcular dos intervalos de confianza más, correspondientes a los dos tipos de contrastes unilaterales. En ellos toda la “probabilidad de equivocarnos” se concentra a un lado del intervalo de confianza, en lugar de repartirse por igual a ambos lados. t.test(x, mu=2)$p.value ## [1] 4.23159e-06 t.test(x, mu=2)$conf.int ## [1] 2.52384 3.08576 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 t.test(x, mu=2)$conf.int[1] ## [1] 2.52384 t.test(x, mu=2)$conf.int[2] ## [1] 3.08576 t.test(x, mu=3, alternative=&quot;less&quot;, conf.level=0.9)$conf.int ## [1] -Inf 2.9842 ## attr(,&quot;conf.level&quot;) ## [1] 0.9 Podéis consultar los sufijos necesarios para obtener las otras componentes del resultado en la Ayuda de la función. Ejemplo 5.3 Queremos contrastar si el valor medio del nivel de colesterol en una población es de 220 mg/dl o no, a un nivel de significación del 5%. Es decir, si llamamos \\(\\mu\\) a la media de la variable aleatoria “Nivel de colesterol de un individuo de esta población, en mg/dl”, queremos realizar el contraste bilateral \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu=220\\\\ H_{1}:\\mu \\neq 220 \\end{array}\\right. \\] Para ello, hemos tomado una muestra del nivel de colesterol en plasma de 9 individuos de la población. Los datos obtenidos, en mg/dl, son los siguientes: colesterol=c(203,229,215,220,223,233,208,228,209) Suponemos que el nivel de colesterol en plasma sigue una ley normal y que por lo tanto nos podemos fiar del resultado de un test t: t.test(colesterol, mu=220) ## ## One Sample t-test ## ## data: colesterol ## t = -0.3801, df = 8, p-value = 0.714 ## alternative hypothesis: true mean is not equal to 220 ## 95 percent confidence interval: ## 210.577 226.756 ## sample estimates: ## mean of x ## 218.667 El p-valor es 0.714, muy grande y en particular superior a 0.05, por lo tanto no podemos rechazar la hipótesis nula de que el valor medio sea 220 mg/dl. Además, el intervalo de confianza del 95% del contraste es [210.58, 226.76], y contiene holgadamente el valor 220. Más adelante en esta misma sección discutiremos qué podemos hacer si el nivel de colesterol en plasma no sigue una ley aproximadamente normal, en cuyo caso el resultado de este test t no sirve para nada. Ejemplo 5.4 Recordad el dataframe iris, que recoge datos de las flores de 50 ejemplares de cada una de tres especies de iris. str(iris) ## &#39;data.frame&#39;:\t150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Queremos estudiar si la longitud media \\(\\mu_v\\) de los sépalos de las Iris virginica es mayor que la longitud media \\(\\mu_s\\) de los sépalos de las Iris setosa usando las muestras contenidas en esta tabla de datos. Para ello realizamos el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu_s=\\mu_v\\\\ H_{1}:\\mu_s&lt; \\mu_v \\end{array}\\right. \\] En este caso, se trata de un contraste de dos muestras independientes, y como las muestras son grandes, podemos usar con garantías un test t. Ahora bien, recordad que el test t concreto que hay que usar depende de si las varianzas de las dos variables cuyas medias comparamos son iguales o diferentes. Como no sabemos nada de las varianzas de las longitudes de los sépalos de estas dos especies, y no nos supone apenas esfuerzo realizar los tests, llevaremos a cabo el contraste en los dos casos: varianzas iguales y varianzas diferentes.1 Más adelante, en el Ejemplo 5.11, explicamos cómo contrastar si estas dos varianzas son iguales o diferentes. S=iris[iris$Species==&quot;setosa&quot;,]$Sepal.Length V=iris[iris$Species==&quot;virginica&quot;,]$Sepal.Length El test suponiendo que las dos varianzas son iguales: t.test(S, V, alternative=&quot;less&quot;, var.equal=TRUE) ## ## Two Sample t-test ## ## data: S and V ## t = -15.39, df = 98, p-value &lt;2e-16 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -1.41126 ## sample estimates: ## mean of x mean of y ## 5.006 6.588 El test suponiendo que las dos varianzas son diferentes: t.test(S, V, alternative=&quot;less&quot;, var.equal=FALSE) ## ## Welch Two Sample t-test ## ## data: S and V ## t = -15.39, df = 76.52, p-value &lt;2e-16 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -1.4108 ## sample estimates: ## mean of x mean of y ## 5.006 6.588 En los dos casos el p-valor es prácticamente 0 y por lo tanto podemos rechazar la hipótesis nula en favor de la alternativa: tenemos evidencia estadísticamente muy significativa de que, en promedio, las flores de la especie setosa tienen sépalos más cortos que las de la especie virginica. El intervalo de confianza del 95% para la diferencia de medias \\(\\mu_s-\\mu_v\\) en este contraste es en ambos casos \\((-\\infty, -1.41]\\) y no contiene el 0, que sería el valor de esta diferencia si la hipótesis nula \\(\\mu_s=\\mu_v\\) fuera verdad. Ejemplo 5.5 En un experimento clásico de la primera década del siglo XX, Student quiso comparar el efecto somnífero de dos compuestos químicos, la hiosciamina y la hioscina. La hipótesis a contrastar era que la hioscina es más efectiva que la hiosciamina. Para ello, tomó 10 sujetos y midió su promedio de horas de sueño durante períodos de entre 3 y 9 días bajo tres tratamientos: en condiciones normales, tomando antes de acostarse 0.6 mg de hiosciamina y tomando antes de acostarse 0.6 mg de hioscina. A continuación, calculó para cada sujeto y cada compuesto la diferencia “promedio de horas de sueño tomando el compuesto menos promedio de horas de sueño en condiciones normales”. Las diferencias que obtuvo fueron las siguientes (las podéis encontrar en el dataframe sleep de la instalación básica de R, aunque no lo vamos a usar): Sujeto Hiosciamina Hioscina 1 0.7 1.9 2 -1.6 0.8 3 -0.2 1.1 4 -1.2 0.1 5 -0.1 -0.1 6 3.4 4.4 7 3.7 5.5 8 0.8 1.6 9 0.0 4.6 10 2.0 3.4 Una manera de comparar el efecto en las horas de sueño de estos compuestos es comparando las medias de estas diferencias de promedios de horas de sueño: una diferencia media mayor significa que, de media, el compuesto “ha añadido” más horas de sueño al promedio normal. Digamos \\(\\mu_1\\) a la media de las diferencias individuales del promedio de horas de sueño tomando hiosciamina menos el promedio en condiciones normales y \\(\\mu_2\\) a la media de las diferencias individuales del promedio de horas de sueño tomando hioscina menos el promedio en condiciones normales. Tomaremos como hipótesis nula \\(H_0: \\mu_1= \\mu_2\\) (ambos compuestos tienen el mismo efecto medio sobre las horas de sueño de los individuos) e hipótesis alternativa \\(H_1: \\mu_1&lt;\\mu_2\\) (la hioscina aumenta más las horas de sueño que la hiosciamina). Si podemos rechazar la hipótesis nula en favor de la alternativa, concluiremos que la hioscina tiene un mayor efecto somnífero que la hiosciamina. Observad que se trata de un contraste de dos muestras emparejadas, porque los datos refieren a los mismos 10 pacientes. Vamos a suponer que las diferencias medias en horas de sueño en ambos casos siguen leyes normales (Student así lo hizo) y que, por lo tanto, el resultado de un test t es fiable. Hiosciamina=c(0.7,-1.6,-0.2,-1.2,-0.1,3.4,3.7,0.8,0,2.0) Hioscina=c(1.9,0.8,1.1,0.1,-0.1,4.4,5.5,1.6,4.6,3.4) t.test(Hiosciamina, Hioscina, alternative=&quot;less&quot;, paired=TRUE) ## ## Paired t-test ## ## data: Hiosciamina and Hioscina ## t = -4.062, df = 9, p-value = 0.00142 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -0.866995 ## sample estimates: ## mean of the differences ## -1.58 El p-valor es 0.001, mucho menor que 0.05, y por lo tanto podemos rechazar la hipótesis nula con un nivel de significación del 5%. Observad también que el intervalo de confianza del 95% para la diferencia de medias \\(\\mu_1-\\mu_2\\) es \\((-\\infty,-0.867]\\) y está totalmente a la izquierda del 0. La conclusión es, pues, que efectivamente la hioscina tiene un mayor efecto somnífero que la hiosciamina y que con un 95% de confianza podemos afirmar que añade, de media, como mínimo 52 minutos (0.867 horas) diarios más de sueño. Ejemplo 5.6 Veamos un ejemplo de aplicación de t.test a una fórmula. Queremos contrastar si es cierto que fumar durante el embarazo está asociado a un peso menor del recién nacido. Si llamamos \\(\\mu_n\\) y \\(\\mu_f\\) al peso medio de un recién nacido de madre no fumadora y fumadora, respectivamente, el contraste que queremos realizar es \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu_n=\\mu_f\\\\ H_{1}:\\mu_n&gt; \\mu_f \\end{array}\\right. \\] Vamos a usar los datos incluidos en la tabla birthwt incluida en el paquete MASS, que recoge información sobre una muestra de madres y sus hijos. library(MASS) str(birthwt) ## &#39;data.frame&#39;:\t189 obs. of 10 variables: ## $ low : int 0 0 0 0 0 0 0 0 0 0 ... ## $ age : int 19 33 20 21 18 21 22 17 29 26 ... ## $ lwt : int 182 155 105 108 107 124 118 103 123 113 ... ## $ race : int 2 3 1 1 1 3 1 3 1 1 ... ## $ smoke: int 0 0 1 1 1 0 0 0 1 1 ... ## $ ptl : int 0 0 0 0 0 0 0 0 0 0 ... ## $ ht : int 0 0 0 0 0 0 0 0 0 0 ... ## $ ui : int 1 0 0 1 1 0 0 0 0 0 ... ## $ ftv : int 0 3 1 2 0 0 1 1 1 0 ... ## $ bwt : int 2523 2551 2557 2594 2600 2622 2637 2637 2663 2665 ... En la Ayuda de birthwt nos enteramos de que la variable smoke indica si la madre ha fumado durante el embarazo (1) o no (0), y de que la variable bwt da el peso del recién nacido en gramos. Lo primero que haremos será mirar si las muestras de madres fumadoras y no fumadoras contenidas en esta tabla son lo suficientemente grandes como para que el resultado del test t sea fiable. table(birthwt$smoke) ## ## 0 1 ## 115 74 Vemos que sí, que ambas son suficientemente grandes. Para entrar en la instrucción t.test los vectores de pesos de hijos de fumadoras y no fumadoras, usaremos la fórmula bwt~smoke especificando que data=birthwt. Fijaos en que los valores de smoke son 0 y 1, y que R los considera ordenados en este orden (basta ver el resultado de la función table anterior). Por consiguiente, bwt~smoke representa, en este orden, el vector de pesos de recién nacidos de madres no fumadoras (smoke=0) y el vector de pesos de recién nacidos de madres fumadoras (smoke=1). Como la hipótesis alternativa es \\(\\mu_n&gt;\\mu_f\\), deberemos especificar en la función t.test que alternative=\"greater\". Es un contraste de muestras independientes, y por lo tanto el procedimiento para llevarlo a cabo depende de la igualdad o no de las varianzas de los pesos de los recién nacidos en los dos grupos de madres. Como en el Ejemplo 5.4, vamos a llevar a cabo el test t suponiendo que estas varianzas son iguales y que son diferentes, y cruzaremos los dedos para que la conclusión sea la misma. Otra posibilidad es contrastar antes la igualdad de las varianzas. t.test(bwt~smoke, data=birthwt, alternative=&quot;greater&quot;, paired=FALSE, var.equal=TRUE) ## ## Two Sample t-test ## ## data: bwt by smoke ## t = 2.653, df = 187, p-value = 0.00433 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 106.953 Inf ## sample estimates: ## mean in group 0 mean in group 1 ## 3055.70 2771.92 t.test(bwt~smoke, data=birthwt, alternative=&quot;greater&quot;, paired=FALSE, var.equal=FALSE) ## ## Welch Two Sample t-test ## ## data: bwt by smoke ## t = 2.73, df = 170.1, p-value = 0.0035 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 111.855 Inf ## sample estimates: ## mean in group 0 mean in group 1 ## 3055.70 2771.92 En ambos casos hemos obtenido un p-valor un orden de magnitud inferior a 0.05 y un interval de confianza del 95% para \\(\\mu_n-\\mu_f\\) del orden de \\([110,\\infty)\\), lo que nos permite concluir que, en efecto, los hijos de las madres no fumadoras pesan más al nacer que los de las fumadoras. En vez de especificar los vectores de pesos con bwt~smoke,data=birthwt, hubiéramos podido usar birthwt$bwt~birthwt$smoke. Por ejemplo: t.test(birthwt$bwt~birthwt$smoke, alternative=&quot;greater&quot;, paired=FALSE, var.equal=TRUE) ## ## Two Sample t-test ## ## data: birthwt$bwt by birthwt$smoke ## t = 2.653, df = 187, p-value = 0.00433 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 106.953 Inf ## sample estimates: ## mean in group 0 mean in group 1 ## 3055.70 2771.92 Tests no paramétricos Cuando comparamos dos medias, o una media con un valor, usando un test t sobre muestras pequeñas, suponemos que las variables poblacionales que han producido las muestras son normales. En la Lección 6 estudiaremos los contrastes que nos permiten aceptar o rechazar que una muestra provenga de una variable aleatoria con una distribución concreta, pero en estos momentos ya tendría que ser claro que nos podemos encontrar con conjuntos de datos para los cuales el supuesto de normalidad de la variable poblacional no esté justificado: por ejemplo, porque sean datos cuantitativos discretos o porque la variable sea claramente muy asimétrica (por citar una, la duración del embarazo, con una clara cola a la izquierda: hay un número no despreciable de partos prematuros, pero ningún embarazo dura 11 meses). En las situaciones en las que no estamos seguros de que las variables poblacionales satisfagan aproximadamente las hipótesis de los teoremas que nos garantizan la fiabilidad de las conclusiones de un contraste, por ejemplo de un test t, una salida razonable es usar un test no paramétrico alternativo. En el caso de los contrastes de medias, los tests no paramétricos para comparar medias en realidad lo que comparan son las medianas. Los más populares son los siguientes: El test de signos, que permite contrastar si la mediana de una variable aleatoria cualquiera (incluso ordinal) es un valor dado \\(M_0\\) estudiando la distribución de los signos de las diferencias entre este valor y los de una muestra (si la mediana fuera \\(M_0\\), los números de diferencias positivas y negativas en muestras aleatorias seguirían distribuciones binomiales con \\(p=0.5\\)). En R está implementado en la función SIGN.test del paquete BSDA. Su sintaxis es similar a la de t.test para una muestra, cambiando el parámetro mu, que en t.test sirve para especificar el valor de la media que contrastamos, por md, que en SIGN.test sirve para especificar el valor de la mediana que contrastamos. Esta función también se puede aplicar a dos muestras emparejadas: en este caso, la hipótesis nula del contraste que realiza es que “la mediana de las diferencias de las dos variables es 0”. El test de Wilcoxon para comparar la media de una variable continua simétrica con un valor dado o las medias de dos variables continuas cuya diferencia sea simétrica por medio de muestras emparejadas. Más en general, se puede usar para comparar la mediana de una variable continua con un valor dado o para comparar la mediana de la diferencia de dos variables continuas (medidas sobre muestras emparejadas) con 0. Observad que cuando las variables en juego son simétricas, las medianas coinciden con las medias y el contraste de medianas es también un contraste de medias. En R está implementado en la función wilcox.test y su sintaxis es la misma que la de t.test para una muestra o para dos muestras emparejadas (en este último caso, hay que especificar paired=TRUE). El test de Mann-Whitney para comparar las medianas de dos variables aleatorias por medio de muestras independientes. En R también está implementado en la función wilcox.test y su sintaxis es la misma que la de t.test para dos muestras independientes (especificando paired=FALSE), salvo que aquí no hay que especificar si las varianzas son iguales o diferentes, puesto que esto no se usa en este test. Ejemplo 5.7 Si los niveles de colesterol no siguen una distribución normal, el test t realizado en el Ejemplo 5.3 no sirve para nada. Una posibilidad es entonces no contrastar si el nivel medio de colesterol es 220, sino si el nivel mediano es 220. Para ello vamos realizar un test de signos. Los parámetros alternative=\"two.sided\" y conf.level=0.95 son los que usa la función SIGN.test por defecto, así que no haría falta especificarlos; los incluimos para que los veáis. library(BSDA) SIGN.test(colesterol, md=220, alternative=&quot;two.sided&quot;, conf.level=0.95) ## ## One-sample Sign-Test ## ## data: colesterol ## s = 4, p-value = 1 ## alternative hypothesis: true median is not equal to 220 ## 95 percent confidence interval: ## 208.078 228.922 ## sample estimates: ## median of x ## 220 ## ## Achieved and Interpolated Confidence Intervals: ## ## Conf.Level L.E.pt U.E.pt ## Lower Achieved CI 0.8203 209.000 228.000 ## Interpolated CI 0.9500 208.078 228.922 ## Upper Achieved CI 0.9609 208.000 229.000 Observad que la salida de la función es muy similar a la de t.test (salvo por los últimos intervalos de confianza, que no vamos a explicar). El p-valor ha dado directamente 1 y el intervalo de confianza al 95% para la mediana ha dado [208.1, 228.9]: por lo tanto, no podemos rechazar que la mediana del nivel de colesterol en la población de la que hemos extraído la muestra sea 220. También podríamos usar el test de Wilcoxon para realizar este contraste de una mediana: wilcox.test(colesterol, mu=220, alternative=&quot;two.sided&quot;,conf.level=0.95) ## Warning in wilcox.test.default(colesterol, mu = 220, alternative = ## &quot;two.sided&quot;, : cannot compute exact p-value with zeroes ## ## Wilcoxon signed rank test with continuity correction ## ## data: colesterol ## V = 15, p-value = 0.726 ## alternative hypothesis: true location is not equal to 220 El p-valor es 0.726, la conclusión es la misma. El mensaje de advertencia nos avisa de que la muestra ha contenido valores iguales al valor de la mediana contrastado, por lo que el p-valor obtenido no es exacto. Solo os tenéis que preocupar de un mensaje como este si el p-valor fuera muy cercano al nivel de significación deseado, que no es el caso. Ejemplo 5.8 Si las diferencias en promedios de horas de sueño no siguen distribuciones normales, el test t realizado en el Ejemplo @ref(exm:sueño) no sirve para nada. En este caso, vamos a usar un test de Wilcoxon para muestras emparejadas. Este test en realidad contrastará la hipótesis nula de que si para cada individuo calculamos la diferencia entre el aumento promedio de horas de sueño cuando toma hiosciamina y el aumento promedio tomando hioscina, la mediana de la variable aleatoria que define estas diferencias es 0, y como hipótesis alternativa que esta mediana es menor que 0 (y que por lo tanto más de la mitad de las veces es negativa, es decir, que a más de la mitad de la población la hioscina le añade más tiempo promedio de sueño que la hiosciamina). Si las variables “aumento de horas de sueño” en juego son simétricas, estas medianas coinciden con las correspondientes medias y llevamos a cabo el contraste del Ejemplo @ref(exm:sueño). Si no son simétricas, igualmente estamos contrastando si la hioscina es más efectiva que la hiosciamina, solo que planteándolo de otra manera. wilcox.test(Hiosciamina, Hioscina, alternative=&quot;less&quot;, paired=TRUE) ## Warning in wilcox.test.default(Hiosciamina, Hioscina, alternative = ## &quot;less&quot;, : cannot compute exact p-value with ties ## Warning in wilcox.test.default(Hiosciamina, Hioscina, alternative = ## &quot;less&quot;, : cannot compute exact p-value with zeroes ## ## Wilcoxon signed rank test with continuity correction ## ## data: Hiosciamina and Hioscina ## V = 0, p-value = 0.00455 ## alternative hypothesis: true location shift is less than 0 En este caso R nos avisa de nuevo de que el p-valor no es exacto, pero esto no afecta a la conclusión dado que el p-valor es muy pequeño: rechazamos la hipótesis nula en favor de la alternativa y también concluimos con este test no paramétrico que la hioscina tiene un mayor efecto somnífero que la hiosciamina. Ejemplo 5.9 Nos preguntamos si los hijos de madres de 20 años pesan lo mismo al nacer que los de madres de 30 años, o no. Querríamos responder esta pregunta planteándola como un contraste bilateral de los pesos medios de los hijos de madres de 20 y 30 años y usando la muestra recogida en la tabla de datos birthwt del paquete MASS, que contiene la variable age con la edad de las madres. hijos.20=birthwt[birthwt$age==20,&quot;bwt&quot;] hijos.30=birthwt[birthwt$age==30,&quot;bwt&quot;] c(length(hijos.20),length(hijos.30)) ## [1] 18 7 Las muestras no son lo suficientemente grandes como para usar un test t si no estamos seguros de que las variables poblacionales sean normales. Como las muestras son independientes, vamos a usar un test de Mann-Whitney para comparar los pesos medianos. Es decir, en vez de traducir “los hijos de madres de 20 años pesan lo mismo al nacer que los de madres de 30 años” en términos de igualdad de pesos medios, lo traducimos en términos de igualdad de pesos medianos. (En realidad, la hipótesis nula de este test es que “Es igual de probable que un hijo de madre de 20 años pese más que un hijo de madre de 30 años que al revés”, pero no vamos a entrar en este nivel de precisión. Es otra manera de decir que no hay tendencia a que unos pesen más que los otros.) wilcox.test(hijos.20, hijos.30, alternative=&quot;two.sided&quot;,paired=FALSE) ## Warning in wilcox.test.default(hijos.20, hijos.30, alternative = ## &quot;two.sided&quot;, : cannot compute exact p-value with ties ## ## Wilcoxon rank sum test with continuity correction ## ## data: hijos.20 and hijos.30 ## W = 43.5, p-value = 0.25 ## alternative hypothesis: true location shift is not equal to 0 El p-valor es 0.25, por lo que no podemos rechazar que las medianas de los pesos al nacer de los hijos de madres de 20 años y de 30 sean iguales. 5.2 Contrastes para varianzas El test \\(\\chi^2\\) para comparar la varianza \\(\\sigma^2\\) (o la desviación típica \\(\\sigma\\)) de una población normal con un valor dado \\(\\sigma_0^2\\) (o \\(\\sigma_0\\)) usa el estadístico \\[ \\frac{(n-1)\\widetilde{S}_X^2}{\\sigma_0^2} \\] que, si la hipótesis nula \\(\\sigma^2=\\sigma_0^2\\) es verdadera, sigue una distribución \\(\\chi^2_{n-1}\\), de ahí su nombre. Dicho test está convenientemente implementado en la función sigma.test del paquete TeachingDemos. Su sintaxis es la misma que la de la función t.test para una muestra, substituyendo el parámetro mu de t.test por el parámetro sigma (para especificar el valor de la desviación típica que contrastamos, \\(\\sigma_0\\)) o sigmasq (por “sigma al cuadrado”, para especificar el valor de la varianza que contrastamos, \\(\\sigma_0^2\\)). Como siempre, los valores por defecto de alternative y conf.level son \"two.sided\" y 0.95, respectivamente. La salida de la función es también similar a la de t.test. Veamos un ejemplo. Ejemplo 5.10 Se ha realizado un experimento para estudiar el tiempo \\(X\\) (en minutos) que tarda un lagarto del desierto en llegar a los 45o partiendo de su temperatura normal mientras está a la sombra. Los tiempos obtenidos (en minutos) en una muestra aleatoria de lagartos fueron los siguientes: TL45=c(10.1,12.5,12.2,10.2,12.8,12.1,11.2,11.4,10.7,14.9,13.9,13.3) Supongamos que estos tiempos siguen una ley normal. ¿Aporta este experimento evidencia de que la desviación típica \\(\\sigma\\) de \\(X\\) es inferior a 1.5 minutos? Para responder esta pregunta, hemos de realizar el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:\\sigma= 1.5 \\\\ H_{1}:\\sigma&lt; 1.5 \\end{array}\\right. \\] Para ello, usaremos la función sigma.test aplicada a esta muestra y a sigma=1.5: library(TeachingDemos) sigma.test(TL45, sigma=1.5, alternative=&quot;less&quot;) ## ## One sample Chi-squared test for variance ## ## data: TL45 ## X-squared = 10.69, df = 11, p-value = 0.53 ## alternative hypothesis: true variance is less than 2.25 ## 95 percent confidence interval: ## 0.00000 5.25686 ## sample estimates: ## var of TL45 ## 2.18629 El p-valor que obtenemos es 0.53, muy grande, por lo que no tenemos evidencia que nos permita concluir que \\(\\sigma&lt;1.5\\). ¡Atención! El intervalo de confianza que da la función sigma.test es siempre para la varianza, aunque le entréis el valor de la desviación típica. Así que si queréis un intervalo de confianza para la desviación típica, tenéis que tomar la raíz cuadrada del que os da sigma.test: sqrt(sigma.test(TL45, sigma=1.5, alternative=&quot;less&quot;)$conf.int) ## [1] 0.00000 2.29279 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 El test \\(\\chi^2\\) no se usa mucho en la práctica. En parte, porque realmente es poco interesante ya que suele ser difícil conjeturar la desviación típica a contrastar, y en parte porque su validez depende fuertemente de la hipótesis de que la variable aleatoria poblacional sea normal. En cambio, el contraste de las desviaciones típicas de dos poblaciones sí que es muy utilizado. Por ejemplo, en un contraste de dos medias usando un test t sobre dos muestras independientes, nos puede interesar conocer a priori si las varianzas poblacionales son iguales o diferentes, en lugar de realizar el test bajo ambas suposiciones. Si no las conocemos, ¿cómo podemos saber cuál es el caso? Si las dos variables poblacionales son normales, podemos contrastar la igualdad de las varianzas con el test F, basado en el estadístico \\[ \\frac{\\widetilde{S}_{X_1}^2} {\\widetilde{S}_{X_2}^2} \\] que, si las dos poblaciones son normales y tienen la misma varianza, sigue una distribución F de Fisher-Snedecor. Por desgracia, este test es también muy sensible a la no normalidad de las poblaciones objeto de estudio: a la que una de ellas se aleja un poco de la normalidad, el test deja de dar resultados fiables.2 La función para efectuar este test es var.test y su sintaxis básica es la misma que la de t.test para dos muestras: var.test(x, y, alternative=..., conf.level=...) donde x e y son los dos vectores de datos, que se pueden especificar mediante una fórmula como en el caso de t.test, y el parámetro alternative puede tomar los tres mismos valores que en los tests anteriores: su valor por defecto es, como siempre, \"two.sided\", que es el que nos permite contrastar si las varianzas son iguales o diferentes. Ejemplo 5.11 Suponiendo que las longitudes de los sépalos de las flores de las diferentes especies de iris siguen leyes normales, ¿hubiéramos podido considerar a priori iguales las varianzas de las dos muestras en el Ejemplo 5.4? Veamos: S=iris[iris$Species==&quot;setosa&quot;,]$Sepal.Length V=iris[iris$Species==&quot;virginica&quot;,]$Sepal.Length var.test(S,V) ## ## F test to compare two variances ## ## data: S and V ## F = 0.3073, num df = 49, denom df = 49, p-value = 6.37e-05 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.174378 0.541496 ## sample estimates: ## ratio of variances ## 0.307286 El p-valor es \\(6.4\\times 10^{-5}\\), muy pequeño. Por lo tanto, podemos rechazar la hipótesis nula de que las dos varianzas son iguales, en favor de la hipótesis alternativa de que las dos varianzas son diferentes. Así, pues, bastaba realizar solo el t.test con var.equal=FALSE. Puede ser conveniente remarcar aquí que el intervalo de confianza obtenido con var.test es para el cociente de varianzas poblacionales \\(\\sigma^2_x/\\sigma^2_y\\), no para su diferencia. Por lo tanto, para contrastar si las varianzas son iguales o diferentes, hay que mirar si el 1 pertenece o no al intervalo obtenido. En este ejemplo, el intervalo de confianza al 95% ha sido [0.174, 0.541] y no contiene el 1, lo que confirma la evidencia de que las varianzas son diferentes. Ejemplo 5.12 Queremos contrastar si los gatos adultos macho pesan más que los gatos adultos hembra. Para ello usaremos los datos recogidos en el dataframe cats del paquete MASS, que contiene información sobre el peso de una muestra de gatos adultos, separados por su sexo. str(cats) ## &#39;data.frame&#39;:\t144 obs. of 3 variables: ## $ Sex: Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Bwt: num 2 2 2 2.1 2.1 2.1 2.1 2.1 2.1 2.1 ... ## $ Hwt: num 7 7.4 9.5 7.2 7.3 7.6 8.1 8.2 8.3 8.5 ... table(cats$Sex) ## ## F M ## 47 97 Consultando la Ayuda de cats nos enteramos de que la variable Bwt contiene el peso de cada gato en kg, y la variable Sex contiene el sexo de cada gato: F para hembra (female) y M para macho (male). Como vemos en la tabla de frecuencias, los números de ejemplares de cada sexo son grandes. Así pues, si llamamos \\(\\mu_m\\) al peso medio de un gato macho adulto y \\(\\mu_h\\) al peso medio de un gato hembra adulto, el contraste que vamos a realizar es \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu_m=\\mu_h\\\\ H_{1}:\\mu_m&gt;\\mu_h \\end{array}\\right. \\] y para ello antes vamos a contrastar si las varianzas de ambas poblaciones son iguales o diferentes, para luego poder aplicar la función t.test con el valor de var.equal adecuado. Vamos a suponer que los pesos en ambos sexos siguen leyes normales. Para que el contraste de las varianzas sea fiable es necesario que esta suposición sea cierta; para el de los pesos medios, no, ya que ambas muestras son grandes. El contraste de la igualdad de varianzas es el siguiente: var.test(Bwt~Sex, data=cats) ## ## F test to compare two variances ## ## data: Bwt by Sex ## F = 0.3435, num df = 46, denom df = 96, p-value = 0.000116 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.212628 0.580348 ## sample estimates: ## ratio of variances ## 0.343501 El p-valor es \\(1.2\\times 10^{-4}\\), y por lo tanto podemos rechazar la hipótesis nula de que las varianzas son iguales y concluir que son diferentes. Así que en el test t las consideraremos diferentes. Recordemos ahora que la hipótesis alternativa que queremos contrastar es \\(H_{1}:\\mu_m&gt;\\mu_h\\). En el factor cats$Sex, la F (hembra) va antes que la M (macho), y, por tanto, si entramos los vectores de pesos mediante Bwt~Sex,data=cats, el primer vector corresponderá a las gatas y el segundo a los gatos. Así pues, la hipótesis alternativa que tenemos que especificar es que la media del primer vector es inferior a la del segundo vector: alternative=\"less\". t.test(Bwt~Sex, data=cats, alternative=&quot;less&quot;,var.equal=FALSE) ## ## Welch Two Sample t-test ## ## data: Bwt by Sex ## t = -8.709, df = 136.8, p-value = 4.42e-15 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -0.437666 ## sample estimates: ## mean in group F mean in group M ## 2.35957 2.90000 Como el p-valor es prácticamente 0, podemos concluir que, efectivamente, de media, los gatos adultos pesan más que las gatas adultas. Hemos insistido en que el test F solo es válido si las dos poblaciones cuyas varianzas comparamos son normales. ¿Qué podemos hacer si dudamos de su normalidad? Usar un test no paramétrico que no presuponga esta hipótesis. Hay diversos tests no paramétricos para realizar contrastes bilaterales de dos varianzas. Aquí os recomendamos el test de Fligner-Killeen, implementado en la función fligner.test. Se aplica o bien a una list formada por las dos muestras, o bien a una fórmula que separe un vector numérico en dos muestras por medio de un factor de dos niveles. Ejemplo 5.13 Si queremos contrastar si las varianzas de las longitudes de los sépalos de las flores iris setosa y virginica son iguales o no sin presuponer que siguen leyes normales, podemos usar el test de Fligner-Killeen de la manera siguiente: fligner.test(list(S,V)) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: list(S, V) ## Fligner-Killeen:med chi-squared = 9.984, df = 1, p-value = 0.00158 El p-valor es 0.0016, por lo que podemos concluir que las varianzas son diferentes. Ejemplo 5.14 Si queremos contrastar si las varianzas de los pesos de los gatos y las gatas adultos son iguales o no sin presuponer que dichos pesos tienen distribuciones normales, podemos usar el test de Fligner-Killeen de la manera siguiente: fligner.test(Bwt~Sex, data=cats) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: Bwt by Sex ## Fligner-Killeen:med chi-squared = 16.91, df = 1, p-value = ## 3.92e-05 El p-valor es \\(4\\times 10^{-5}\\), por lo que podemos concluir que las varianzas son diferentes. 5.3 Contrastes para proporciones Cuando tenemos que efectuar un contraste sobre una probabilidad de éxito \\(p\\) de una variable Bernoulli, podemos emplear el test binomial exacto. Este test se basa en que, si la hipótesis nula \\(H_0: p=p_0\\) es verdadera, los números de éxitos en muestras aleatorias simples de tamaño \\(n\\) de la variable poblacional, que será de tipo \\(Be(p_0)\\), siguen una ley binomial \\(B(n,p_0)\\). Este test está implementado en la función binom.test, cuya sintaxis es binom.test(x, n, p=..., alternative=..., conf.level=...) donde x y n son números naturales: el número de éxitos y el tamaño de la muestra. p es la probabilidad de éxito que queremos contrastar. El significado de alternative y conf.level, y sus posibles valores, son los usuales. Fijaos en particular que binom.test no se aplica directamente al vector de una muestra, sino a su número de éxitos y a su longitud. Si la muestra es un vector binario X, el número de éxitos será sum(X) y la longitud length(X). Nota. El intervalo de confianza para la \\(p\\) que da binom.test en un contraste bilateral es el de Clopper-Pearson. Ejemplo 5.15 Recordemos el Ejemplo 5.1, donde, en una serie de 20 lanzamientos de una moneda, había obtenido 15 caras. ¿Podemos sospechar que la moneda está trucada a favor de cara? Como comentamos en ese ejemplo, si llamamos \\(p\\) a la probabilidad de obtener cara con esta moneda, el contraste que queremos realizar es \\[ \\left\\{\\begin{array}{l} H_{0}:p=0.5\\\\ H_{1}:p&gt; 0.5 \\end{array}\\right. \\] Usaremos la función binom.test: binom.test(15,20, p=0.5, alternative=&quot;greater&quot;) ## ## Exact binomial test ## ## data: 15 and 20 ## number of successes = 15, number of trials = 20, p-value = 0.0207 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.544418 1.000000 ## sample estimates: ## probability of success ## 0.75 El p-valor del test es 0.0207 y el intervalo de confianza que nos da este test para la \\(p\\) es [0.5444, 1]. Ambos valores coinciden con los dados en el ejemplo original Cuando la muestra es grande, pongamos de 40 o más sujetos, podemos usar también el test aproximado, basado en la aproximación de la distribución de la proporción muestral por medio de una normal dada por el Teorema Central del Límite. En R está implementado en la función prop.test, que además también sirve para contrastar dos proporciones por medio de muestras independientes grandes. Su sintaxis es prop.test(x, n, p =..., alternative=..., conf.level=...) donde: x puede ser dos cosas: Un número natural: en este caso, R entiende que es el número de éxitos en una muestra. Un vector de dos números naturales: en este caso, R entiende que es un contraste de dos proporciones y que estos son los números de éxitos en las muestras. Cuando trabajamos con una sola muestra, n es su tamaño. Cuando estamos trabajando con dos muestras, n es el vector de dos entradas de sus tamaños. Cuando trabajamos con una sola muestra, p es la proporción poblacional que contrastamos. En el caso de un contraste de dos muestras, no hay que especificarlo. El significado de alternative y conf.level, y sus posibles valores, son los usuales. Veamos algunos ejemplos más. Ejemplo 5.16 Queremos contrastar si la proporción de estudiantes zurdos en la UIB es diferente del 10%, el porcentaje estimado de zurdos en España. Es decir, si llamamos \\(p\\) a la proporción de estudiantes zurdos en la UIB, queremos realizar el contraste \\[ \\left\\{ \\begin{array}{l} H_0:p=0.1\\\\ H_1:p\\neq 0.1 \\end{array} \\right. \\] Para ello, tomamos una muestra de 50 estudiantes de la UIB encuestados al azar y resulta que 3 son zurdos. Vamos a suponer que forman una muestra aleatoria simple. Como la muestra es grande (\\(n=50\\)) usaremos la función prop.test. prop.test(3, 50, p=0.1) ## ## 1-sample proportions test with continuity correction ## ## data: 3 out of 50, null probability 0.1 ## X-squared = 0.5, df = 1, p-value = 0.48 ## alternative hypothesis: true p is not equal to 0.1 ## 95 percent confidence interval: ## 0.0156246 0.1754187 ## sample estimates: ## p ## 0.06 El p-valor obtenido en el test es 0.48, muy superior a 0.05. Por lo tanto, no podemos rechazar que un 10% de los estudiantes de la UIB sean zurdos. El intervalo de confianza del 95% para \\(p\\) que hemos obtenido es [0.016, 0.175]. La conclusión usando el test binomial hubiera sido la misma: binom.test(3, 50, p=0.1) ## ## Exact binomial test ## ## data: 3 and 50 ## number of successes = 3, number of trials = 50, p-value = 0.48 ## alternative hypothesis: true probability of success is not equal to 0.1 ## 95 percent confidence interval: ## 0.0125486 0.1654819 ## sample estimates: ## probability of success ## 0.06 Ya que estamos, comprobemos que el intervalo de confianza del 95% obtenido con binom.test es efectivamente el de Clopper-Pearson: library(epitools) binom.exact(3,50) ## x n proportion lower upper conf.level ## 1 3 50 0.06 0.0125486 0.165482 0.95 Nota. El intervalo de confianza que se obtiene con prop.test en un contraste bilateral es el de Wilson modificado mediante una corrección de continuidad, un ajuste que se recomienda realizar cuando una distribución discreta (en este caso, una binomial) se aproxima mediante una distribución continua. En concreto, la fórmula que utiliza prop.test es la que se explica en esta entrada de la Wikipedia. Podéis indicar que R no efectue esta corrección de continuidad con el parámetro correct=FALSE: prop.test(3, 50, p=0.1, correct=FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 3 out of 50, null probability 0.1 ## X-squared = 0.8889, df = 1, p-value = 0.346 ## alternative hypothesis: true p is not equal to 0.1 ## 95 percent confidence interval: ## 0.020615 0.162171 ## sample estimates: ## p ## 0.06 (Observad que sin el correct=FALSE, el encabezamiento del resultado del prop.test es 1-sample proportions test with continuity correction, mientras que con correct=FALSE es 1-sample proportions test without continuity correction). Comprobemos que el intervalo de confianza del 95% que hemos obtenido ahora sí que es el de Wilson: binom.wilson(3, 50) ## x n proportion lower upper conf.level ## 1 3 50 0.06 0.020615 0.162171 0.95 Ejemplo 5.17 Una empresa que fabrica trampas para cucarachas ha producido una nueva versión de su trampa más popular y afirma que la nueva trampa mata más cucarachas que la vieja. Hemos llevado a cabo un experimento para comprobarlo. Hemos situado dos trampas en dos habitaciones. En cada habitación hemos soltado 60 cucarachas. La versión vieja de la trampa ha matado 40 y la nueva, 48. ¿Es suficiente evidencia de que la nueva trampa es más efectiva que la vieja? Digamos \\(p_v\\) y \\(p_n\\) a las proporciones de cucarachas que matan la trampa vieja y la trampa nueva, respectivamente. La hipótesis nula será que las trampas de los dos tipos son igual de efectivas, \\(H_0:p_v=p_n\\), y la hipótesis alternativa que las trampas nuevas son más efectivas que las viejas, \\(H_1:p_v&lt;p_n\\). Los tamaños de las muestras nos permiten usar la función prop.test. prop.test(c(40,48),c(60,60),alternative=&quot;less&quot;) ## ## 2-sample test for equality of proportions with continuity ## correction ## ## data: c(40, 48) out of c(60, 60) ## X-squared = 2.088, df = 1, p-value = 0.0742 ## alternative hypothesis: less ## 95 percent confidence interval: ## -1.0000000 0.0146167 ## sample estimates: ## prop 1 prop 2 ## 0.666667 0.800000 El p-valor es 0.074, y el intervalo de confianza que nos da el test, [-1, 0.015], es para la diferencia de proporciones \\(p_v-p_n\\) y contiene el 0, aunque por poco. En resumen, a un nivel de significación de 0.05 no encontramos evidencia de que la trampa nueva sea mejor que la vieja, pero el resultado no es del todo concluyente y convendría llevar a cabo otro experimento con más cucarachas para aumentar la potencia (cf. Ejemplo 5.23 en la próxima sección). La función prop.test solo sirve para contrastar dos proporciones cuando las dos muestras son independientes y grandes. Un test que se puede usar siempre para contrastar dos proporciones usando muestras independientes es el test exacto de Fisher, que usa una distribución hipergeométrica. Supongamos que evaluamos una característica dicotómica (es decir, que solo puede tomar dos valores y por tanto define distribuciones de Bernoulli) sobre dos poblaciones y tomamos dos muestras independientes, una de cada población. Resumimos los resultados en una tabla como la que sigue: \\[ \\begin{array}{r|c} &amp; \\quad\\mbox{Población}\\quad \\\\ \\mbox{Característica} &amp;\\quad 1 \\qquad 2\\quad \\\\\\hline \\mbox{Sí} &amp;\\quad a \\qquad b\\quad \\\\ \\mbox{No} &amp;\\quad c \\qquad d\\quad \\end{array} \\] Llamemos \\(p_{1}\\) a la proporción de individuos con la característica bajo estudio en la población 1 y \\(p_{2}\\) a su proporción en la población 2. Queremos contrastar la hipótesis nula \\(H_{0}:p_1=p_2\\) contra alguna hipótesis alternativa. Por ejemplo, en el experimento de las trampas para cucarachas, las poblaciones vendrían definidas por el tipo de trampa, y la característica que tendríamos en cuenta sería si la cucaracha ha muerto o no, lo que nos daría la tabla siguiente: \\[ \\begin{array}{r|c} &amp; \\qquad\\mbox{Trampas}\\quad \\\\ &amp;\\quad \\mbox{Viejas}\\qquad \\mbox{Nuevas}\\\\\\hline \\mbox{Muertas} &amp;\\qquad 40 \\qquad\\qquad 48\\quad \\\\ \\mbox{Vivas} &amp;\\qquad 20 \\qquad\\qquad 12\\quad \\end{array} \\] El test exacto de Fisher está implementado en la función fisher.test. Su sintaxis es fisher.test(x, alternative=..., conf.level=...) donde x es la matriz \\(\\left(\\begin{array}{cc} a &amp; b\\\\ c &amp; d\\end{array}\\right)\\), en la que los números de éxitos van en la primera fila y los de fracasos en la segunda, y las poblaciones se ordenan por columnas. El significado de alternative y conf.level, y sus posibles valores, son los usuales. Así, en el ejemplo de las trampas para cucarachas, entraríamos: Datos=rbind(c(40,48),c(20,12)) Datos ## [,1] [,2] ## [1,] 40 48 ## [2,] 20 12 fisher.test(Datos, alternative=&quot;less&quot;) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: Datos ## p-value = 0.0739 ## alternative hypothesis: true odds ratio is less than 1 ## 95 percent confidence interval: ## 0.00000 1.08414 ## sample estimates: ## odds ratio ## 0.502909 y obtenemos de nuevo un p-valor cercano a 0.074. Hay que ir con cuidado con la interpretación del intervalo de confianza que da esta función: no es ni para la diferencia de las proporciones ni para su cociente, sino para su odds ratio: el cociente \\[ \\Big({\\frac{p_v}{1-p_v}}\\Big)\\Big/\\Big({\\frac{p_n}{1-p_n}}\\Big). \\] Recordad que si la probabilidad de un suceso \\(A\\) es \\(P(A)\\), sus odds son el cociente \\[ \\mbox{Odds}(A)=\\frac{P(A)}{1-P(A)} \\] que mide cuántas veces es más probable \\(A\\) que su contrario. Las odds son una función creciente de la probabilidad, y por lo tanto \\[ \\mbox{Odds}(A)&lt;\\mbox{Odds}(B)\\Longleftrightarrow P(A)&lt;P(B). \\] Esto permite comparar odds en vez de probabilidades, con la misma conclusión. Por ejemplo, en nuestro caso, como el intervalo de confianza para la odds ratio va de 0 a 1.084, en particular contiene el 1, por lo que no podemos rechazar que \\[ \\Big({\\frac{p_v}{1-p_v}}\\Big)\\Big/\\Big({\\frac{p_n}{1-p_n}}\\Big)=1, \\] es decir, no podemos rechazar que \\[ \\frac{p_v}{1-p_v}=\\frac{p_n}{1-p_n} \\] y esto es equivalente a \\(p_v=p_n\\). Si, por ejemplo, el intervalo de confianza hubiera ido de 0 a 0.8, entonces la conclusión a este nivel de confianza hubiera sido que \\[ \\Big({\\frac{p_v}{1-p_v}}\\Big)\\Big/\\Big({\\frac{p_n}{1-p_n}}\\Big)&lt;1 \\] es decir, que \\[ \\frac{p_v}{1-p_v}&lt;\\frac{p_n}{1-p_n} \\] y esto es equivalente a \\(p_v&lt;p_n\\). Ejemplo 5.18 Para determinar si el Síndrome de Muerte Súbita del Recién Nacido (SIDS, por sus siglas en inglés) tiene algún componente genético, se estudiaron parejas de gemelos y mellizos en las que se dio algún caso de SIDS. Sean \\(p_1\\) la proporción de casos con exactamente una muerte por SIDS entre las parejas de gemelos con algún caso de SIDS, y \\(p_2\\) la proporción de casos con exactamente una muerte por SIDS entre las parejas de mellizos con algún caso de SIDS. La hipótesis de trabajo es que si el SIDS tiene componente genético, será más probable que un gemelo de un muerto por SIDS también lo sufra que si solo es mellizo, y por lo tanto que en las parejas de gemelos ha de ser más raro que haya exactamente un caso de SIDS que en las parejas de mellizos. Es decir, que \\(p_1&lt;p_2\\). Así pues, queremos realizar el contraste \\[ \\left\\{\\begin{array}{l} H_0:p_1=p_2\\\\ H_1:p_1&lt; p_2 \\end{array}\\right. \\] En un estudio se obtuvieron los datos siguientes: \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Tipo de gemelos}\\ \\\\ \\mbox{Casos de SIDS} &amp;\\ \\mbox{Gemelos}\\qquad \\mbox{Mellizos}\\\\\\hline \\mbox{Uno} &amp; \\quad\\ 23 \\qquad\\quad\\quad\\quad \\ 35\\quad \\\\ \\mbox{Dos} &amp; \\quad\\ 1 \\quad\\quad\\qquad\\quad \\ \\hphantom{3} 2 \\quad \\end{array} \\] Vamos a realizar el contraste. Observad que damos la tabla de manera que \\(p_1\\) es la proporción de parejas con un solo caso de SIDS entre las de la población 1 (gemelos), y \\(p_{2}\\) es la proporción de parejas con un solo caso de SIDS entre las de la población 2 (mellizos). Por tanto hemos de aplicar fisher.test a esta matriz y \\(p_1&lt;p_2\\) corresponderá a alternative=\"less\". Datos=rbind(c(23,35),c(1,2)) Datos ## [,1] [,2] ## [1,] 23 35 ## [2,] 1 2 fisher.test(Datos, alternative=&quot;less&quot;) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: Datos ## p-value = 0.784 ## alternative hypothesis: true odds ratio is less than 1 ## 95 percent confidence interval: ## 0.0000 39.7395 ## sample estimates: ## odds ratio ## 1.30859 El p-valor es 0.784, muy grande, por lo que no obtenemos evidencia de componente genético en el SIDS. Supongamos ahora que queremos comparar dos proporciones usando muestras emparejadas. Por ejemplo, supongamos que evaluamos dos características dicotómicas sobre una misma muestra de \\(n\\) sujetos. Resumimos los resultados obtenidos en la tabla siguiente: \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Característica 1}\\ \\\\ \\mbox{Característica 2} &amp;\\ \\ \\, \\mbox{Sí}\\qquad \\mbox{No}\\\\\\hline \\mbox{Sí} &amp; \\quad\\ \\ a \\qquad \\ \\ \\, b\\quad \\\\ \\mbox{No} &amp; \\quad\\ \\ c \\qquad \\ \\ \\, d\\quad \\end{array} \\] donde \\(a+b+c+d=n\\). Esta tabla quiere decir, naturalmente, que \\(a\\) sujetos de la muestra tuvieron la característica 1 y la característica 2, que \\(b\\) sujetos de la muestra tuvieron la característica 2 pero no tuvieron la característica 1, etc. Vamos a llamar \\(p_{1}\\) a la proporción poblacional de individuos con la característica 1, y \\(p_{2}\\) a la proporción poblacional de individuos con la característica 2. Queremos contrastar la hipótesis nula \\(H_{0}:p_1=p_2\\) contra alguna hipótesis alternativa. En este caso, no pueden usarse las funciones prop.test o fisher.test. Tenemos dos soluciones posibles. La primera nos permite realizar el contraste bilateral \\[ \\left\\{\\begin{array}{l} H_{0}:p_1=p_2\\\\ H_{1}:p_1\\neq p_2 \\end{array}\\right. \\] cuando \\(n\\) es grande (digamos que \\(n\\geq 100\\)) y el número \\(b+c\\) de casos discordantes (en los que una característica da Sí y la otra da No) es razonablemente grande, pongamos \\(\\geq 20\\). En esta situación podemos usar el test de McNemar, que se lleva a cabo en R con la instrucción mcnemar.test. Su sintaxis básica es mcnemar.test(X) donde X es la matriz \\(\\left(\\begin{array}{cc} a &amp; b\\\\ c&amp; d \\end{array}\\right)\\) que corresponde a la tabla anterior. No se especifica el nivel de confianza, porque no produce intervalos de confianza. Ejemplo 5.19 Para comparar la efectividad de dos tratamientos del asma, se escogieron 200 pacientes con asma severo, y a cada uno se le trató durante un mes con el tratamiento A o el tratamiento B, decidiéndose cada tratamiento al azar; tras esta fase de tratamiento, se les dejó sin tratamiento durante un mes, y a continuación a cada uno se le trató durante un mes con el otro tratamiento (B si antes había recibido A, A si antes había recibido B). Se anotó si durante cada periodo de tratamiento cada enfermo visitó o no el servicio de urgencias por dificultades respiratorias. Los resultados del experimento se resumen en la tabla siguiente (“Sí” significa que el enfermo acudió a urgencias por dificultades respiratorias): \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Tratamiento A}\\ \\\\ \\mbox{Trat. B} &amp;\\quad \\ \\mbox{ Sí}\\qquad\\quad \\mbox{No}\\quad \\\\\\hline \\mbox{Sí} &amp; \\quad \\ 71 \\qquad\\quad 48\\quad \\\\ \\mbox{No} &amp; \\quad \\ 30 \\qquad\\quad 51\\quad \\end{array} \\] Queremos determinar si hay diferencia en la efectividad de los dos tratamientos. Para ello, entramos la tabla anterior en una matriz y le aplicamos la función mcnemar.test: Datos=matrix(c(71,48,30,51),nrow=2,byrow=TRUE) Datos ## [,1] [,2] ## [1,] 71 48 ## [2,] 30 51 mcnemar.test(Datos) ## ## McNemar&#39;s Chi-squared test with continuity correction ## ## data: Datos ## McNemar&#39;s chi-squared = 3.705, df = 1, p-value = 0.0542 El p-valor del test es 0.054, ligeramente superior a 0.05, por lo tanto no permite concluir a un nivel de significación del 5% que haya evidencia de que la efectividad de los dos tratamientos sea diferente, pero sería conveniente llevar a cabo un estudio más amplio. Otra posibilidad para realizar un contraste de dos proporciones usando muestras emparejadas, que no requiere de ninguna hipótesis sobre los tamaños de las muestras, es usar de manera adecuada la función binom.test. Para explicar este método, consideremos la tabla siguiente, donde ahora damos las probabilidades poblacionales de las cuatro combinaciones de resultados: \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Característica 1}\\ \\\\ \\mbox{Característica 2} &amp;\\quad \\ \\!\\mbox{Sí}\\qquad\\quad\\, \\mbox{No}\\quad \\\\\\hline \\mbox{Sí} &amp; \\quad \\ p_{11} \\qquad\\quad p_{01}\\quad \\\\ \\mbox{No} &amp; \\quad \\ p_{10} \\qquad\\quad p_{00}\\quad \\end{array} \\] De esta manera \\(p_1=p_{11}+p_{10}\\) y \\(p_2=p_{11}+p_{01}\\). Entonces, \\(p_1=p_2\\) es equivalente a \\(p_{10}=p_{01}\\) y cualquier hipótesis alternativa se traduce en la misma desigualdad, pero para \\(p_{10}\\) y \\(p_{01}\\): \\(p_1\\neq p_2\\) es equivalente a \\(p_{10}\\neq p_{01}\\); \\(p_1&lt; p_2\\) es equivalente a \\(p_{10}&lt; p_{01}\\); y \\(p_1&gt; p_2\\) es equivalente a \\(p_{10}&gt; p_{01}\\). Por lo tanto podemos traducir el contraste sobre \\(p_1\\) y \\(p_2\\) al mismo contraste sobre \\(p_{10}\\) y \\(p_{01}\\). La gracia ahora está en que si la hipótesis nula \\(p_{10}=p_{01}\\) es cierta, entonces, en el total de casos discordantes, el número de sujetos en los que la característica 1 da Sí y la característica 2 da No sigue una ley binomial con \\(p=0.5\\). Por lo tanto, podemos efectuar el contraste usando un test binomial exacto tomando como muestra los casos discordantes de nuestra muestra, de tamaño \\(b+c\\), como éxitos los sujetos que han dado Sí en la característica 1 y No en la característica 2, de tamaño \\(c\\), con proporción a contrastar \\(p=0.5\\) y con hipótesis alternativa la que corresponda. La ventaja de este test es que su validez no requiere de ninguna hipótesis sobre los tamaños de las muestras. El inconveniente es que el intervalo de confianza que nos dará será para \\(p_{10}/(p_{10}+p_{01})\\), y no permite obtener un intervalo de confianza para la diferencia o el cociente de las probabilidades \\(p_1\\) y \\(p_2\\) de interés. Ejemplo 5.20 Usemos el test binomial para llevar a cabo el contraste bilateral del Ejemplo 5.19. Habíamos obtenido 30+48=78 casos discordantes, de los que 48 eran casos en los que el tratamiento A había dado Sí y el tratamiento B había dado No. binom.test(48, 78, p=0.5) ## ## Exact binomial test ## ## data: 48 and 78 ## number of successes = 48, number of trials = 78, p-value = 0.0535 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.498331 0.723398 ## sample estimates: ## probability of success ## 0.615385 Obtenemos de nuevo un p-valor en la zona de penumbra, ligeramente superior a 0.05. Ejemplo 5.21 Para determinar si un test casero de VIH basado en un frotis bucal da más positivos (que seguramente serán falsos positivos) que el test de VIH de referencia, basado en una analítica de sangre que mide la carga viral y mucho más costoso, se tomó una muestra aleatoria de 241 individuos en situación de riesgo, y a todos se les realizaron ambos tests. Los resultados se resumen en la tabla siguiente: \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Test estándar}\\ \\\\ \\mbox{Test casero} &amp;\\quad \\ \\mbox{ Positivo}\\qquad\\quad \\quad \\mbox{Negativo}\\quad \\\\\\hline \\mbox{Positivo} &amp; \\quad\\ \\ 72 \\qquad\\qquad\\qquad\\ 10 \\quad \\\\ \\mbox{Negativo} &amp; \\quad\\quad 2 \\qquad\\qquad\\quad\\quad \\ 157 \\quad \\end{array} \\] Si llamamos \\(p_{c}\\) a la probabilidad de que el test casero dé positivo y \\(p_{e}\\) a la probabilidad de que el test estándar dé positivo, queremos realizar el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:p_{e}=p_{c}\\\\ H_{1}:p_{e}&lt; p_{c} \\end{array}\\right. \\] El número de casos discordantes es pequeño (10+2=12) y además el test es unilateral, así que usaremos el test binomial. Como queremos realizar un contraste unilateral, hay que pensar en cómo traducir la hipótesis alternativa en términos de una hipótesis sobre la probabilidad \\(p=p_{10}/(p_{10}+p_{01})\\) de que un caso discordante tenga la característica 1 (la de las columnas). Veamos, \\(p_{e}&lt; p_{c}\\) significa que la característica de las columnas es menos probable que la de las filas, por tanto se ha de traducir en que la probabilidad de tener la característica de las columnas y no la de las filas es más pequeña que la probabilidad de tener la característica de las filas y no la de las columnas, es decir, en que \\(p&lt;0.5\\): hemos de usar alternative=less. binom.test(2, 12, alternative=&quot;less&quot;, p=0.5) ## ## Exact binomial test ## ## data: 2 and 12 ## number of successes = 2, number of trials = 12, p-value = 0.0193 ## alternative hypothesis: true probability of success is less than 0.5 ## 95 percent confidence interval: ## 0.000000 0.438105 ## sample estimates: ## probability of success ## 0.166667 Obtenemos evidencia significativa de que, efectivamente, el test casero da positivo con mayor frecuencia que el de referencia. 5.4 Cálculo de la potencia de un contraste Recordemos que la potencia de un contraste de hipótesis es la probabilidad de no cometer un error de tipo II, es decir, la probabilidad de aceptar la hipótesis alternativa si es verdadera. Usualmente, la probabilidad de cometer un error de tipo II se denota por \\(\\beta\\), y por lo tanto la potencia es \\(1-\\beta\\). La potencia de un contraste está relacionada con lo que se llama la magnitud del efecto (effect size). En un contraste, el efecto es la diferencia entre el valor estimado del parámetro a partir de la muestra usada y el valor que se da a dicho parámetro como hipótesis nula: por ejemplo, en el contraste de una media, la diferencia entre la media muestral \\(\\overline{x}\\) y el valor contrastado \\(\\mu_0\\); o, en el contraste de dos medias, la diferencia entre las dos medias muestrales. Se rechaza entonces la hipótesis nula si el efecto observado es tan grande que es muy improbable cuando la hipótesis nula es verdadera. Pero recordad que, en realidad, no se tiene en cuenta si el efecto observado ha sido grande o no por si mismo, solo si es estadísticamente significativo, es decir, si es improbable cuando la hipótesis nula es verdadera. Entonces, sin entrar en detalle, digamos que la magnitud del efecto es una medida estadística específica del tamaño del efecto observado respecto de su valor esperado si la hipótesis nula es verdadera. La fórmula para calcular la magnitud del efecto depende del contraste y del estadístico usado. Para cada tipo de test se han consensuado unos valores de la magnitud del efecto considerados como “pequeño”, “medio” y “grande”. Estos valores se obtienen con R con la función cohen.ES del paquete pwr. Su sintaxis básica es cohen.ES(test=..., size=...) donde: el parámetro test sirve para indicar el tipo de test: por ejemplo, test=\"t\" para un test t usando t.test, o test=\"p\" para un test aproximado de proporciones usando prop.test; el parámetro size sirve para indicar la magnitud esperada: \"small\", \"medium\" o \"large\". A modo de ejemplo, la siguiente instrucción nos da la magnitud de efecto que se considera pequeña en un test t: library(pwr) cohen.ES(test=&quot;t&quot;,size=&quot;small&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = t ## size = small ## effect.size = 0.2 De manera similar, para saber la magnitud de efecto que se considera media en un test aproximado de proporciones podemos usar instrucción siguiente: cohen.ES(test=&quot;p&quot;,size=&quot;medium&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = p ## size = medium ## effect.size = 0.5 Si se desea solo el valor de la magnitud del efecto, para poderlo entrar en otras funciones, se obtiene con el sufijo $effect.size: cohen.ES(test=&quot;p&quot;,size=&quot;medium&quot;)$effect.size ## [1] 0.5 Así pues, en un contraste de hipótesis intervienen cuatro cantidades fundamentales: el tamaño de la muestra, \\(n\\); el nivel de significación, \\(\\alpha\\); la potencia, \\(1-\\beta\\); y la magnitud del efecto. El tamaño de la muestra y el nivel de significación están bajo el control del investigador; sin embargo, la potencia del contraste y la magnitud del efecto afectan al contraste de forma más indirecta y su control escapa al investigador. Por ejemplo, si incrementamos el tamaño de la muestra, la potencia aumenta, pero el aumento preciso depende de la magnitud del efecto esperada. De hecho, las cuatro cantidades anteriores no son independientes, sino que, a partir de tres cualesquiera de ellas, se puede calcular la cuarta. Las funciones del paquete pwr permiten realizar estos cálculos para algunos contrastes básicos. Las funciones de dicho paquete que por ahora nos interesan en este sentido son las siguientes: pwr.t.test, para utilizar en tests t de una media, de dos medias usando muestras emparejadas o de dos medias usando muestras independientes del mismo tamaño. pwr.t2n.test, para utilizar en tests t de dos medias usando muestras independientes de distinto tamaño. pwr.p.test, para utilizar en contrastes aproximados de una proporción. pwr.2p.test, para utilizar en contrastes aproximados de dos proporciones usando muestras independientes del mismo tamaño. pwr.2p2n.test, para utilizar en contrastes aproximados de dos proporciones usando muestras de distinto tamaño. Estas funciones tienen los parámetros básicos siguientes: n: el tamaño de la muestra (o de las muestras cuando son del mismo tamaño). n1 y n2: los tamaños de las dos muestras en pwr.2p2n.test y pwr.t2n.test. d (en las dos primeras) o h (en las tres últimas): la magnitud del efecto. sig.level: el nivel de significación. power: la potencia. type (en la primera): el tipo de muestras usado, siendo sus posibles valores \"one.sample\" (para contrastes de una muestra), \"two.sample\" (para contrastes de dos muestras independientes), o \"paired\" (para contrastes de dos muestras emparejadas). alternative: el tipo de hipótesis alternativa, con sus valores usuales. Si, en una cualquiera de estas funciones se especifican todos los parámetros n (o n1 y n2), d (o h), sig.level y power menos uno, la función da el valor del parámetro que falta. Veamos algunos ejemplos de uso. Ejemplo 5.22 Queremos calcular la potencia del contraste llevado a cabo en el Ejemplo 5.2. Se trataba de un contraste bilateral de una media usando un test t, por lo que utilizaremos la función pwr.t.test. Los parámetros que le entraremos son: n, el tamaño de la muestra; en este ejemplo, \\(n=25\\). d, la magnitud del efecto. Para tests t de una media e hipótesis nula \\(H_0: \\mu = \\mu_0\\), la magnitud del efecto se calcula con la fórmula \\[ d=\\frac{|\\overline{x}-\\mu_0|}{\\widetilde{s}_x}. \\] En nuestro ejemplo, \\(d=\\frac{|2.8048-2|}{0.68064}= 1.1824\\). sig.level, el nivel de significación; en este ejemplo, \\(\\alpha=0.05\\). Además como es un contraste bliateral de una media, especificaremos type=\"one.sample\" y alternative=\"two.sided\" (esto último en realidad no hace falta: como siempre, este es su valor por defecto). x=c(2.2,2.66,2.74,3.41,2.46,2.96,3.34,2.16,2.46,2.71,2.04,3.74,3.24, 3.92,2.38,2.82,2.2,2.42,2.82,2.84,4.22,3.64,1.77,3.44,1.53) mag.ef=abs(mean(x)-2)/sd(x) #Magnitud del efecto pwr.t.test(n=25, d=mag.ef, sig.level=0.05, type=&quot;one.sample&quot;, alternative=&quot;two.sided&quot;) ## ## One-sample t test power calculation ## ## n = 25 ## d = 1.18241 ## sig.level = 0.05 ## power = 0.999893 ## alternative = two.sided Obtenemos que la potencia del test es prácticamente 1. Si estuviéramos diseñando el experimento y quisiéramos calcular el tamaño mínimo de una muestra para tener un nivel de significación del 5% y potencia del 99%, suponiendo a priori que la magnitud del efecto esperado va a ser grande (y que por lo tanto detectar que la hipótesis alternativa es verdadera va a ser fácil), primero calcularíamos cuánto vale una magnitud del efecto grande: cohen.ES(test=&quot;t&quot;,size=&quot;large&quot;)$effect.size ## [1] 0.8 y a continuación la usaríamos en la función pwr.t.test: pwr.t.test(d=0.8, sig.level=0.05, power=0.99, type=&quot;one.sample&quot;) ## ## One-sample t test power calculation ## ## n = 30.7143 ## d = 0.8 ## sig.level = 0.05 ## power = 0.99 ## alternative = two.sided Bastarían 31 observaciones para tener la potencia deseada. Si en cambio esperáramos una magnitud del efecto pequeña: pwr.t.test(d=cohen.ES(test=&quot;t&quot;,size=&quot;small&quot;)$effect.size, sig.level=0.05, power=0.99, type=&quot;one.sample&quot;) ## ## One-sample t test power calculation ## ## n = 461.238 ## d = 0.2 ## sig.level = 0.05 ## power = 0.99 ## alternative = two.sided En este caso necesitaríamos 462 observaciones. Podemos obtener solo una de las componentes del resultado de una de estas funciones añadiéndole el sufijo adecuado. Por ejemplo, la potencia se obtiene con el sufijo $power y el valor de \\(n\\) con el sufijo $n: pwr.t.test(n=25, d=mag.ef, sig.level=0.05, type=&quot;one.sample&quot;, alternative=&quot;two.sided&quot;)$power ## [1] 0.999893 pwr.t.test(d=0.8, sig.level=0.05, power=0.99, type=&quot;one.sample&quot;)$n ## [1] 30.7143 Ejemplo 5.23 Vamos a calcular la potencia del contraste \\[ \\left\\{ \\begin{array}{l} H_0:p_v=p_n\\\\ H_1:p_v&lt;p_n \\end{array} \\right. \\] del Ejemplo 5.17. En este caso, usamos la función pwr.2p.test, ya que usamos dos muestras del mismo tamaño, y le entramos los parámetros siguientes: n, el tamaño de las muestras; en este ejemplo, \\(n=60\\). h, la magnitud del efecto. Para calcularla,3 usamos la función ES.h del mismo paquete pwr y que se aplica a las proporciones muestrales de éxitos: en este ejemplo, \\(\\widehat{p}_v=0.67\\) y \\(\\widehat{p}_n =0.8\\) y la magnitud del efecto vale: ES.h(0.67,0.8) ## [1] -0.296584 sig.level, el nivel de significación, 0.05. Como solo nos interesa la potencia, añadiremos al pwr.2p.test el sufijo $power: pwr.2p.test(h=ES.h(0.67,0.8), n=60, sig.level=0.05,alternative=&quot;less&quot;)$power ## [1] 0.491864 Hemos obtenido una potencia de, aproximadamente, un 49%. Si estuviéramos diseñando el experimento y quisiéramos calcular el tamaño de las muestras necesario para tener una potencia del 90% al nivel de significación del 5% y esperando una magnitud del efecto pequeña (porque esperamos una mejora con las nuevas trampas, pero solo pequeña), entraríamos: cohen.ES(test=&quot;p&quot;,size=&quot;small&quot;)$effect.size ## [1] 0.2 pwr.2p.test(h=-0.2, sig.level=0.05, power=0.9,alternative=&quot;less&quot;)$n ## [1] 428.192 Tendríamos que usar dos muestras de 429 cucarachas cada una. Observad que en pwr.2p.test hemos entrado en h la magnitud del efecto en negativo: esto es debido a que usamos alternative=\"less\" y por lo tanto esperamos que la primera proporción sea menor que la segunda. Ejemplo 5.24 En el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu_n=\\mu_f\\\\ H_{1}:\\mu_n&gt; \\mu_f \\end{array}\\right. \\] del Ejemplo 5.6, ¿qué tamaño de la muestra de mujeres fumadoras tendríamos que tomar si usáramos una muestra de 100 no fumadoras, quisiéramos una potencia del 90% y un nivel de significación del 5% y esperáramos una magnitud del efecto media? Como es un contraste de dos medias independientes y los tamaños de las muestras pueden ser diferentes, usaremos la función pwr.t2n.test. Entraremos como n1 el tamaño de la muestra de fumadoras y le pediremos que nos dé solo el valor de n2, el tamaño de la “otra” muestra: pwr.t2n.test(n1=100, d=cohen.ES(test=&quot;t&quot;,size=&quot;medium&quot;)$effect.size, sig.level=0.05, power=0.9, alternative=&quot;greater&quot;)$n2 ## [1] 52.8251 Bastaría estudiar 53 madres fumadoras. 5.5 Guía rápida Excepto en las que decimos lo contrario, todas las funciones para realizar contrastes que damos a continuación admiten los parámetros alternative, que sirve para especificar el tipo de contraste (unilateral en un sentido u otro o bilateral), y conf.level, que sirve para indicar el nivel de confianza \\(1-\\alpha\\). Sus valores por defecto son contraste bilateral y nivel de confianza 0.95. t.test realiza tests t para contrastar una o dos medias (tanto usando muestras independientes como emparejadas). Aparte de alternative y conf.level, sus parámetros principales son: mu para especificar el valor de la media que queremos contrastar en un test de una media. paired para indicar si en un contraste de dos medias usamos muestras independientes o emparejadas. var.equal para indicar en un contraste de dos medias usando muestras independientes si las varianzas poblacionales son iguales o diferentes. SIGN.test, del paquete BSDA, realiza un test de signos para contrastar una mediana. Dispone del parámetro md para entrar la mediana a contrastar. wilcox.test realiza tests de Wilcoxon y de Mann-Whitney para contrastar una o dos medianas (tanto usando muestras independientes como emparejadas). Sus parámetros son los mismos que los de t.test (salvo var.equal, que en estos tests no tiene sentido). sigma.test realiza tests \\(\\chi^2\\) para contrastar una varianza (o una desviación típica). Dispone de los parámetros sigma y sigmasq para indicar, respectivamente, la desviación típica o la varianza a contrastar. var.test realiza tests F para contrastar dos varianzas (o dos desviaciones típicas). fligner.test realiza tests no paramétricos de Fligner-Killeen para contrastar dos varianzas (o dos desviaciones típicas). No dispone de los parámetros alternative (solo sirve para contrastes bilaterales) ni conf.level (no calcula intervalos de confianza). binom.test realiza tests binomiales exactos para contrastar una proporción. Dispone del parámetro p para indicar la proporción a contrastar. prop.test realiza tests aproximados para contrastar una proporción o dos proporciones de poblaciones usando muestras independientes. También dispone del parámetro p para indicar la proporción a contrastar en un contraste de una proporción. fisher.test realiza tests exactos de Fisher para contrastar dos proporciones usando muestras independientes. mcnemar.test realiza tests bilaterales de McNemar para contrastar dos proporciones usando muestras emparejadas. No dispone de los parámetros alternative ni conf.level. cohen.ES, del paquete pwr, da los valores aceptados por convenio como “pequeño”, “mediano” y “grande” para diferentes tests. pwr.t.test, del paquete pwr, relaciona el tamaño de la(s) muestra(s), el nivel de significación, la potencia y la magnitud del efecto (en el sentido de que si se entran tres de estos valores se obtiene el cuarto) en tests t de una media, de dos medias usando muestras emparejadas o de dos medias usando muestras independientes del mismo tamaño. Sus parámetros, son n: el tamaño de la muestra o de las muestras. sig.level: el nivel de significación. power: la potencia. d: la magnitud del efecto type: el tipo de muestras (una muestra, dos muestras emparejadas, dos muestras independientes). alternative: el tipo de hipótesis alternativa. pwr.t2n.test, del paquete pwr, relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en tests t de dos medias usando muestras independientes de distinto tamaño. Sus parámetros son n1 y n2: los tamaños de las dos muestras. sig.level, power, d y alternative como en pwr.t.test. pwr.p.test, del paquete pwr, relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en contrastes aproximados de una proporción. Sus parámetros son n, sig.level, power y alternative como en pwr.t.test. h: la magnitud del efecto pwr.2p.test, del paquete pwr, relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en contrastes aproximados de dos proporciones usando muestras independientes del mismo tamaño. Sus parámetros son los mismos que los de pwr.p.test. pwr.2p2n.test, del paquete pwr, relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en contrastes aproximados de dos proporciones usando muestras de distinto tamaño. Sus parámetros son n1 y n2: los tamaños de las dos muestras. sig.level, power, h y alternative como en pwr.p.test. 5.6 Ejercicios Modelo de test (1) Tenemos una m.a.s. de una población normal \\(X\\sim N(\\mu,\\sigma)\\) formada por los números 2,5,3,5,6,6,7,2. Usando la función t.test, calculad el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del contraste \\(H_0: \\mu=4\\) contra \\(H_1:\\mu \\neq 4\\) y decid (contestando SI, sin acento, o NO) si podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.05. Tenéis que dar las dos respuestas en este orden, separadas por un único espacio en blanco. (2) Tenemos dos muestras de poblaciones normales, \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\). Sean \\(x_1=(2,5,3,5,6,6,7,2)\\) y \\(x_2=(3,2,5,4,2,2,4,5,1,6,2)\\) muestras aleatorias simples de \\(X_1\\) y \\(X_2\\), respectivamente. Usando la función t.test, calculad el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del contraste \\(H_0: \\mu_1=\\mu_2\\) contra \\(H_1:\\mu_1&gt;\\mu_2\\) suponiendo que las varianzas son diferentes y decid (contestando SI, sin acento, o NO) si podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.1. Tenéis que dar las dos respuestas en este orden, separadas por un único espacio en blanco. (3) Tenemos dos muestras de poblaciones normales, \\(X_1\\sim N(\\mu_1,\\sigma_1)\\) y \\(X_2\\sim N(\\mu_2,\\sigma_2)\\). Sean \\(x_1=(2,5,3,5,6,6,7,2)\\) y \\(x_2=(3,2,10,9,2,2,4,5,1,10,2)\\) muestras aleatorias simples de \\(X_1\\) y \\(X_2\\), respectivamente. Usando la función var.test, calculad los extremos inferior y superior de un intervalo de confianza del 95% para \\(\\sigma_1^2/\\sigma_2^2\\) (redondeados a 3 cifras decimales, sin ceros innecesarios a la derecha) y decid (contestando SI, sin acento, o NO) si en el contraste \\(H_0: \\sigma_1=\\sigma_2\\) contra \\(H_1:\\sigma_1 \\neq \\sigma_2\\) podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.05. Tenéis que dar las tres respuestas en este orden, separadas por un único espacio en blanco. (4) Tenemos dos variables aleatorias de Bernoulli de proporciones poblacionales \\(p_1\\) y \\(p_2\\), respectivamente. En una muestra de 100 observaciones de la primera hemos obtenido 20 éxitos, y en una muestra de 150 observaciones de la segunda, hemos obtenido 40 éxitos. Usando la función prop.test, calculad el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del contraste \\(H_0: p_1=p_2\\) contra \\(H_1:p_1&lt;p_2\\) y decid (contestando SI, sin acento, o NO) si podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.05. Tenéis que dar las dos respuestas en este orden, separadas por un único espacio en blanco. Ejercicios (1) Para satisfacer las necesidades respiratorias de los peces de agua caliente, el contenido de oxígeno disuelto debe presentar un promedio de 6.5 partes por millón (ppm), con una desviación típica no mayor de 1.2 ppm. Cuando la temperatura del agua crece, el oxígeno disuelto disminuye, y esto causa la asfixia del pez. Se realizó un estudio sobre los efectos del calor en verano en el contenido de oxígeno disuelto en un gran lago. Después de un período particularmente caluroso, se tomaron muestras de agua en \\(35\\) lugares aleatoriamente seleccionados en el lago, y se determinó el contenido de oxígeno disuelto. Los resultados (en ppm) fueron los siguientes: O2=c(9.1,6.8,7.0,7.5,8.7,3.2,5.4,8.1,4.4,5.1,6.2,6.9,6.9,4.3, 8.0,5.3,6.2,6.4,7.8,5.8,6.9,7.7,5.2,5.8,6.3,5.9,8.5,7.5,8.9, 5.6,6.6,5.3,5.7,6.9,6.6) Suponemos que estos contenidos de oxígeno siguen una distribución normal. (a) ¿Hay evidencia de que el contenido medio de oxígeno en el lago sea inferior al nivel aceptable de \\(6.5\\) ppm? (b) ¿Hay evidencia de que la desviación típica del contenido de oxígeno en el lago sea superior a \\(1.2\\) ppm? (2) Los angiogramas son la técnica estándar para diagnosticar un ictus, pero tienen un ligero riesgo de mortalidad (inferior al 1%). Algunos investigadores han propuesto usar una prueba PET para diagnosticar el ictus de manera no invasiva. Sobre 64 pacientes ingresados en urgencias con síntomas de ictus se usaron ambas técnicas de diagnóstico. Los resultados obtenidos se resumen en la tabla siguiente: \\[ \\begin{array}{r|c} &amp; \\quad\\mbox{Angiograma}\\quad \\\\ \\mbox{PET} &amp;\\quad \\mbox{Positivo} \\qquad \\mbox{Negativo}\\quad \\\\\\hline \\mbox{Positivo} &amp;\\quad 32 \\qquad\\qquad\\ \\ 8\\quad \\\\ \\mbox{Negativo} &amp;\\quad\\quad 3 \\qquad\\qquad\\ \\ 21\\quad\\ \\end{array} \\] Contrastad si ambas técnicas de diagnóstico tienen la misma probabilidad de dar positivo. (3) Muchos autores afirman que los pacientes con depresión tienen una función cortical por debajo de lo normal debido a una disminución en el riego sanguíneo cerebral. A dos muestras de individuos, unos con depresión y otros normales, se les midió un índice que indica el flujo sanguíneo en la materia gris (dado en mg/(100g/min)). Suposemos que los valores de este índice en ambos grupos siguen leyes normales. Los vectores con los valores obtenidos son: Depresivos=c(51, 37, 46, 46, 54, 66, 44, 43, 54, 54, 42, 56, 44, 46, 52, 53, 45, 39, 41, 43, 41, 50, 33, 62, 50, 50, 48, 47, 50, 46, 54, 57, 50, 41, 29, 50, 39, 35, 45) Normales=c(47, 43, 45, 59, 53, 43, 56, 68, 47, 53, 53, 60, 67, 66, 48, 65, 45, 60, 53, 56, 43, 51, 56, 47, 47, 61, 47, 55, 53, 53, 65, 59, 59, 61, 48, 47, 59, 51, 57, 52, 50, 50) ¿Aportan estos datos evidencia a un nivel de significación del 5% de que los pacientes con depresión tienen el riego sanguíneo cerebral por debajo de lo normal? (4) En un ensayo clínico del 2003, se estudió el efecto de una terapia antirretroviral altamente activa (HAART) sobre el recuento de células T CD4 en enfermos de SIDA. Con este fin, se contó (en millones por litro de plasma) en un grupo de pacientes este número antes del tratamiento y al cabo de 12 meses de tratamiento. Los datos obtenidos se recogen en la tabla de datos haart.txt que encontraréis en el aula Digital. A partir de estos datos, ¿hay evidencia a un nivel de significación del 5% de que esta terapia aumenta la población de células T CD4? Respuestas al test (1) 0.487 NO Nosotros lo hemos resuelto con x=c(2,5,3,5,6,6,7,2) round(t.test(x,mu=4)$p.value,3) ## [1] 0.487 (2) 0.083 SI Nosotros lo hemos resuelto con x1=c(2,5,3,5,6,6,7,2) x2=c(3,2,5,4,2,2,4,5,1,6,2) round(t.test(x1,x2,alternative=&quot;greater&quot;)$p.value,3) ## [1] 0.083 (3) 0.078 1.465 NO Nosotros lo hemos resuelto con x1=c(2,5,3,5,6,6,7,2) x2=c(3,2,10,9,2,2,4,5,1,10,2) round(var.test(x1,x2)$conf.int,3) ## [1] 0.078 1.465 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 (4) 0.145 NO Nosotros lo hemos resuelto con round(prop.test(c(20,40),c(100,150),alternative=&quot;less&quot;)$p.value,3) ## [1] 0.145 Respuestas sucintas a los ejercicios (1) (a) No: O2=c(9.1,6.8,7.0,7.5,8.7,3.2,5.4,8.1,4.4,5.1,6.2,6.9,6.9,4.3, 8.0,5.3,6.2,6.4,7.8,5.8,6.9,7.7,5.2,5.8,6.3,5.9,8.5,7.5,8.9, 5.6,6.6,5.3,5.7,6.9,6.6) t.test(O2,mu=6.5,alternative=&quot;less&quot;) ## ## One Sample t-test ## ## data: O2 ## t = 0.124, df = 34, p-value = 0.549 ## alternative hypothesis: true mean is less than 6.5 ## 95 percent confidence interval: ## -Inf 6.91816 ## sample estimates: ## mean of x ## 6.52857 (b) Tampoco: library(TeachingDemos) sigma.test(O2, sigma=1.2, alternative=&quot;greater&quot;) ## ## One sample Chi-squared test for variance ## ## data: O2 ## X-squared = 43.87, df = 34, p-value = 0.12 ## alternative hypothesis: true variance is greater than 1.44 ## 95 percent confidence interval: ## 1.29976 Inf ## sample estimates: ## var of O2 ## 1.85798 (2) No: binom.test(3,11, p=0.5) ## ## Exact binomial test ## ## data: 3 and 11 ## number of successes = 3, number of trials = 11, p-value = 0.227 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.0602177 0.6097426 ## sample estimates: ## probability of success ## 0.272727 (3) Sí. Es un contraste de dos medias usando muestras independientes. Primero contrastamos la igualdad de varianzas: var.test(Depresivos, Normales) ## ## F test to compare two variances ## ## data: Depresivos and Normales ## F = 1.182, num df = 38, denom df = 41, p-value = 0.599 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.629926 2.235350 ## sample estimates: ## ratio of variances ## 1.18209 Aceptamos que las varianzas son iguales. Realizamos el test t: t.test(Depresivos, Normales,alternative=&quot;less&quot;,var.equal=TRUE) ## ## Two Sample t-test ## ## data: Depresivos and Normales ## t = -4.184, df = 79, p-value = 3.69e-05 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -4.07174 ## sample estimates: ## mean of x mean of y ## 47.0000 53.7619 (4) Sí: haart=read.table(&quot;haart.txt&quot;,header=TRUE) str(haart) ## &#39;data.frame&#39;:\t250 obs. of 2 variables: ## $ CD4baseline: int 57 69 72 73 105 48 117 66 85 90 ... ## $ CD4after : int 183 285 298 290 225 306 225 74 167 222 ... t.test(haart[,1],haart[,2],alternative=&quot;less&quot;,paired=TRUE) ## ## Paired t-test ## ## data: haart[, 1] and haart[, 2] ## t = -23.23, df = 249, p-value &lt;2e-16 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -147.601 ## sample estimates: ## mean of the differences ## -158.892 Se sabe que si las dos muestras provienen de poblaciones normales y son del mismo tamaño, el test t tiende a dar la misma conclusión tanto si se supone que las dos varianzas son iguales como si se supone que son diferentes: véase C. A. Markowski y E. P. Markowski, “Conditions for the Effectiveness of a Preliminary Test of Variance,” The American Statistician 44 (1990), pp. 322-326. Pero no sabemos si estas longitudes siguen distribuciones normales o no, y que “tienda a dar” la misma conclusión no significa que en un ejemplo concreto con p-valores cercanos al nivel de significación no pueda dar conclusiones diferentes.↩︎ Véanse: E. S. Pearson, “The analysis of variance in cases of non-normal variation,” Biometrika 23 (1931), pp. 114-133; G. E. P. Box, “Non-normality and tests on variances,” Biometrika 40 (1953), pp. 318-335.↩︎ Por si a alguien le interesa, la fórmula para esta magnitud del efecto es\n\\[\nh=2\\left(\\arcsin\\big(\\sqrt{\\widehat{p}_1}\\,\\big)-\\arcsin\\big(\\sqrt{\\widehat{p}_2}\\,\\big)\\right),\n\\]\nsiendo \\(\\widehat{p}_1\\) y \\(\\widehat{p}_2\\) las proporciones muestrales de éxitos de las dos muestras.↩︎ "],
["chap-bondad.html", "Lección 6 Contrastes de bondad de ajuste 6.1 Pruebas gráficas: Q-Q-plots 6.2 El test \\(\\chi^2\\) de Pearson 6.3 El test \\(\\chi^2\\) para distribuciones continuas 6.4 El test de Kolgomorov-Smirnov 6.5 Tests de normalidad 6.6 Guía rápida 6.7 Ejercicios", " Lección 6 Contrastes de bondad de ajuste Una de las condiciones habituales que requerimos sobre una muestra, por ejemplo, al razonar sobre la distribución de sus estadísticos o al realizar contrastes de hipótesis, es que la población de la que la hemos extraído siga una determinada distribución. En la Lección ?? de la primera parte del curso comprobábamos gráficamente el ajuste de una muestra a una distribución normal mediante histogramas y dibujando las curvas de densidad muestral y de densidad de la normal. En esta lección presentamos algunas instrucciones que implementan contrastes de bondad de ajuste (goodness of fit), técnicas cuantitativas que permiten decidir si los datos de una muestra “se ajustan” a una determinada distribución de probabilidad, es decir, si la variable aleatoria que los ha generado sigue o no esta distribución de probabilidad. Los contrastes de bondad de ajuste tienen el mismo significado que los explicados en la Lección 5. Se contrasta una hipótesis nula \\(H_0\\): La variable aleatoria poblacional tiene distribución \\(X\\) contra la hipótesis alternativa \\(H_1\\): La variable aleatoria poblacional no tiene distribución \\(X\\) Como siempre, para llevar a cabo el contraste tomamos una muestra aleatoria de la población. Entonces, obtenemos evidencia significativa de que la población no tiene distribución \\(X\\) cuando es muy raro obtener nuestra muestra si la población tiene esta distribución. En este caso, rechazamos la hipótesis nula en favor de la alternativa. Si, en cambio, no es del todo inverosímil que la muestra se haya generado con la distribución \\(X\\), aceptamos la hipótesis nula “por defecto” y concluimos que la población sí que tiene esta distribución. Pero que aceptemos la hipótesis nula no nos da evidencia de que la población tenga distribución \\(X\\): simplemente nos dice que no encontramos motivos para rechazarlo. Naturalmente, a efectos prácticos, si aceptamos la hipótesis nula de que la población tiene la distribución \\(X\\), actuaremos como si creyéramos que efectivamente esta hipótesis es verdadera, por ejemplo a la hora de decidir que fórmulas usar para calcular un intervalo de confianza o para efectuar un contraste de hipótesis. Pero no estaremos seguros de que podamos emplear estas fórmulas sobre nuestra muestra, simplemente no tendremos motivos para dudarlo. Los pasos habituales para contrastar la bondad del ajuste de una muestra a una distribución son los siguientes: Fijar la familia de distribuciones teóricas a la que queremos ajustar los datos. Esta familia estará parametrizada por uno o varios parámetros. Recordemos los ejemplos más comunes: Si la familia es la Bernoulli, el parámetro es \\(p\\): la probabilidad poblacional de éxito. Si la familia es la Poisson, el parámetro es \\(\\lambda\\): la esperanza. Si la familia es la binomial, los parámetros son \\(n\\) y \\(p\\): el tamaño de las muestras y la probabilidad de éxito, respectivamente. Si la familia es la normal, los parámetros son \\(\\mu\\) y \\(\\sigma\\): la esperanza y la desviación típica, respectivamente. Si la familia es la \\(\\chi^2\\), el parámetro es el número de grados de libertad. Si la familia es la t de Student, el parámetro es de nuevo el número de grados de libertad. Otras familias de distribuciones tienen parámetros de localización (location), escala (scale) o forma (shape), por lo que no nos ha de extrañar si R nos pide que asignemos parámetros con estos nombres. Si el diseño del experimento no fija sus valores, tendremos que estimar a partir de la muestra los valores de los parámetros que mejor se ajusten a nuestros datos. Ya hemos tratado la estimación de parámetros en la Lección 3. Determinar qué tipo de contraste vamos a utilizar. En esta lección veremos dos tipos básicos de contrastes generales: El test \\(\\chi^2\\) de Pearson. Este test es válido tanto para variables discretas como para continuas, pero solo se puede aplicar a conjuntos grandes de datos (por fijar una cota concreta, de 30 o más elementos). Además, si el espacio muestral, es decir, el conjunto de resultados posibles, es infinito, es necesario agrupar estos resultados en un número finito de clases. El test de Kolgomorov-Smirnov. Este test solo es válido para variables continuas, y compara la función de distribución acumulada muestral con la teórica. No requiere que la muestra sea grande, pero en cambio, en principio, no admite que los datos de la muestra se puedan repetir.4 Por desgracia, las repeticiones suelen ser habituales si la muestra es grande y la precisión de los datos es baja o la variabilidad de la población muestreada es pequeña. Aparte, determinados tipos de distribuciones tienen sus contrastes de bondad de ajustes específicos. Este es el caso especialmente de la normal, para la que explicaremos algunos tests que permiten contrastar si una muestra proviene de alguna distribución normal. Realizar el contraste y redactar las conclusiones. Es conveniente apoyar los resultados del contraste con gráficos. En esta lección explicaremos los gráficos cuantil-cuantil, o Q-Q-plots, que sirven para visualizar el ajuste de unos datos a una distribución conocida y son una buena alternativa a los histogramas con curvas de densidad. 6.1 Pruebas gráficas: Q-Q-plots Para comparar la distribución de una muestra con una distribución poblacional teórica se pueden realizar diversas pruebas gráficas. En la Lección ?? de la primera parte del curso usábamos para ello histogramas con densidades estimadas y teóricas. En esta sección explicamos otro tipo de gráficos que pueden usarse con el mismo fin, los gráficos cuantil-cuantil, o, para abreviar, Q-Q-plots. Estos gráficos comparan los cuantiles observados de la muestra con los cuantiles teóricos de la distribución teórica. Figura 6.1: Q-Q-plot básico de la muestra del Ejemplo 6.1 contra una t de Student con 4 grados de libertad. La Figura 6.1 muestra un Q-Q-plot. Cada punto corresponde a un cuantil: grosso modo, hay un punto para cada \\(k/n\\)-cuantil, siendo \\(n\\) la longitud de la muestra y \\(k=1,\\ldots,n\\). Para cada uno de estos cuantiles, el punto correspondiente tiene abscisa el cuantil de la distribución teórica (en este caso, una t de Student con 4 grados de libertad) y ordenada el cuantil de la muestra. Por lo tanto, si el ajuste es bueno, para cada \\(k/n\\), el cuantil muestral y el cuantil teórico han de ser parecidos, de manera que los puntos del gráfico (les llamaremos Q-Q-puntos, para abreviar) han de estar cerca de la diagonal \\(y=x\\), que hemos añadido al gráfico. En general, se considera que un Q-Q-plot muestra un buen ajuste cuando no se observa una tendencia marcada de desviación respecto de la diagonal. Sin embargo, a menudo los Q-Q-plots son difíciles de interpretar, y es conveniente combinarlos con algún contraste de bondad de ajuste. Hay varias maneras de producir Q-Q-plots con R. Aquí solo explicaremos una: la función qqPlot del paquete car. Su sintaxis básica es qqPlot(x, distribution=..., parámetros, id=FALSE, ...) donde: x es el vector con la muestra. El parámetro distribution se ha de igualar al nombre de la familia de distribuciones entre comillas, y puede tomar como valor cualquier familia de distribuciones de la que R sepa calcular la densidad y los cuantiles: esto incluye las distribuciones que hemos estudiado hasta el momento: \"norm\", \"binom\", \"poisson\", \"t\", etc. A continuación, se tienen que entrar los parámetros de la distribución, igualando su nombre habitual (mean para la media, sd para la desviación típica, df para los grados de libertad, etc.) a su valor. En algunos casos, si no se especifican los parámetros, qqPlot toma sus valores por defecto: por ejemplo, si queremos realizar un Q-Q-plot contra una normal y no especificamos los valores de la media y la desviación típica de la distribución teórica, qqPlot los toma iguales a 0 y 1, respectivamente. Por defecto, el gráfico obtenido con la función qqPlot identifica los dos Q-Q-puntos con ordenadas más extremas. Para omitirlos, usad el parámetro id=FALSE. Otros parámetros a tener en cuenta: qqPlot añade por defecto una rejilla al gráfico, que podéis eliminar con grid=FALSE. qqPlot añade por defecto una línea recta que une los Q-Q-puntos correspondientes al primer y tercer cuartil: se la llama recta cuartil-cuartil. Un buen ajuste de los Q-Q-puntos a esta recta significa que la muestra se ajusta a la distribución teórica, pero posiblemente con parámetros diferentes a los especificados. Os recomendamos mantenerla, pero si queréis eliminarla por ejemplo para substituirla por la diagonal \\(y=x\\), podéis usar el parámetro line=\"none\". qqPlot también añade dos curvas discontinuas que abrazan una “región de confianza del 95%” para el Q-Q-plot. Sin entrar en detalles, esta región contendría todos los Q-Q-puntos en un 95% de las ocasiones que tomáramos una muestra de la distribución teórica del mismo tamaño que la nuestra. Por lo tanto, si todos los Q-Q-puntos caen dentro de esta franja, no hay evidencia para rechazar que la muestra provenga de la distribución teórica. Esta franja de confianza es muy útil para interpretar el Q-Q-plot, pero la podéis eliminar con envelope=FALSE. Se pueden usar los parámetros usuales de plot para poner nombres a los ejes, título, modificar el estilo de los puntos, etc., y otros parámetros específicos para modificar el aspecto del gráfico. Por ejemplo, col.lines sirve para especificar el color de las líneas que añade. Consultad la Ayuda de la función. Ejemplo 6.1 Consideremos la siguiente muestra: muestra=c(0.27,0.81,-0.73,-0.96,1.33,0.91,-1.70,0.24,-0.19,0.29,1.41,0.13,-0.06, -0.85,-0.59,-3.62,-1.02,2.36,0.34,-0.31,0.81,-0.88,0.27,0.52,1.05,0.20,0.76,0.25, -1.43,3.71,-0.78,0.39,-1.01,1.53,-0.72,1.22,0.56,-1.17,-0.65,-0.33,-0.07,0.31, -0.74,0.36,-1.72,-1.21,-0.05,-1.17,0.28,1.30,0.89,1.45,0.13,-1.12,3.13,-1.21, -0.90,-0.31,-1.05,0.89,-1.06,0.21,-0.50,-0.36,-0.29,-0.19,-1.71,0.09,0.21,0.55, -1.42,0.19,-0.62,2.46,-0.17,-0.63,0.77,0.94,0.55,0.35,-4.47,1.71,0.07,-0.57, -1.43,-0.85,1.06,0.82,0.19,-1.08,0.30,-0.87,0.77,1.23,-0.04,0.66,-0.87,-0.86, -1.06,0.10) Queremos comprobar gráficamente si sigue una distribución t de Student de 4 grados de libertad. Vamos a usar la función qqPlot con sus parámetros por defecto: library(car) qqPlot(muestra, distribution=&quot;t&quot;, df=4, id=FALSE) Figura 6.2: Q-Q-plot de la muestra del Ejemplo 6.1 contra una t de Student con 4 grados de libertad producido por defecto con qqPlot. Como todos los Q-Q-puntos están dentro de la región de confianza del 95%, podemos aceptar que la muestra proviene de una t de Student. El Q-Q-plot básico de la Figura 6.1 se ha obtenido con el código siguiente: qqPlot(muestra, distribution=&quot;t&quot;, df=4, envelope=FALSE, xlab=&quot;Cuantiles de t&quot;, ylab=&quot;Cuantiles de la muestra&quot;, line=&quot;none&quot;, pch=20, grid=FALSE, id=FALSE) abline(0,1, col=&quot;red&quot;, lwd=1.5) Veamos otro ejemplo. Ejemplo 6.2 Consideremos el data frame iris que contiene información sobre medidas relacionadas con las flores de una muestra de iris de tres especies. Vamos a producir un Q-Q-plot que ilustre si las longitudes de los sépalos de las plantas iris recogidas en esta tabla de datos siguen una distribución normal. A un Q-Q-plot que compara una muestra con una distribución normal se le suele llamar, para abreviar, un normal-plot. En primer lugar, estimamos los parámetros máximo verosímiles de la distribución normal que podría haber generado nuestra muestra: library(MASS) iris.sl=iris$Sepal.Length mu=fitdistr(iris.sl,&quot;normal&quot;)$estimate[1] sigma=fitdistr(iris.sl,&quot;normal&quot;)$estimate[2] round(c(mu,sigma),3) ## mean sd ## 5.843 0.825 y ahora generamos el Q-Q-plot usando estos parámetros qqPlot(iris.sl, distribution=&quot;norm&quot;, mean=mu, sd=sigma, xlab=&quot;Cuantiles de la normal&quot;, ylab=&quot;Cuantiles de la muestra&quot;, main=&quot;&quot;, pch=20, cex=0.7, id=FALSE) Figura 6.3: Normal-plot de longitudes de sépalos de flores iris. Vemos cómo los primeros puntos salen de la región de confianza del 95% por encima. Esto significa que los valores más pequeños de la muestra son mayores de lo que sería de esperar si la muestra viniera de la variable normal. Interpretamos este Q-Q-plot como evidencia de que estas longitudes no siguen una distribución normal. Más adelante usaremos tests de normalidad específicos para contrastar la normalidad de estos datos. 6.2 El test \\(\\chi^2\\) de Pearson El test \\(\\chi^2\\) de Pearson contrasta si una muestra ha sido generada o no con una cierta distribución, cuantificando si sus valores aparecen con una frecuencia cercana a la que sería de esperar si la muestra siguiera esa distribución. Esto se lleva a cabo por medio del estadístico de contraste \\[ X^2=\\sum_{i=1}^k\\frac{(\\mbox{frec. observada}_i-\\mbox{frec. esperada}_i)^2}{\\mbox{frec. esperada}_i} \\] donde k es el número de clases e i es el índice de las clases, de manera que “frec. observadai” y “frec. esperadai” denotan, respectivamente, la frecuencia observada de la clase i-ésima y su frecuencia esperada bajo la distribución que contrastamos. Si se satisfacen una serie de condiciones, este estadístico sigue aproximadamente una ley \\(\\chi^2\\) con un número de grados de libertad igual al número de clases menos uno y menos el número de parámetros de la distribución teórica que hayamos estimado. Las condiciones que se han de satisfacer son: La muestra ha de ser grande, digamos que de tamaño como mínimo 30; Si los posibles valores son infinitos, hay que agruparlos en un número finito k de clases que cubran todos los posibles valores (recordad que en la Lección ?? de la primera parte del curso ya explicamos cómo agrupar variables aleatorias continuas con la función cut); Las frecuencias esperadas de las clases en las que hemos agrupado el espacio muestral han de ser todas, o al menos una gran mayoría, mayores o iguales que 5. La instrucción básica en R para realizar un test \\(\\chi^2\\) es chisq.test. Su sintaxis básica es chisq.test(x, p=..., rescale.p=..., simulate.p.value=...) donde: x es el vector (o la tabla, calculada con table) de frecuencias absolutas observadas de las clases en la muestra. p es el vector de probabilidades teóricas de las clases para la distribución que queremos contrastar. Si no lo especificamos, se entiende que la probabilidad es la misma para todas las clases. Obviamente, estas probabilidades se tienen que especificar en el mismo orden que las frecuencias de x y, como son las probabilidades de todos los resultados posibles, en principio tienen que sumar 1; esta condición se puede relajar con el siguiente parámetro. rescale.p es un parámetro lógico que, si se iguala a TRUE, indica que los valores de p no son probabilidades, sino solo proporcionales a las probabilidades; esto hace que R tome como probabilidades teóricas los valores de p partidos por su suma, para que sumen 1. Por defecto vale FALSE, es decir, se supone que el vector que se entra como p son probabilidades y por lo tanto debe sumar 1, y si esto no pasa se genera un mensaje de error indicándolo. Igualarlo a TRUE puede ser útil, porque nos permite especificar las probabilidades mediante las frecuencias esperadas o mediante porcentajes. Pero también es peligroso, porque si nos hemos equivocado y hemos entrado un vector en p que no corresponda a una probabilidad, R no nos avisará. simulate.p.value es un parámetro lógico que indica a la función si debe optar por una simulación para estimar el p-valor del contraste. Por defecto vale FALSE, en cuyo caso este p-valor no se estima sino que se calcula mediante la distribución \\(\\chi^2\\) correspondiente. Si se especifica como TRUE, R realiza una serie de replicaciones aleatorias de la situación teórica: por defecto, 2000, pero su número se puede especificar mediante el parámetro B. Es decir, genera un conjunto de vectores aleatorios de frecuencias con la distribución que queremos contrastar, cada uno de suma total la de x. A continuación, calcula la proporción de estas repeticiones en las que el estadístico de contraste es mayor o igual que el obtenido para x, y éste es el p-valor que da. Cuando no se satisfacen las condiciones para que \\(X^2\\) siga aproximadamente una distribución \\(\\chi^2\\), estimar el p-valor mediante simulaciones es una buena alternativa. Veamos un primer ejemplo sencillo. Ejemplo 6.3 Tenemos un dado, y queremos contrastar si está equilibrado o trucado. Lo hemos lanzado 40 veces y hemos obtenido los resultados siguientes: Resultados Frecuencias 1 8 2 4 3 6 4 3 5 7 6 12 Si el dado está equilibrado, la probabilidad de cada resultado es 1/6 y por lo tanto la frecuencia esperada de cada resultado es 40/6=6.667. Como la muestra tiene más de 30 elementos y las frecuencias esperadas son todas mayores que 5, podemos realizar de manera segura un test \\(\\chi^2\\). Por lo tanto, entraremos estas frecuencias en un vector y le aplicaremos la función chisq.test. Como contrastamos si todas las clases tienen la misma probabilidad, no hace falta especificar el valor del parámetro p. freqs=c(8,4,6,3,7,12) chisq.test(freqs) ## ## Chi-squared test for given probabilities ## ## data: freqs ## X-squared = 7.7, df = 5, p-value = 0.174 Observemos la estructura del resultado de un chisq.test. Nos da el valor del estadístico \\(X^2\\) (X-squared), el p-valor del contraste (p-value), y los grados de libertad de la distribución \\(\\chi^2\\) que ha usado para calcularlo (df). En este caso, el p-valor es 0.174, y por lo tanto no podemos rechazar que el dado esté equilibrado: en más de 1 de cada 6 secuencias de 40 lanzamientos de un dado equilibrado obtenemos un valor de \\(X^2\\) tan grande como el de esta secuencia, o mayor. Queremos remarcar que, como R no sabe si hemos estimado o no parámetros, el número de grados de libertad que usa chisq.test es simplemente el número de clases menos 1. Si no hemos estimado parámetros para calcular las probabilidades teóricas, ya va bien, pero si lo hemos hecho y por lo tanto el número de grados de libertad no es el adecuado, tendremos que calcular el p-valor correcto a partir del valor del estadístico. Veremos varios ejemplos más adelante. El resultado de un chisq.test es una list, de la que podemos extraer directamente la información que deseemos con los sufijos adecuados. En concreto, podemos obtener el valor del estadístico \\(X^2\\) con el sufijo $statistic, los grados de libertad con el sufijo $parameter y el p-valor con el sufijo $p.value. chisq.test(freqs)$statistic ## X-squared ## 7.7 chisq.test(freqs)$parameter ## df ## 5 chisq.test(freqs)$p.value ## [1] 0.173563 Imaginemos ahora que, en vez de lanzar el dado 40 veces, lo lanzamos 20 veces, y obtenemos los resultados siguientes: Resultados Frecuencias 1 4 2 2 3 3 4 2 5 3 6 6 ¿Hay evidencia de que el dado esté trucado? Ahora la muestra no es grande y las frecuencias esperadas son todas 20/6=3.333, menores que 5. Por tanto, el p-valor del test \\(\\chi^2\\) que se obtiene usando una distribución \\(\\chi^2_5\\) no tiene por qué tener ningún significado. En una situación como ésta es cuando conviene usar el parámetro simulate.p.value. Vamos a pedir a R que simule 5000 veces el experimento de lanzar 20 veces un dado equilibrado, y que calcule como p-valor la proporción de simulaciones en las que el estadístico \\(X^2\\) haya dado un valor mayor o igual que el que se obtiene con nuestra muestra. freqs2=c(4,2,3,2,3,6) chisq.test(freqs2, simulate.p.value=TRUE, B=5000) ## ## Chi-squared test for given probabilities with simulated p-value ## (based on 5000 replicates) ## ## data: freqs2 ## X-squared = 3.4, df = NA, p-value = 0.7 Resulta que en un 70% de las simulaciones el valor de \\(X^2\\) ha sido mayor o igual que el de nuestra muestra, 3.4. Por lo tanto, nuestra muestra entra dentro de lo normal para un dado equilibrado, por lo que no hay evidencia de que el dado esté trucado. Como este p-valor se basa en simulaciones, en cada aplicación del test el p-valor puede dar resultados diferentes, pero en general la conclusión es robusta si se toma un número suficiente de simulaciones. chisq.test(freqs2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.70126 chisq.test(freqs2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.717656 chisq.test(freqs2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.708858 Como vemos, los p-valores son todos similares. Por curiosidad, ¿qué p-valor da el test \\(\\chi^2\\) usando la distribución de \\(\\chi^2_5\\)? chisq.test(freqs2)$p.value ## Warning in chisq.test(freqs2): Chi-squared approximation may be incorrect ## [1] 0.63857 El p-valor no es muy diferente y la conclusión en este caso sería la misma, pero fijaos en el mensaje de advertencia: para la muestra dada, la aproximación de la distribución de \\(X^2\\) mediante una \\(\\chi^2\\) no tiene por qué ser correcta. Ejemplo 6.4 Vamos a estudiar las frecuencias de los nucleótidos en una cadena de ADN, y contrastar si aparecen los cuatro con la misma probabilidad o no. En este caso, el espacio muestral son los cuatro nucleótidos: adenina (A), citosina (C), guanina (G) y timina (T). Identificaremos una cadena de ADN con un vector de letras a, c, g y t. Si llamamos \\(p_a\\), \\(p_c\\), \\(p_g\\) y \\(p_t\\) a las probabilidades de aparición de estas letras, el contraste que queremos realizar es \\[ \\left\\{\\begin{array}{l} H_0 : p_a=p_c=p_g=p_t=0.25\\\\ H_1: \\mbox{Algunos nucleótidos son más probables que otros} \\end{array} \\right. \\] Vamos a analizar una cadena de ADN “de verdad”, extraída de la base de datos GenBank. Para ello, utilizaremos el paquete ape, que incorpora una función read.GenBank que permite leer secuencias de genes incluidas en esta base de datos y convertirlas en vectores de letras a, c, g y t. En concreto, si la aplicamos al número de acceso (accession number) de una secuencia (entrado entre comillas, ya que es una palabra) y usamos el parámetro as.character=TRUE, nos devuelve dicha secuencia como un vector de letras junto con otra información sobre la secuencia. En este ejemplo, nos vamos a interesar por el gen que codifica la mioglobina humana, que es una proteína relativamente pequeña constituida por una sola cadena polipeptídica de 153 aminoácidos. Su número de acceso es AH002877.2. El código siguiente lee este gen y lo guarda en un objeto. library(ape) myoglobin=read.GenBank(&quot;AH002877.2&quot;, as.character=TRUE) Consultemos cómo es el objeto donde hemos guardado esta secuencia: str(myoglobin) ## List of 1 ## $ AH002877.2: chr [1:6889] &quot;g&quot; &quot;t&quot; &quot;a&quot; &quot;c&quot; ... ## - attr(*, &quot;species&quot;)= chr &quot;Homo_sapiens&quot; Vemos que se trata una list formada por una sola componente, el vector de bases, y un atributo. Vamos a extraer el vector, para poder trabajar con él. La manera más sencilla es añadiendo a la list el sufijo [[1]]: myoglobin=myoglobin[[1]] myoglobin[1:10] ## [1] &quot;g&quot; &quot;t&quot; &quot;a&quot; &quot;c&quot; &quot;t&quot; &quot;g&quot; &quot;t&quot; &quot;a&quot; &quot;t&quot; &quot;t&quot; Nos preguntamos si podemos aceptar que en esta secuencia las cuatro bases aparecen de manera equiprobable. Para responder esta pregunta, calculamos las frecuencias de las letras con la función table y aplicamos el test \\(\\chi^2\\) a los resultados. table(myoglobin) ## myoglobin ## a c g n t ## 1718 1453 2019 200 1499 Aparecen valores n, que corresponden a bases no resueltas. Vamos borrarlas de la secuencia: myoglobin=myoglobin[myoglobin!=&quot;n&quot;] table(myoglobin) ## myoglobin ## a c g t ## 1718 1453 2019 1499 Ahora ya estamos en condiciones de llevar a cabo el contraste deseado con la función chisq.test. Puesto que miramos si todos los resultados aparecen con la misma probabilidad, no hace falta especificar el vector p de probabilidades. chisq.test(table(myoglobin)) ## ## Chi-squared test for given probabilities ## ## data: table(myoglobin) ## X-squared = 119.8, df = 3, p-value &lt;2e-16 El p-valor es prácticamente 0, podemos rechazar que las cuatro bases aparezcan con la misma probabilidad: las diferencias entre las frecuencias de los cuatro aminoácidos son lo suficientemente grandes como para hacer inverosímil que se hayan generado con la misma probabilidad. Ejemplo 6.5 Siguiendo con el ejemplo anterior, vamos a contrastar ahora si las bases siguen una distribución en la que A y G aparecen un 25% de veces más que C y T. Usaremos p=c(1.25,1,1.25,1) para especificar estas proporciones. chisq.test(table(myoglobin),p=c(1.25,1,1.25,1)) ## Error in chisq.test(table(myoglobin), p = c(1.25, 1, 1.25, 1)): probabilities must sum to 1. ¡Vaya! Nos habíamos olvidado de especificar rescale.p=TRUE, para poder entrar como p un vector proporcional a las probabilidades. chisq.test(table(myoglobin),p=c(1.25,1,1.25,1),rescale.p=TRUE)$p.value ## [1] 1.30045e-05 De nuevo, tenemos que rechazar la hipótesis nula. Ejemplo 6.6 Vamos a realizar otro experimento con la cadena del gen de la mioglobina. Este gen consta de tres exones. El primero corresponde al número de acceso M10090.1. Vamos a comparar si la frecuencia de bases en este exón es similar a la de la cadena total, que hará de distribución teórica. Para ello, leemos el exón y lo guardamos en una cadena exon1=read.GenBank(&quot;M10090.1&quot;, as.character=TRUE)[[1]] Calculemos la tabla de frecuencias relativas de las bases en la cadena completa probs.tot=prop.table(table(myoglobin)) round(probs.tot,3) ## myoglobin ## a c g t ## 0.257 0.217 0.302 0.224 Vamos a usar esta tabla como parámetro p de la función chisq.test: chisq.test(table(exon1),p=probs.tot)$p.value ## [1] 9.78458e-06 El p-valor es muy pequeño, y por lo tanto podemos rechazar que en este exón las bases aparezcan con la misma probabilidad que en la cadena total. Ejemplo 6.7 Ahora vamos a llevar a cabo el experimento siguiente. Queremos contrastar si la aparición de pares “cg” en la mioglobina humana es aleatoria, en el sentido de que se debe simplemente a las apariciones al azar de sus dos bases, o si por el contrario hay algún otro mecanismo que los produce. Para ello, tomaremos la secuencia completa de la mioglobina humana, que tenemos almacenada en myoglobin, y repetiremos 100 veces el proceso siguiente: extraemos una muestra aleatoria simple de 20 posiciones de la secuencia y contamos cuántas de ellas contienen el par de bases “cg”. Luego contrastaremos si la muestra así obtenida proviene de una distribución binomial con la probabilidad de aparición de “cg” como si las dos bases fueran independientes. Fijamos la semilla de aleatoriedad para que se pueda reproducir el experimento.5 El código, que luego explicamos, y el resultado son los siguientes: cg.in.sample=function(x,S){#x un vector, S vector de índices length(which(x[S]==&quot;c&quot; &amp; x[S+1]==&quot;g&quot;)) } set.seed(1700) muestra=replicate(100, cg.in.sample(myoglobin, sample(1:(length(myoglobin)-1), 20,replace=TRUE))) muestra ## [1] 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 ## [38] 0 0 0 0 0 0 0 1 1 0 0 2 0 2 0 0 0 1 0 1 0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 ## [75] 0 0 2 0 1 2 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 La función cg.in.sample que hemos definido toma un vector x y un vector de índices S y cuenta el número de índices s de S en los que x[s] es c y x[s+1] es g. Entonces, con el replicate, hemos repetido 100 veces el proceso de extraer una muestra aleatoria simple de 20 índices de myoglobin (excluyendo el último, para que sean posiciones donde empieza un par de letras) y aplicar la función cg.in.sample a myoglobin y a este vector de índices. Queremos determinar si esta muestra sigue una distribución binomial. En concreto, vamos a plantear tres casos de esta pregunta: Con probabilidad de aparición de la pareja “cg” 0.25·0.25=0.0625, que correspondería al hecho de que las dos bases aparecieran de manera equiprobable e independiente. Con probabilidad de aparición de la pareja “cg” el producto de las frecuencias relativas de las bases en la secuencia global, que correspondería al hecho de que las dos bases aparecieran de manera independiente, pero no equiprobable sino con sus probabilidades dentro de la secuencia de la mioglobina. Estimando el valor “real” de \\(p\\), lo que correspondería al hecho de que su probabilidad de aparición no tuviera nada que ver con las probabilidades individuales de sus dos bases. Empezamos con el primer caso. Calculemos las frecuencias con las que aparecen los diferentes resultados en la muestra: table(muestra) ## muestra ## 0 1 2 ## 76 19 5 Y las frecuencias esperadas con las que deberían aparecer si siguieran una distribución B(20,0.0625): round(dbinom(0:20,20,0.0625)*100,2) ## [1] 27.51 36.67 23.23 9.29 2.63 0.56 0.09 0.01 0.00 0.00 0.00 ## [12] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Las frecuencias esperadas a partir de 4 son inferiores a 5 (recordad que la primera frecuencia corresponde al 0), y además su suma no llega a 5. Por lo tanto, vamos a agrupar en una sola clase los resultados mayores o iguales que 3. La nueva tabla de frecuencias esperadas es: round(c(dbinom(0:2,20,0.0625),1-pbinom(2,20,0.0625))*100,2) ## [1] 27.51 36.67 23.23 12.59 Ahora tenemos dos opciones: o bien tomamos como resultados posibles “0”, “1”, “2” y “3 o más”, en cuyo caso contaríamos que hemos observado 0 veces este último resultado en nuestra muestra, o bien tomamos como resultados posibles “0”, “1”, y “2 o más”, que se corresponde con los valores observados. Como norma general, es recomendable usar el mayor número de clases posible. Por consiguiente, vamos a optar por la primera estrategia: 4 clases y no 3. Por lo tanto, hay que añadir a la tabla de frecuencias de la muestra un 0 en la columna correspondiente a 3 o más observaciones. freq.obs=c(table(muestra),0) prob.teor=c(dbinom(0:2,20,0.0625),1-pbinom(2,20,0.0625)) chisq.test(freq.obs,p=prob.teor)$p.value ## [1] 4.91134e-26 El p-valor es prácticamente 0, por lo que podemos concluir que la muestra no sigue una distribución B(20,0.0625): las apariciones de los pares “cg” no se explican por la aparición de sus dos bases de manera independiente y equiprobable. ¿Hubiera variado la conclusión si hubiéramos optado por solo considerar tres clases? freq.obs=table(muestra) prob.teor=c(dbinom(0:1,20,0.0625),1-pbinom(1,20,0.0625)) chisq.test(freq.obs,p=prob.teor)$p.value ## [1] 6.70877e-27 El p-valor es prácticamente el mismo, la conclusión es la misma. Pasemos al segundo caso de nuestro problema. Vamos a calcular la frecuencia relativa de “c” y “g” en la secuencia completa de la mioglobina humana y tomaremos como probabilidad \\(p\\) el producto de ambas frecuencias relativas. prop.table(table(myoglobin)) ## myoglobin ## a c g t ## 0.256840 0.217222 0.301839 0.224099 p=prod(prop.table(table(myoglobin))[2:3]) p ## [1] 0.0655661 Calculemos ahora las frecuencias esperadas tomando esta \\(p\\): round(dbinom(0:20,20,p)*100,2) ## [1] 25.76 36.15 24.10 10.15 3.03 0.68 0.12 0.02 0.00 0.00 0.00 ## [12] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Vamos a tener que agrupar de nuevo en una sola clase los resultados mayores o iguales que 3. freq.obs=c(table(muestra),0) prob.teor=c(dbinom(0:2,20,p),1-pbinom(2,20,p)) chisq.test(freq.obs,p=prob.teor)$p.value ## [1] 4.02804e-29 Obtenemos de nuevo un valor prácticamente 0: las apariciones de los pares “cg” no se explican por la aparición de sus dos bases de manera independiente. Finalmente, vamos a estimar el parámetro \\(p\\). La binomial es otra de las distribuciones no cubiertas por fitdistr, por lo que tendremos que apelar a lo que sabemos de teoría para hacerlo. Como el valor esperado de una variable aleatoria \\(X\\sim B(n,p)\\) es \\(np\\), estimaremos \\(p\\) mediante \\(\\overline{X}/n\\). De hecho, éste es el estimador máximo verosímil de \\(p\\) cuando \\(n\\) es conocida. p.estim=mean(muestra)/20 p.estim ## [1] 0.0145 Repetimos el proceso: calculemos las frecuencias teóricas round(dbinom(0:20,20,p.estim)*100,2) ## [1] 74.67 21.97 3.07 0.27 0.02 0.00 0.00 0.00 0.00 0.00 0.00 ## [12] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 En este caso, hemos de agrupar los resultados en “0”, “1” y “2 o más”, para que las frecuencias teóricas sean mayores que 5. Coincide con los diferentes valores observados en la muestra. freq.obs=table(muestra) prob.teor=c(dbinom(0:1,20,p.estim),1-pbinom(1,20,p.estim)) chisq.test(freq.obs,p=prob.teor) ## ## Chi-squared test for given probabilities ## ## data: freq.obs ## X-squared = 1.226, df = 2, p-value = 0.542 ¡Cuidado! Este p-valor no es el correcto. Hemos estimado un parámetro, pero R no lo sabe. Por lo tanto tenemos que bajar en 1 los grados de libertad y calcular el p-valor a mano, mediante \\[ P(\\chi_1^2\\geq X^2)=1-P(\\chi_1^2\\leq 1.226) \\] 1-pchisq(chisq.test(freq.obs,p=prob.teor)$statistic,1) ## X-squared ## 0.268154 El p-valor es grande. Por lo tanto, no podemos rechazar la hipótesis nula de que las apariciones de “cg” en muestras aleatorias de 20 posiciones sigan una ley binomial de parámetro \\(p=0.0145\\). Que es, por otro lado, lo que debería pasar si las “cg” estuvieran repartidas de manera aleatoria en la secuencia original, por lo que no podemos rechazar esto último. La conclusión es, por lo tanto, que aceptamos que las “cg” aparecen distribuidas de manera aleatoria en la secuencia de la mioglobina humana, pero tenemos evidencia estadísticamente significativa de que las “c” y las “g” que las forman no aparecen de manera independiente. 6.3 El test \\(\\chi^2\\) para distribuciones continuas El procedimiento de contraste de bondad de ajuste mediante el test \\(\\chi^2\\) para variables continuas tiene la particularidad de que es necesario un paso preliminar que consiste en definir los intervalos de clase para los que realizaremos el conteo de las frecuencias observadas. El proceso es similar al que estudiamos en la Lección ?? de la primera parte del curso para dibujar histogramas. Por lo tanto, necesitaremos definir unos intervalos de clase para el conteo de frecuencias absolutas observadas, y con las funciones cut y table obtendremos las frecuencias observadas de estas clases en la muestra de la variable continua. Para obtener los intervalos podemos seguir dos estrategias razonables: reutilizar los generados por la función hist, o dividir el rango de la variable en un número prefijado \\(k\\) de intervalos de amplitud fija. Vamos a ver en detalle un ejemplo de cada tipo. Ejemplo 6.8 Vamos a contrastar si las longitudes de los sépalos de las plantas iris recogidas en la tabla de datos iris siguen una distribución normal. Recordaréis que el Q-Q-plot de estas longitudes que mostrábamos en la Figura 6.3 mostraba evidencia de que no la siguen. Primero vamos a estimar de nuevo la media y la desviación típica de la distribución de estas longitudes. iris.sl=iris$Sepal.Length mu=fitdistr(iris.sl,&quot;normal&quot;)$estimate[1] sigma=fitdistr(iris.sl,&quot;normal&quot;)$estimate[2] round(c(mu,sigma),3) ## mean sd ## 5.843 0.825 En este ejemplo, usaremos los intervalos en los que la función hist agrupa por defecto estos datos. h=hist(iris.sl, plot=FALSE) h$breaks ## [1] 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 Ahora se nos presenta el problema de que los intervalos que definen estos puntos de corte (breaks) no cubren toda la recta real, que es el espacio muestral de una variable aleatoria normal. Así que tenemos que reemplazar los extremos de este vector de breaks por los límites del espacio muestral de la variable, que en este caso son \\(-\\infty\\) e \\(\\infty\\). breaks2=h$breaks breaks2[1]=-Inf #Cambiamos el primer elemento por -Infinito breaks2[length(breaks2)]=Inf #Cambiamos el último elemento por Infinito breaks2 ## [1] -Inf 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Inf Ahora podemos calcular las frecuencias de la muestra en los intervalos definidos por estos puntos de corte: freq.obs=table(cut(iris.sl,breaks=breaks2)) freq.obs ## ## (-Inf,4.5] (4.5,5] (5,5.5] (5.5,6] (6,6.5] (6.5,7] ## 5 27 27 30 31 18 ## (7,7.5] (7.5, Inf] ## 6 6 Ahora calcularemos las probabilidades teóricas. Para cada intervalo \\((x,y]\\) en los que hemos cortado la recta real, tenemos que calcular \\(P(x &lt; X\\leq y)= P(X\\leq y)-P( X\\leq x)\\), para lo que usaremos expresiones de la forma pnorm(y,mu,sigma)-pnorm(x,mu,sigma). Definimos dos vectores que nos den los extremos izquierdo y derecho de cada intervalo. extremo.izq=breaks2[-length(breaks2)] extremo.der=breaks2[-1] extremo.izq ## [1] -Inf 4.5 5.0 5.5 6.0 6.5 7.0 7.5 extremo.der ## [1] 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Inf Ahora podemos calcular las probabilidades teóricas y las frecuencias esperadas de todos los intervalos de golpe. La probabilidades teóricas son: probs.teor=pnorm(extremo.der,mu,sigma)-pnorm(extremo.izq,mu,sigma) probs.teor ## [1] 0.0517955 0.1016307 0.1852753 0.2365772 0.2116091 0.1325812 0.0581747 ## [8] 0.0223563 Las frecuencias esperadas son: freq.esp=probs.teor*length(iris.sl) round(freq.esp,3) ## [1] 7.769 15.245 27.791 35.487 31.741 19.887 8.726 3.353 La frecuencia esperada de la última clase es inferior a 5, así que vamos a fundirla con la penúltima y así la clase resultante tendrá una frecuencia esperada superior a 5. k=length(probs.teor) #Nuevas probabilidades teóricas: probs.teor2=c(probs.teor[1:(k-2)],sum(probs.teor[(k-1):k])) #Nuevas frecuencias observadas: freq.obs2=c(freq.obs[1:(k-2)],sum(freq.obs[(k-1):k])) chisq.test(freq.obs2,p=probs.teor2) ## ## Chi-squared test for given probabilities ## ## data: freq.obs2 ## X-squared = 11.12, df = 6, p-value = 0.08475 Recordemos que el p-valor obtenido no es el correcto: como hemos estimado dos parámetros, lo tenemos que calcular con una \\(\\chi^2\\) con 4 grados de libertad (dos menos de los que ha usado chisq.test): test.iris=chisq.test(freq.obs2,p=probs.teor2) 1-pchisq(test.iris$statistic,test.iris$parameter-2) ## X-squared ## 0.0252518 El p-valor es inferior a 0.05, por tanto obtenemos evidencia de que la muestra no proviene de una población normal, es decir, de que las longitudes de los sépalos de las flores iris no siguen una ley normal. Ejemplo 6.9 Vamos a repetir el estudio del ejemplo anterior, pero ahora calculando a mano los intervalos. En general, el número de intervalos debe ser suficiente para cubrir toda la forma de la distribución, pero tampoco conviene que haya muchos para evitar frecuencias esperadas pequeñas que obliguen a agrupar intervalos. Para una distribución normal se recomienda tomar entre 5 y 15 intervalos. Otra posibilidad es decidir el número de intervalos con alguna de las reglas explicadas en la Lección ?? de la primera parte del curso. En nuestro ejemplo, vamos a usar 10 intervalos. Para calcularlos, tomamos el máximo y el mínimo de las observaciones, los restamos y dividimos por el número de intervalos (y, si fuera necesario, redondearíamos adecuadamente). Ampl=(max(iris.sl)-min(iris.sl))/10 Ampl ## [1] 0.36 Los extremos de los intervalos en los que dividimos la muestra forman la secuencia que empieza en el mínimo y va sumando la amplitud hasta definir los \\(k=10\\) intervalos. Luego hay que adecuar los dos extremos para que cubran el dominio de la densidad de la distribución teórica, en nuestro caso toda la recta real. breaks=min(iris.sl)+Ampl*(0:10) breaks ## [1] 4.30 4.66 5.02 5.38 5.74 6.10 6.46 6.82 7.18 7.54 7.90 breaks2=breaks breaks2[1]=-Inf breaks2[length(breaks2)]=Inf breaks2 ## [1] -Inf 4.66 5.02 5.38 5.74 6.10 6.46 6.82 7.18 7.54 Inf Calculemos, como en el ejemplo anterior, las frecuencias observadas, las probabilidades teóricas y las frecuencias esperadas. frec.obs=table(cut(iris.sl,breaks=breaks2)) frec.obs ## ## (-Inf,4.66] (4.66,5.02] (5.02,5.38] (5.38,5.74] (5.74,6.1] (6.1,6.46] ## 9 23 14 27 22 20 ## (6.46,6.82] (6.82,7.18] (7.18,7.54] (7.54, Inf] ## 18 6 5 6 extremo.izq=breaks2[-length(breaks2)] extremo.der=breaks2[-1] prob.teor=pnorm(extremo.der,mu,sigma)-pnorm(extremo.izq,mu,sigma) frec.esp=round(prob.teor*length(iris.sl),2) frec.esp ## [1] 11.37 12.51 19.20 24.44 25.79 22.56 16.37 9.85 4.91 2.99 Agruparemos las frecuencias de los dos últimos intervalos y aplicaremos el test \\(\\chi^2\\) con el número adecuado de grados de libertad: frec.obs2=c(frec.obs[1:8], sum(frec.obs[9:10])) prob.teor2=c(prob.teor[1:8], sum(prob.teor[9:10])) test.iris.2=chisq.test(frec.obs2,p=prob.teor2) 1-pchisq(test.iris.2$statistic, test.iris.2$parameter-2) ## X-squared ## 0.0227734 El p-valor es de nuevo inferior a 0.05: volvemos a obtener evidencia significativa de que la muestra no proviene de una población normal. 6.4 El test de Kolgomorov-Smirnov El test de Kolgomorov-Smirnov (K-S) es un test genérico para contrastar la bondad de ajuste a distribuciones continuas. Se puede usar con muestras pequeñas (se suele recomendar 5 elementos como el tamaño mínimo para que el resultado sea significativo), pero la muestra no puede contener valores repetidos: si los contiene, la distribución del estadístico de contraste bajo la hipótesis nula no es la que predice la teoría sino que solo se aproxima a ella, y por lo tanto los p-valores que se obtienen son aproximados. Hay que tener en cuenta que el test K-S realiza un contraste en el que la hipótesis nula es que la muestra proviene de una distribución continua completamente especificada. Es decir, no sirve para contrastar si la muestra proviene, pongamos, de “alguna” distribución normal, sino solo para contrastar si proviene de una distribución normal con una media y una desviación típica concretas. Así pues, si queremos contrastar que la muestra proviene de alguna distribución de una familia concreta y estimamos sus parámetros a partir de la muestra, el test K-S solo nos permite rechazar o no la hipótesis de que la muestra proviene de la distribución de esa familia con exactamente esos parámetros. Por lo tanto, si el resultado es rechazar la hipótesis nula, esto no excluye que la muestra provenga de una distribución de la misma familia con otros parámetros. En la próxima sección veremos algunos tests que permiten contrastar, en general, si una muestra proviene de alguna distribución normal. La función básica para realizar el test K-S es ks.test. Su sintaxis básica para una muestra es ks.test(x, y, parámetros) donde: x es la muestra de una variable continua. y puede ser un segundo vector, y entonces se contrasta si ambos vectores han sido generados por la misma distribución continua, o el nombre de la función de distribución (empezando con p) que queremos contrastar, entre comillas; por ejemplo \"pnorm\" para la distribución normal. Los parámetros de la función de distribución si se ha especificado una; por ejemplo mean=0, sd=1 para una distribución normal estándar. Ejemplo 6.10 Efectuemos el test de Kolmogorov-Smirnov para contrastar si las longitudes de sépalos de flores iris siguen una distribución normal de media y desviación típica sus estimaciones máximo verosímiles a partir la muestra iris.sl. Recordemos que tenemos guardados de los dos últimos ejemplos los valores de estas estimaciones en las variables mu y sigma: round(c(mu,sigma),3) ## mean sd ## 5.843 0.825 ks.test(iris.sl, &quot;pnorm&quot;, mean=mu, sd=sigma) ## Warning in ks.test(iris.sl, &quot;pnorm&quot;, mean = mu, sd = sigma): ties should ## not be present for the Kolmogorov-Smirnov test ## ## One-sample Kolmogorov-Smirnov test ## ## data: iris.sl ## D = 0.08945, p-value = 0.181 ## alternative hypothesis: two-sided Obtenemos un p-valor de 0.181, que no nos permite rechazar la hipótesis de que siguen una ley N(5.843, 0.825). Pero R nos avisa de que hay empates. ¿Hay muchos? Vamos a calcular su frecuencia. La función unique aplicada a un vector nos da el vector de sus elementos sin repeticiones. De esta manera podemos saber cuántos elementos diferentes hay en un vector, y por consiguiente también cuántas repeticiones. length(unique(iris.sl)) ## [1] 35 1-length(unique(iris.sl))/length(iris.sl) ## [1] 0.766667 Por tanto, el vector (de 150 entradas) de longitudes de sépalos solo tiene 35 valores diferentes. El resto, un 76.67%, son valores repetidos. Hay muchos empates, y el resultado de este test en este caso es poco fiable Como hemos comentado, el test K-S también se puede usar para contrastar si dos muestras se han obtenido de poblaciones con la misma distribución continua. Para hacerlo, se ha de aplicar la función ks.test a las dos muestras. Ejemplo 6.11 La tabla de datos Salaries del paquete car contiene información sobre los sueldos de 397 profesores de una universidad norteamericana en el curso 2008-09. Démosle un vistazo. library(car) str(Salaries) ## &#39;data.frame&#39;:\t397 obs. of 6 variables: ## $ rank : Factor w/ 3 levels &quot;AsstProf&quot;,&quot;AssocProf&quot;,..: 3 3 1 3 3 2 3 3 3 3 ... ## $ discipline : Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ yrs.since.phd: int 19 20 4 45 40 6 30 45 21 18 ... ## $ yrs.service : int 18 16 3 39 41 6 23 45 20 18 ... ## $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 2 2 2 2 2 1 ... ## $ salary : int 139750 173200 79750 115000 141500 97000 175000 147765 119250 129000 ... La variable sex nos da el sexo del profesor y la variable salary su sueldo anual en dólares. Queremos contrastar si los sueldos de hombres y mujeres siguen la misma distribución. Para ello, vamos a suponer que provienen de distribuciones continuas y usaremos el test K-S. Primero miraremos si hay muchos empates. sal.female=Salaries[Salaries$sex==&quot;Female&quot;,]$salary #Salarios de mujeres sal.male=Salaries[Salaries$sex==&quot;Male&quot;,]$salary #Salarios de hombres 1-length(unique(sal.female))/length(sal.female) #Proporción de salarios de mujeres repetidos ## [1] 0.05128205 1-length(unique(sal.male))/length(sal.male) #Proporción de salarios de hombres repetidos ## [1] 0.05027933 1-length(unique(Salaries$salary))/length(Salaries$salary) #Proporción global de salarios repetidos ## [1] 0.06549118 Las repeticiones en cada lista significan alrededor del 5% de los datos, y en total un 6.5%. No son muchas, así que vamos a arriesgarnos con el test K-S. ks.test(sal.male,sal.female) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: sal.male and sal.female ## D = 0.2472, p-value = 0.0271 ## alternative hypothesis: two-sided El p-valor pequeño nos permite rechazar que los salarios de hombres y mujeres sigan la misma distribución. Pero no nos paremos aquí. Si dibujamos un boxplot (véase la Figura 6.4) de los salarios según el sexo, observaremos que los sueldos de los hombres tienen mayor mediana y variabilidad que los de las mujeres, incluyendo algunos valores atípicos grandes (¿el rector y otros altos cargos académicos?). boxplot(salary~sex, data=Salaries, main=&quot;&quot;) Figura 6.4: Boxplot de sueldos según el sexo en la tabla de datos Salaries Si cancelamos este efecto, estandarizando las muestras, ¿siguen saliendo distribuciones diferentes? Para estandarizar las muestras usaremos la función scale, que explicaremos con más detalle en la Lección 8. Esta función, aplicada tal cual a un vector, le resta su media y divide el resultado por su desviación típica muestral. ks.test(scale(sal.male),scale(sal.female)) ## Warning in ks.test(scale(sal.male), scale(sal.female)): p-value will be ## approximate in the presence of ties ## ## Two-sample Kolmogorov-Smirnov test ## ## data: scale(sal.male) and scale(sal.female) ## D = 0.139, p-value = 0.505 ## alternative hypothesis: two-sided Al estandarizar, ya no tenemos evidencia de que provengan de distribuciones diferentes. Es decir, podemos aceptar que sus valores tipificados siguen la misma distribución. 6.5 Tests de normalidad Existen algunos tests específicos de normalidad que permiten contrastar si una muestra proviene de alguna distribución normal. El más conocido es el test de normalidad de Kolmogorov-Smirnov-Lilliefors (K-S-L). Se trata de una variante del test K-S, y se puede realizar aplicando a la muestra la función lillie.test del paquete nortest. Vamos a usar el test K-S-L para contrastar si las longitudes de los sépalos de las iris siguen una ley normal. library(nortest) iris.sl=iris$Sepal.Length lillie.test(iris.sl) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: iris.sl ## D = 0.08865, p-value = 0.00579 El p-valor es muy pequeño, y nos permite rechazar que la muestra provenga de una población normal. La ventaja del test K-S-L es que es muy conocido, ya que es una variante del K-S (incluso usa el mismo estadístico), pero tiene un inconveniente: aunque es muy sensible a las diferencias entre la muestra y la distribución teórica alrededor de sus valores medios, le cuesta detectar diferencias prominentes en un extremo u otro de la distribución. Esto afecta su potencia. Por ejemplo, sabemos que una t de Student se parece bastante a una normal estándar, pero su densidad es algo más aplanada y hace que en los dos extremos esté por encima de la de la normal. Al test K-S-L le cuesta detectar esta discrepancia, como podemos ver en el siguiente ejemplo: set.seed(100) x=rt(50,3) #Una muestra de una t de Student con 3 g.l. lillie.test(x) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: x ## D = 0.1033, p-value = 0.201 Este inconveniente del test K-S-L lo resuelve el test de normalidad de Anderson-Darling (A-D). Para realizarlo podemos usar la función ad.test del paquete nortest. Encontraréis los detalles del estadístico que usa en la Ayuda de la función. ad.test(iris.sl) ## ## Anderson-Darling normality test ## ## data: iris.sl ## A = 0.8892, p-value = 0.0225 De nuevo obtenemos un p-valor muy pequeño. Veamos ahora que este test sí que detecta que la muestra anterior de una t de Student con 3 grados de libertad no proviene de una normal: set.seed(100) x=rt(50,3) ad.test(x) ## ## Anderson-Darling normality test ## ## data: x ## A = 1.166, p-value = 0.00433 Un inconveniente común a los tests K-S-L y A-D es que, si bien pueden usarse con muestras pequeñas (pongamos de más de 5 elementos), se comportan mal con muestras grandes, de varios miles de elementos. En muestras de este tamaño, cualquier pequeña divergencia de la normalidad se magnifica y en estos dos tests aumenta la probabilidad de errores de tipo I. Un test que resuelve este problema es el de Shapiro-Wilk (S-W), implementado en la función shapiro.test de la instalación básica de R. Este test es importante, porque un experimento reciente ha mostrado evidencia significativa de que su potencia es mayor que la de los tests anteriores.6 De nuevo, los detalles del estadístico que usa los encontraréis en la Ayuda de la función. shapiro.test(iris.sl) ## ## Shapiro-Wilk normality test ## ## data: iris.sl ## W = 0.9761, p-value = 0.0102 set.seed(100) x=rt(50,3) shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.8949, p-value = 0.000329 Un último inconveniente que afecta a todos los tests explicados hasta ahora es el de los empates. Sus estadísticos tienen las distribuciones que se usan para calcular los p-valores cuando la muestra no tiene datos repetidos, y por lo tanto, si hay muchos, el p-valor puede no tener ningún significado. De los tres, el menos sensible a repeticiones es el S-W, pero si hay muchas es conveniente usar un test que no sea sensible a ellas, como por ejemplo el test omnibus de D’Agostino-Pearson. Este test se encuentra implementado en la función dagoTest del paquete fBasics, y lo que hace es cuantificar lo diferentes que son la asimetría y la curtosis de la muestra (dos parámetros estadísticos relacionados con la forma de la gráfica de la función de densidad muestral) respecto de los esperados en una distribución normal, y resume esta discrepancia en un p-valor con el significado usual. library(fBasics) dagoTest(iris.sl) ## ## Title: ## D&#39;Agostino Normality Test ## ## Test Results: ## STATISTIC: ## Chi2 | Omnibus: 5.7356 ## Z3 | Skewness: 1.5963 ## Z4 | Kurtosis: -1.7853 ## P VALUE: ## Omnibus Test: 0.05682 ## Skewness Test: 0.1104 ## Kurtosis Test: 0.07421 ## ## Description: ## Wed Mar 18 09:28:34 2020 by user: El p-valor relevante es el del “Omnibus test”, en este caso 0.0568 cae en la zona de penumbra. Queremos hacer una última advertencia en esta sección. Aunque los tests que hemos explicado se pueden aplicar a muestras pequeñas, es muy difícil rechazar la normalidad de una muestra muy pequeña. Por ejemplo, una muestra de 10 valores escogidos con distribución uniforme entre 0 y 5 pasa holgadamente todos los tests de normalidad (salvo el de D’Agostino-Pearson, que requiere una muestra de al menos 20 elementos): set.seed(100) x=runif(10,0,5) lillie.test(x) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: x ## D = 0.1459, p-value = 0.79 ad.test(x) ## ## Anderson-Darling normality test ## ## data: x ## A = 0.1663, p-value = 0.912 shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.9803, p-value = 0.967 dagoTest(x) ## Error in .omnibus.test(x): sample size must be at least 20 6.6 Guía rápida qqPlot, del paquete car, sirve para dibujar un Q-Q-plot de una muestra contra una distribución teórica. Sus parámetros principales son: distribution: el nombre de la familia de distribuciones, entre comillas. Los parámetros de la distribución: mean para la media, sd para la desviación típica, df para los grados de libertad, etc. Los parámetros usuales de plot. chisq.test sirve para realizar tests \\(\\chi^2\\) de bondad de ajuste. Sus parámetros principales son: p: el vector de probabilidades teóricas. rescale.p: igualado a TRUE, indica que los valores de p no son probabilidades, sino sólo proporcionales a las probabilidades. simulate.p.value: igualado a TRUE, R calcula el p-valor mediante simulaciones. B: en este último caso, permite especificar el número de simulaciones. ks.test realiza el test de Kolmogorov-Smirnov. Tiene dos tipos de uso: ks.test(x,y): contrasta si los vectores x e y han sido generados por la misma distribución continua. ks.test(x, \"distribución\", parámetros): contrasta si el vector x ha sido generado por la distribución especificada, que se ha de indicar con el nombre de la función de distribución de R (la que empieza con p). lillie.test, del paquete nortest, realiza el test de normalidad de Kolmogorov-Smirnov-Lilliefors. ad.test, del paquete nortest, realiza el test de normalidad de Anderson-Darling. shapiro.test realiza el test de normalidad de Shapiro-Wilk. dagoTest, del paquete fBasics, realiza el test ómnibus de D’Agostino-Pearson. 6.7 Ejercicios Modelo de test (1) Un determinado experimento tiene cinco resultados posibles: A, B, C, D, E. Lo repetimos un cierto número de veces y obtenemos 65 veces el resultado A, 95 veces el resultado B, 87 veces el resultado C, 70 veces el resultado D y 193 veces el resultado E. Realizad un test \\(\\chi^2\\) para contrastar si los resultados A, B, C y D tienen la misma probabilidad y E tiene el doble de probabilidad que cada uno de los otros resultados. Dad el p-valor del contraste (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decid (contestando SI, en mayúsculas y sin acento, o NO) si tendríamos que rechazar la hipótesis nula con un nivel de significación \\(\\alpha=0.05\\). Tenéis que dar las respuestas en este orden y separadas por un único espacio en blanco. (2) Queremos contrastar si una determinada variable sigue una distribución de Poisson. Hemos efectuado algunas observaciones y hemos obtenido 10 veces el resultado 0, 32 veces el resultado 1, 18 veces el resultado 2, 19 veces el resultado 3 y 6 veces el resultado 4. Tenéis que calcular el estimador máximo verosímil del parámetro \\(\\lambda\\) de una variable de Poisson que haya generado estas observaciones (redondeado a 3 cifras decimales, y sin ceros innecesarios a la derecha), calcular el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del test \\(\\chi^2\\) para determinar si la muestra sigue alguna distribución de Poisson habiendo estimado como su parámetro \\(\\lambda\\) este valor redondeado, y decir (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.1\\), tendríamos que rechazar la hipótesis nula de que esta muestra proviene de una variable aleatoria de Poisson. Dad las tres respuestas en este orden y separadas por un único espacio en blanco. (3) Queremos contrastar si una cierta variable sigue una distribución normal. Hemos efectuado 150 observaciones y hemos obtenido 9 veces un valor dentro de ]0,3], 27 veces un valor dentro de ]3,6], 51 veces un valor dentro de ]6,9], 46 veces un valor dentro de ]9,12] y 17 veces un valor dentro de ]12,15]. Tenéis que: calcular los estimadores máximo verosímiles del parámetro \\(\\mu\\) y del parámetro \\(\\sigma\\) de una variable normal que haya generado estas observaciones (ambos redondeados a 2 cifras decimales y sin ceros innecesarios a la derecha): calcular el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del test \\(\\chi^2\\) para contrastar si la muestra sigue alguna distribución normal, empleando estos valores estimados redondeados de los parámetros que habéis dado para especificar la distribución teórica; y decir (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.05\\), tendríamos que rechazar la hipótesis nula de que esta muestra proviene de una variable aleatoria normal. Dad las cuatro respuestas en este orden y separadas por un único espacio en blanco. (4) Generad una muestra aleatoria x de 25 valores de una distribución \\(\\chi^2\\) con 10 grados de libertad, fijando antes set.seed(2014), y aplicad el test de Kolmogorov-Smirnov para contrastar si x proviene de una distribución N(10,3.16). Dad el p-valor del test (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decid (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.05\\), tendríamos que rechazar la hipótesis nula de que esta muestra proviene de esta distribución. Tenéis que dar las dos respuestas en este orden y separadas por un único espacio en blanco. (5) Queremos contrastar si la muestra siguiente sigue una distribución normal: 4.6, 0.97, 0.3, 1.11, 2.16, 15.52, 1.13, 0.17, 0.64, 2.00. Dad el p-valor (redondeado a 3 cifras decimales y sin ceros innecesarios a la derecha) del test de Kolmogorov-Smirnov-Lilliefors para esta muestra y decid (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.05\\), tendríamos que rechazar la hipótesis nula de que esta muestra sigue una distribución normal. Tenéis que dar las dos respuestas en este orden y separadas por un único espacio en blanco. (6) Generad una muestra aleatoria x de 15 valores de una distribución normal con \\(\\mu=2\\) y \\(\\sigma=0.8\\) fijando antes set.seed(2014), y una muestra aleatoria y de 25 valores de una distribución exponencial de parámetro \\(1/\\lambda=0.5\\) fijando antes set.seed(1007). Aplicad el test de Kolmogorov-Smirnov para contrastar si x e y provienen de una misma distribución continua. Tenéis que dar el p-valor del test (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decir (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.05\\), tendríamos que rechazar la hipótesis nula de que estas dos muestras provienen de la misma distribución continua. Dad las dos respuestas en este orden y separadas por un único espacio en blanco. Problemas (1) La tabla siguiente da, por cada Comunidad Autónoma, su población según el censo de 2013 y el número de defunciones durante el primer semestre de 2014: Comunidad Población Defunciones Andalucía 8440300 34810 Aragón 1347150 7196 Asturias (Principado de) 1068165 6623 Balears (Illes) 1111674 4088 Canarias 2118679 7593 Cantabria 591888 3031 Castilla-La Mancha 2100998 14532 Castilla y León 2519875 9765 Cataluña 7553650 32383 Comunitat Valenciana 5113815 21614 Extremadura 1104004 5751 Galicia 2765940 15888 Madrid (Comunidad de) 6495551 22446 Murcia (Región de) 1472049 5330 Navarra (Comunidad Foral de) 644477 2846 País Vasco 2191682 10639 Rioja (La) 322027 1489 Ceuta 84180 252 Melilla 83679 234 Tenéis esta tabla de datos en el url https://raw.githubusercontent.com/AprendeR-UIB/AprendeR2privat/master/INE.txt. Efectuad un test \\(\\chi^2\\) para contrastar si la población por comunidades autónomas y el número de defunciones por comunidad autónoma tienen la misma distribución. (2) En los cursos introductorios de estadística se suele dar como ejemplo de variable aleatoria de Poisson los números de goles marcados en partidos de fútbol. Vamos a contrastarlo en un ejemplo concreto. La tabla siguiente da los números de partidos en los que se marcaron los diferentes números de goles en el Mundial de Sudáfrica del 2010: Goles Partidos 0 7 1 17 2 13 3 14 4 8 5 3 7 1 Usad un test \\(\\chi^2\\) para contrastar si podemos aceptar que estos números de goles siguen una ley de Poisson. (3) Considerad la tabla de datos que encontraréis en el url https://www.dropbox.com/s/qcok2f7u51teqr6/pacientes.txt?raw=1 y que ya usamos en un ejercicio de la Lección 4. (a) Usad el test de Shapiro-Wilk para contrastar si los pesos de los hombres de esta tabla siguen una distribución normal (b) Llevad a cabo el mismo contraste usando un test \\(\\chi^2\\). Respuestas al test (1) 0.02 SI Nosotros lo hemos calculado con x=c(65,95,87,70,193) round(chisq.test(x,p=c(1,1,1,1,2),rescale.p=T)$p.value,3) ## [1] 0.02 (2) 1.753 0.064 SI Nosotros lo hemos calculado con x=c(10,32,18,19,6) lambda=round(fitdistr(rep(0:4,x),&quot;poisson&quot;)$estimate,3) #Estimamos la lambda c(dpois(0:3,lambda),1-ppois(3,lambda))*85 #Frecuencias esperadas ## [1] 14.726539 25.815623 22.627394 13.221940 8.608503 ChiT2=chisq.test(x,p=c(dpois(0:3,lambda),1-ppois(3,lambda))) #Test p.valor2=1-pchisq(ChiT2$statistic,3) #p-valor correcto c(lambda,round(p.valor2,3)) ## lambda X-squared ## 1.753 0.064 (3) 8.2 3.18 0.692 NO Nosotros lo hemos calculado con x=c(9,27,51,46,17) sum(x) ## [1] 150 muestra=rep(c(1.5,4.5,7.5,10.5,13.5),x) fitdistr(muestra,&quot;normal&quot;) ## mean sd ## 8.2000000 3.1827661 ## (0.2598718) (0.1837571) mu=round(fitdistr(muestra,&quot;normal&quot;)$estimate[1],2) sigma=round(fitdistr(muestra,&quot;normal&quot;)$estimate[2],2) left=c(-Inf,3,6,9,12) right=c(3,6,9,12,Inf) probs=pnorm(right,mu,sigma)-pnorm(left,mu, sigma) ChiT3=chisq.test(x,p=probs) p.valor3=1-pchisq(ChiT3$statistic,2) c(mu,sigma,round(p.valor3,3)) ## mean sd X-squared ## 8.200 3.180 0.692 (4) 0.313 NO Nosotros lo hemos calculado con set.seed(2014) x=rchisq(25,10) round(ks.test(x,&quot;pnorm&quot;,mean=10,sd=3.16)$p.value,3) ## [1] 0.313 (5) 0.001 SI Nosotros lo hemos calculado con x=c(4.6, 0.97, 0.3, 1.11, 2.16, 15.52, 1.13, 0.17, 0.64, 2.00) round(lillie.test(x)$p.value,3) ## [1] 0.001 (6) 0.117 NO Nosotros lo hemos calculado con set.seed(2014) x=rnorm(15,2,0.8) set.seed(1007) y=rexp(25,0.5) round(ks.test(x,y)$p.value,3) ## [1] 0.117 Soluciones sucintas de los problemas (1) Para realizar el test \\(\\chi^2\\) “exacto”: INE=read.table(&quot;https://raw.githubusercontent.com/AprendeR-UIB/AprendeR2privat/master/INE.txt&quot;, header=TRUE) str(INE) ## &#39;data.frame&#39;:\t19 obs. of 3 variables: ## $ Comunidad : chr &quot;Andalucía&quot; &quot;Aragón&quot; &quot;Asturias (Principado de)&quot; &quot;Balears (Illes)&quot; ... ## $ Población : int 8440300 1347150 1068165 1111674 2118679 591888 2100998 2519875 7553650 5113815 ... ## $ Defunciones: int 34810 7196 6623 4088 7593 3031 14532 9765 32383 21614 ... Freq.Obs.INE=INE$Defunciones #Frecuencias observadas Prob.Esp.INE=INE$Población/sum(INE$Población) #Probabilitades esperadas chisq.test(Freq.Obs.INE, p=Prob.Esp.INE) ## ## Chi-squared test for given probabilities ## ## data: Freq.Obs.INE ## X-squared = 8008.6, df = 18, p-value &lt; 2.2e-16 Como la muestra no es muy grande, es más conveniente efectuar un test \\(\\chi^2\\) de Montecarlo chisq.test(Freq.Obs.INE, p=Prob.Esp.INE, simulate.p.value=TRUE) ## ## Chi-squared test for given probabilities with simulated p-value ## (based on 2000 replicates) ## ## data: Freq.Obs.INE ## X-squared = 8008.6, df = NA, p-value = 0.0004998 La conclusión es la misma. (2) Goles=c(0,1,2,3,4,5,7) Partidos=c(7,17,13,14,8,3,1) N=sum(Partidos) lambda=sum(Goles*Partidos)/N probs=c(dpois(0:5,lambda),1-ppois(5,lambda)) #Probabilidades teóricas probs*N #Frecuencias esperadas ## [1] 6.936418 15.304161 16.883161 12.416717 6.848903 3.022214 1.588426 Habrá que agrupar las tres últimas clases: probs=c(dpois(0:3,lambda),1-ppois(3,lambda)) Partidos=c(Partidos[1:4],sum(Partidos[5:7])) chisq.test(Partidos,p=probs) ## ## Chi-squared test for given probabilities ## ## data: Partidos ## X-squared = 1.309, df = 4, p-value = 0.8598 (3) Datos=read.table(&quot;https://www.dropbox.com/s/qcok2f7u51teqr6/pacientes.txt?raw=1&quot;, header=TRUE) str(Datos) ## &#39;data.frame&#39;:\t81 obs. of 7 variables: ## $ SEXO : Factor w/ 2 levels &quot;Hombre&quot;,&quot;Mujer&quot;: 1 1 2 1 1 2 1 1 1 1 ... ## $ EDAD : int 47 51 54 50 52 53 49 52 50 48 ... ## $ ALTURA : int 170 170 148 170 165 151 170 168 170 166 ... ## $ PESO : int 80 73 46 74 67 51 73 72 73 73 ... ## $ HIPERT : Factor w/ 2 levels &quot;Hipertenso&quot;,&quot;Normotenso&quot;: 2 1 1 1 2 1 1 2 2 2 ... ## $ IMC : num 27.7 25.3 21 25.6 24.6 ... ## $ CARDIOPA: Factor w/ 2 levels &quot;Con cardiopatia&quot;,..: 2 1 2 2 2 2 2 2 1 1 ... X=Datos$PESO[Datos$SEXO==&quot;Hombre&quot;] (a) library(nortest) lillie.test(X) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: X ## D = 0.15413, p-value = 0.009027 (b) Veamos cuántos datos tenemos n=length(X) n ## [1] 45 Como queremos que todas las clases tengan frecuencias esperadas al menos 5, no podemos usar muchas. Vamos a empezar con 7. paso=round((max(X)-min(X))/7,1) Límites=min(X)+paso*(0:7) Límites[1]=-Inf Límites[8]=Inf Límites ## [1] -Inf 69.1 71.2 73.3 75.4 77.5 79.6 Inf table(cut(X,breaks=Límites)) (-Inf,69.1] (69.1,71.2] (71.2,73.3] (73.3,75.4] (75.4,77.5] (77.5,79.6] (79.6, Inf] 2 2 18 9 8 2 4 mu=mean(X) dt=sd(X) Límites.Izq=Límites[-8] Límites.Der=Límites[-1] probs=pnorm(Límites.Der,mu,dt)-pnorm(Límites.Izq,mu,dt) Frecs.Esp=round(probs*n,2) Frecs.Esp ## [1] 1.93 4.96 9.83 12.23 9.57 4.70 1.77 Habrá que agrupar las dos primeras clases y las dos últimas Límites=Límites[-c(2,7)] Límites ## [1] -Inf 71.2 73.3 75.4 77.5 Inf Frecs.Obs=table(cut(X,breaks=Límites)) Límites.Izq=Límites[-6] Límites.Der=Límites[-1] probs=pnorm(Límites.Der,mu,dt)-pnorm(Límites.Izq,mu,dt) Frecs.Esp=round(probs*n,2) Frecs.Esp ## [1] 6.89 9.83 12.23 9.57 6.47 test=chisq.test(Frecs.Obs,p=probs) test ## ## Chi-squared test for given probabilities ## ## data: Frecs.Obs ## X-squared = 9.1484, df = 4, p-value = 0.05749 p.valor=1-pchisq(test$statistic,test$parameter-2) p.valor ## X-squared ## 0.01031437 La conclusión usando el test S-W y el test \\(\\chi^2\\) es la misma. A las repeticiones se las suele llamar empates, ties en inglés, porque la función de distribución acumulada muestral ordena los datos y las repeticiones producen empates en las posiciones de valores sucesivos.↩︎ Lo confesamos, hemos elegido la semilla de aleatoriedad no porque seamos fans de Daniel Bernoulli sino para que la muestra obtenida solo contenga ceros, unos y doses, lo que, como veréis, motivará una pequeña discusión sobre qué clases tomar.↩︎ Véase N. M. Razali, Y. B. Wah,\n“Power comparisons of Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests.” S. Stat. Model. Anal. 2 (2011), pp. 21–33.↩︎ "],
["chap-indep.html", "Lección 7 Contrastes de independencia y homogeneidad 7.1 Tablas de contingencia 7.2 Contraste de independencia 7.3 Contraste de homogeneidad 7.4 Potencia de un contraste \\(\\chi^2\\) 7.5 Guía rápida 7.6 Ejercicios", " Lección 7 Contrastes de independencia y homogeneidad El test \\(\\chi^2\\) explicado en la lección anterior permite contrastar, en situaciones adecuadas, si una muestra proviene de una determinada distribución y por lo tanto también si una muestra sigue la misma distribución que otra muestra. Como, en última instancia, la independencia de dos variables cualitativas se puede describir en términos de igualdades de probabilidades, el test \\(\\chi^2\\) también nos permitirá contrastar si dos variables cualitativas son independientes, tanto en el sentido de que las probabilidades conjuntas sean el producto de las probabilidades marginales (con un contraste de independencia) como en el sentido de que las distribuciones condicionadas de una respecto de los valores de la otra sean todas iguales (con un contraste de homogeneidad). Aunque, como veremos, los contrastes de independencia y homogeneidad son idénticos desde el punto de vista matemático e incluso utilizan el mismo estadístico \\(\\chi^2\\) y la misma definición de p-valor, provienen de diseños experimentales diferentes: En un contraste de independencia se toma una muestra transversal de la población, es decir, se selecciona al azar una cierta cantidad de individuos de la población, se observan las dos variables sobre cada uno de ellos, y se contrasta si las probabilidades conjuntas son iguales al producto de las probabilidades marginales de cada variable. Formalmente, si \\(X\\) e \\(Y\\) son las dos variables, se contrasta si para cada par de posibles valores \\(x\\) de \\(X\\) e \\(y\\) de \\(Y\\) se tiene que \\[ P(X=x,Y=y)=P(X=x)\\cdot P(Y=y) \\] o si por el contrario hay algún par de valores \\(x\\), \\(y\\) para los que esta igualdad sea falsa. En un contraste de homogeneidad se escoge una de las variables y para cada uno de sus posibles valores se toma una muestra aleatoria, de tamaño prefijado, de individuos con ese valor para esa variable; su unión forma una muestra estratificada en el sentido de la Sección 2.1. A continuación, se observa sobre cada uno de estos individuos la otra variable. En esta situación contrastamos si la distribución de probabilidades de la segunda variable es la misma en los diferentes estratos definidos por los niveles de la primera variable. Formalmente, si \\(Y\\) es la variable que usamos en primer lugar para clasificar los individuos de la población y tomar una muestra de cada clase, con posibles valores \\(y_1,\\ldots,y_k\\), y \\(X\\) es la variable que medimos en segundo lugar sobre los individuos escogidos, se contrasta si, para cada posible valor \\(x\\) de \\(X\\), \\[ P(X=x|Y=y_1)=P(X=x|Y=y_2)=\\cdots=P(X=x|Y=y_k) \\] o si por el contrario existen \\(x\\), \\(y_i\\), \\(y_j\\) tales que \\(P(X=x|Y=y_i)\\neq P(X=x|Y=y_j)\\). En ambos contrastes, la hipótesis nula es que las variables son independientes, bajo una u otra formulación matemática, y la hipótesis alternativa es que son dependientes (o que hay asociación entre ellas). La hipótesis nula se rechaza si se obtiene evidencia que hace inverosímiles las igualdades de probabilidades que se contrastan. Para ilustrar esta lección, hemos generado una muestra aleatoria de cadenas formadas por las bases “a”, “c”, “g” y “t”. En concreto, hemos generado cadenas de longitud 100 de tres tipos: A, B y C. Estos tipos se distinguen por los vectores de probabilidades que han determinado las frecuencias de las cuatro bases en las secuencias. Queremos investigar si hay relación entre el tipo (A, B o C) de una cadena, y la base de frecuencia máxima en ella. Los datos y el método de generación se encuentran en el repositorio siguiente: https://github.com/biocom-uib/Experimento-Cadenas. Este directorio contiene: El fichero LeemeGeneracionDatos, que explica cómo se han generado las muestras. El fichero MuestraTotalBases.txt, que contiene una tabla de datos de 10000 observaciones de las dos variables siguientes sobre cadenas: el tipo, que es un factor con los niveles A, B y C, y max.frec, que es otro factor que indica qué base tiene mayor frecuencia en la cadena. Este fichero es de formato texto, con una primera fila con el nombre de las variables y sus columnas separadas por comas El código siguiente carga la tabla de datos, comprueba que no ha habido problemas, y extrae tres subtablas, una para cada tipo de cadena. poblacion=read.table(&quot;https://raw.githubusercontent.com/biocom-uib/Experimento-Cadenas/master/MuestraTotalBases.txt&quot;, header=TRUE,sep=&quot;,&quot;) str(poblacion) ## &#39;data.frame&#39;:\t10000 obs. of 2 variables: ## $ tipo : Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 2 1 1 3 1 1 1 3 2 2 ... ## $ max.frec: Factor w/ 4 levels &quot;a&quot;,&quot;c&quot;,&quot;g&quot;,&quot;t&quot;: 3 1 2 4 4 1 1 3 2 2 ... head(poblacion) ## tipo max.frec ## 1 B g ## 2 A a ## 3 A c ## 4 C t ## 5 A t ## 6 A a poblacionA=subset(poblacion,tipo==&quot;A&quot;) poblacionB=subset(poblacion,tipo==&quot;B&quot;) poblacionC=subset(poblacion,tipo==&quot;C&quot;) 7.1 Tablas de contingencia Ya estudiamos en la Lección ?? de la primera parte las tablas de contingencia. En esta sección vamos a repasar y ampliar algunas de las funciones de R para el manejo de esta clase de tablas. La tabla de contingencia de frecuencias absolutas conjuntas de las dos variables del data frame poblacion se calcula de la manera siguiente: tabla=table(poblacion$tipo,poblacion$max.frec) tabla ## ## a c g t ## A 1027 1003 998 998 ## B 1259 1297 209 232 ## C 245 602 1453 677 Su tabla de frecuencias relativas conjuntas en el total de la muestra, en términos de proporciones (tantos por uno), es: prop.table(tabla) ## ## a c g t ## A 0.1027 0.1003 0.0998 0.0998 ## B 0.1259 0.1297 0.0209 0.0232 ## C 0.0245 0.0602 0.1453 0.0677 Para añadir las distribuciones marginales de la tabla de contingencia (o márgenes de la tabla), se añade una nueva fila con las sumas de cada columna y una nueva columna con las sumas de cada fila. Con R, esto se puede llevar a cabo fácilmente con la función addmargins. Su sintaxis básica es addmargins(tabla, margin=..., FUN=...) donde: tabla es una table. margin es un parámetro que puede tomar los valores siguientes: 1 si queremos una nueva fila con las marginales de cada columna. 2 si queremos una nueva columna con las marginales de cada fila. c(1,2), que es el valor por defecto para tablas de contingencia bidimensionales (y por lo tanto no hace falta especificarlo), si queremos las marginales por filas y por columnas.7 FUN es la función que se aplica a las filas o columnas para obtener el valor marginal. Por defecto es la suma, que es la función que nos interesa en esta lección, y por tanto tampoco hace falta especificarlo. El resultado es otro objeto de la clase table al que se le han añadido una o varias filas o columnas. Éstas contienen los márgenes resultantes de aplicar la función indicada por FUN. La etiqueta de las nuevas filas o columnas es la función que se aplica. Por ejemplo, para obtener las tablas marginales completas de frecuencias absolutas y relativas en nuestro ejemplo, haríamos: addmargins(tabla) ## ## a c g t Sum ## A 1027 1003 998 998 4026 ## B 1259 1297 209 232 2997 ## C 245 602 1453 677 2977 ## Sum 2531 2902 2660 1907 10000 addmargins(prop.table(tabla)) ## ## a c g t Sum ## A 0.1027 0.1003 0.0998 0.0998 0.4026 ## B 0.1259 0.1297 0.0209 0.0232 0.2997 ## C 0.0245 0.0602 0.1453 0.0677 0.2977 ## Sum 0.2531 0.2902 0.2660 0.1907 1.0000 También podemos calcular la tabla de proporciones por filas y con su marginal por filas comprobar que efectivamente la suma de cada fila es 1: addmargins(prop.table(tabla,margin=1),margin=2) ## ## a c g t Sum ## A 0.25509190 0.24913065 0.24788872 0.24788872 1.00000000 ## B 0.42008675 0.43276610 0.06973640 0.07741074 1.00000000 ## C 0.08229762 0.20221700 0.48807524 0.22741014 1.00000000 Y viceversa, podemos calcular la tabla de proporciones por columnas y con su marginal por columnas comprobar que efectivamente la suma de cada columna es 1: addmargins(prop.table(tabla,margin=2),margin=1) ## ## a c g t ## A 0.40576847 0.34562371 0.37518797 0.52333508 ## B 0.49743185 0.44693315 0.07857143 0.12165705 ## C 0.09679968 0.20744314 0.54624060 0.35500787 ## Sum 1.00000000 1.00000000 1.00000000 1.00000000 Observad que el significado de margin en addmargins es diferente de, por ejemplo, en prop.table o en apply: en estas dos últimas instrucciones indica la dimensión en la que calculamos las proporciones o aplicamos la función, mientras que en addmargins indica la dimensión en la que añadimos el margen, que, por lo tanto, se calcula aplicando la función en la otra dimensión. Si sólo nos interesa la fila o la columna de marginales, podemos usar las instrucciones colSums y rowSums, que suman una tabla por columnas y por filas, respectivamente. Por ejemplo, para obtener los vectores de marginales por columnas y por filas, respectivamente, podríamos entrar: colSums(tabla) ## a c g t ## 2531 2902 2660 1907 rowSums(tabla) ## A B C ## 4026 2997 2977 También podemos obtener estos márgenes extrayendo los márgenes de la tabla con márgenes, obtenida aplicando addmargins a la tabla original, por medio de las instrucciones usuales para extraer filas y columnas. addmargins(tabla)[&quot;Sum&quot;,-dim(addmargins(tabla))[2]] ## a c g t ## 2531 2902 2660 1907 addmargins(tabla)[-dim(addmargins(tabla))[1],&quot;Sum&quot;] ## A B C ## 4026 2997 2977 Observad, por ejemplo, la construcción addmargins(tabla)[&quot;Sum&quot;,-dim(addmargins(tabla))[2]] Con addmargins(tabla)[\"Sum\",] obtendríamos la fila Sum de la tabla con las marginales, incluyendo la última entrada, correspondiente a la columna Sum. Por lo tanto, hay que eliminarla: como dim(addmargins(tabla))[2] es el número de columnas de la tabla addmargins(tabla), es decir, la longitud del vector addmargins(tabla)[\"Sum\",], la última entrada (correspondiente a la última columna) se puede eliminar especificando -dim(addmargins(tabla))[2] en las columnas al extraer la fila Sum. 7.2 Contraste de independencia El contraste de independencia para tablas de contingencia bidimensionales consiste en decidir si las dos variables de la tabla tienen distribuciones independientes, es decir, si la distribución de probabilidades conjunta es igual al producto de las probabilidades marginales. En nuestro ejemplo, queremos decidir si podemos aceptar que las variables tipo y max.frec son independientes o si por el contrario hay evidencia de que la distribución de las bases de máxima frecuencia depende del tipo de cadena. Vamos a extraer una muestra aleatoria simple de la población y observar los valores de las dos variables. En concreto, seleccionaremos una muestra transversal de 150 filas, al azar y con reposición, de entre las 10000 filas del data frame poblacion. El código es el siguiente (fijamos la semilla de aleatoriedad para que sea reproducible): set.seed(42) n=150 indices.muestra=sample(1:10000, size=n, replace=TRUE) muestra.test.indep= poblacion[indices.muestra, ] #Las filas que forman la muestra Ahora calculamos la tabla de contingencia con sus marginales. tabla.ind=table(muestra.test.indep$tipo, muestra.test.indep$max.frec) tabla.ind ## ## a c g t ## A 8 17 16 12 ## B 23 22 3 7 ## C 4 9 17 12 tabla.ind.marg=addmargins(tabla.ind) tabla.ind.marg ## ## a c g t Sum ## A 8 17 16 12 53 ## B 23 22 3 7 55 ## C 4 9 17 12 42 ## Sum 35 48 36 31 150 Extraemos sus dos márgenes. Las frecuencias marginales de las filas: frec.abs.tipo=tabla.ind.marg[-dim(tabla.ind.marg)[1],&quot;Sum&quot;] frec.abs.tipo ## A B C ## 53 55 42 Las frecuencias marginales de las columnas: frec.abs.max.frec=tabla.ind.marg[&quot;Sum&quot;,-dim(tabla.ind.marg)[2]] frec.abs.max.frec ## a c g t ## 35 48 36 31 El test de independencia usa las frecuencias absolutas esperadas bajo la hipótesis nula de independencia, que se obtienen, para cada celda (i,j), multiplicando la frecuencia marginal de la fila i por la de la columna j y dividiendo por el tamaño de la muestra. En nuestro ejemplo estas frecuencias esperadas son \\[ \\begin{array}{l|cccc} &amp; \\mbox{a} &amp; \\mbox{c} &amp; \\mbox{g} &amp; \\mbox{t} \\\\ \\hline \\mbox{A} &amp; 53\\cdot 35/150 &amp; 53\\cdot 48/150 &amp; 53\\cdot 36/150 &amp; 53\\cdot 31/150 \\\\ \\mbox{B} &amp; 55\\cdot 35/150 &amp; 55\\cdot 48/150 &amp; 55\\cdot 36/150 &amp; 55\\cdot 31/150 \\\\ \\mbox{C} &amp; 42\\cdot 35/150 &amp; 42\\cdot 48/150 &amp; 42\\cdot 36/150 &amp; 42\\cdot 31/150 \\end{array} \\] y podemos obtenerlas fácilmente mediante un producto de matrices: \\[ \\frac{1}{150}\\cdot\\left(\\begin{array}{c} 53 \\\\ 55 \\\\ 42\\end{array}\\right)\\cdot \\big( 35 ,48 , 36 , 31\\big). \\] Por lo tanto, con R obtenemos esta tabla de frecuencias esperadas de la manera siguiente: frec.esperadas=frec.abs.tipo%*%t(frec.abs.max.frec)/n frec.esperadas ## a c g t ## [1,] 12.36667 16.96 12.72 10.95333 ## [2,] 12.83333 17.60 13.20 11.36667 ## [3,] 9.80000 13.44 10.08 8.68000 Aunque vayamos a realizar el test de independencia con una función de R, es necesario comprobar que todas estas frecuencias esperadas son mayores o iguales que 5, por lo que no podemos evitar este cálculo. En este caso vemos que se cumple esta condición. Si queremos realizar el test \\(\\chi^2\\) de independencia a mano, podemos calcular el estadístico de forma directa con chi2.estadistico=sum((tabla.ind-frec.esperadas)^2/frec.esperadas) chi2.estadistico ## [1] 32.12115 y el p-valor del contraste, con p.valor=1-pchisq(chi2.estadistico,df=(dim(tabla.ind)[1]-1)*(dim(tabla.ind)[2]-1)) p.valor ## [1] 1.546778e-05 Para realizar el test \\(\\chi^2\\) de independencia con R, es suficiente aplicar la función chisq.test a la tabla de contingencia de frecuencias absolutas: chisq.test(tabla.ind) ## ## Pearson&#39;s Chi-squared test ## ## data: tabla.ind ## X-squared = 32.121, df = 6, p-value = 1.547e-05 Como el p-valor es muy pequeño, podemos rechazar la hipótesis de que las variables objeto de estudio sean independientes: hemos obtenido evidencia estadísticamente significativa de que la distribución de las bases de frecuencia máxima sí que depende del tipo de cadena. Si algunas frecuencias absolutas esperadas fueran inferiores a 5, la aproximación del p-valor por una distribución \\(\\chi^2\\) podría no ser adecuada. En este caso, al ser las variables cualitativas, no podemos recurrir al agrupamiento de valores consecutivos, puesto que no tienen orden. Si se da esta situación, lo mejor es recurrir a simular el p-valor usando el parámetro simulate.p.value=TRUE. Por ejemplo, consideremos la situación siguiente, en la que tomamos una muestra de solo 100 cadenas y con otra semilla de aleatoriedad: set.seed(300) n2=100 indices.muestra2=sample(1:10000,size=n2,replace=TRUE) muestra.test.indep2= poblacion[indices.muestra2,] tabla.ind2=table(muestra.test.indep2$tipo,muestra.test.indep2$max.frec) tabla.ind2 ## ## a c g t ## A 13 10 11 12 ## B 9 18 3 2 ## C 4 3 10 5 Si aplicamos a esta tabla la función chisq.test, obtenemos: chisq.test(tabla.ind2) ## Warning in chisq.test(tabla.ind2): Chi-squared approximation may be ## incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: tabla.ind2 ## X-squared = 21.843, df = 6, p-value = 0.001293 ¡Vaya! Veamos la tabla de frecuencias esperadas: frec.abs.tipo2=rowSums(tabla.ind2) frec.abs.max.frec2=colSums(tabla.ind2) frec.esperadas2=frec.abs.tipo2%*%t(frec.abs.max.frec2)/n2 frec.esperadas2 ## a c g t ## [1,] 11.96 14.26 11.04 8.74 ## [2,] 8.32 9.92 7.68 6.08 ## [3,] 5.72 6.82 5.28 4.18 Hay una frecuencia esperada inferior a 5. Por lo tanto, lo recomendable es estimar el p-valor del test \\(\\chi^2\\) de independencia mediante simulaciones. Pero ahora tenemos que ir con cuidado en una cosa: hemos fijado la semilla de aleatoriedad para obtener una muestra de cadenas con frecuencias esperadas inferiores a 5. Lo recomendable es reiniciar esta semilla a un valor aleatorio con set.seed(NULL). set.seed(NULL) chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.00119976 chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.00239952 chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.00079984 chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.0009998 El p-valor es sistemáticamente pequeño, lo que nos permite rechazar la hipótesis de que las variables son independientes. 7.3 Contraste de homogeneidad Como ya hemos dicho, la diferencia entre el contraste de homogeneidad y el de independencia está en el diseño del experimento: en cada contraste se selecciona la muestra de una manera diferente. En nuestro caso, para contrastar si la distribución de probabilidades de la base de mayor frecuencia es la misma para cada tipo de cadena o no, lo que vamos a hacer es tomar una muestra aleatoria de 50 cadenas de cada tipo, juntarlas en una sola muestra estratificada, y aplicar el test \\(\\chi^2\\) a esta muestra. El código siguiente realiza el muestreo en cada subpoblación de tipo y guarda la muestra total en el data frame muestra.test.homo. Fijamos de nuevo la semilla de aleatoriedad (otra), para que el test sea reproducible. set.seed(100) Generamos los vectores de índices de las muestras: n3=50 indices.muestraA=sample(1:dim(poblacionA)[1],size=n3,replace=TRUE) indices.muestraB=sample(1:dim(poblacionB)[1],size=n3,replace=TRUE) indices.muestraC=sample(1:dim(poblacionC)[1],size=n3,replace=TRUE) Finalmente, tomamos las filas de cada muestra y las combinamos en un data frame: muestraA.50=poblacionA[indices.muestraA,] muestraB.50=poblacionB[indices.muestraB,] muestraC.50=poblacionC[indices.muestraC,] muestra.test.homo=rbind(muestraA.50,muestraB.50,muestraC.50) str(muestra.test.homo) ## &#39;data.frame&#39;:\t150 obs. of 2 variables: ## $ tipo : Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ max.frec: Factor w/ 4 levels &quot;a&quot;,&quot;c&quot;,&quot;g&quot;,&quot;t&quot;: 1 3 1 4 4 3 3 1 4 2 ... Calculamos la tabla de contingencia de la muestra: tabla.homo=table(muestra.test.homo$tipo,muestra.test.homo$max.frec) tabla.homo ## ## a c g t ## A 13 12 16 9 ## B 26 18 3 3 ## C 3 10 28 9 Añadimos los márgenes: addmargins(tabla.homo) ## ## a c g t Sum ## A 13 12 16 9 50 ## B 26 18 3 3 50 ## C 3 10 28 9 50 ## Sum 42 40 47 21 150 Confirmamos que hemos tomado 50 cadenas de cada grupo. Ahora calculamos las frecuencias esperadas bajo la hipótesis nula, para comprobar si son todas mayores o iguales que 5: frec.abs.tipo=rowSums(tabla.homo) frec.abs.tipo ## A B C ## 50 50 50 frec.abs.max.frec=colSums(tabla.homo) frec.abs.max.frec ## a c g t ## 42 40 47 21 frec.esperadas=frec.abs.tipo%*%t(frec.abs.max.frec)/sum(frec.abs.tipo) frec.esperadas ## a c g t ## [1,] 14 13.33333 15.66667 7 ## [2,] 14 13.33333 15.66667 7 ## [3,] 14 13.33333 15.66667 7 Todas las frecuencias son mayores o iguales que 5, así que aplicamos la función chisq.test sin simular el p-valor: chisq.test(tabla.homo) ## ## Pearson&#39;s Chi-squared test ## ## data: tabla.homo ## X-squared = 44.986, df = 6, p-value = 4.71e-08 El p-valor es muy pequeño, por lo que podemos rechazar que las distribuciones de los valores de las bases de máxima frecuencia sean la misma para cada valor de la variable tipo. En definitiva, el tipo de cadena afecta a la distribución de la base de mayor frecuencia. Es la misma conclusión a la que habíamos llegado con el test de independencia, solo que ahora hemos realizado un tipo de experimento diferente. 7.4 Potencia de un contraste \\(\\chi^2\\) La potencia de un contraste \\(\\chi^2\\), tanto de bondad de ajuste como de independencia o de homogeneidad, se puede calcular de manera similar a cómo lo hacíamos en otros tipos de contrastes de uno o dos parámetros. La instrucción para llevarlo a cabo es pwr.chisq.test del paquete pwr. Su sintaxis básica es pwr.chisq.test(N=..., df=..., sig.level=..., w=..., power=...) donde: N es el tamaño de la muestra. df es el número de grados de libertad del estadístico (recordad que en un test de bondad de ajuste es el número de clases menos 1 y menos el número de parámetros estimados, y en un test de independencia o de homogeneidad es el número de niveles de una variable menos 1 por el número de niveles de la otra variable menos 1). sig.level es el nivel de significación \\(\\alpha\\). w es la magnitud del efecto, que en este tipo de tests se define como \\(\\sqrt{X^2/N}\\), siendo \\(X^2\\) el valor del estadístico de contraste y \\(N\\) el tamaño de la muestra completa. power es la potencia \\(1-\\beta\\). Si se especifican todos estos parámetros menos uno, la función da el valor del parámetro que falta. Normalmente, querremos saber la potencia de un contraste a posteriori o el tamaño de la muestra necesario para tener la potencia deseada para una magnitud del efecto esperada concreta. Veamos algunos ejemplos de uso de esta función. Ejemplo 7.1 Vamos a calcular la potencia del contraste del Ejemplo 6.3. En ese ejemplo, el tamaño de la muestra fue \\(N=40\\), el número de grados de libertad fue 5 y obtuvimos que \\(X^2=7.7\\), por lo que la magnitud del efecto fue \\(w=\\sqrt{7.7/40}\\). Tomaremos el nivel de significación usual, \\(\\alpha=0.05\\). library(pwr) pwr.chisq.test(N=40, df=5, sig.level=0.05, w=sqrt(7.7/40)) ## ## Chi squared power calculation ## ## w = 0.4387482 ## N = 40 ## df = 5 ## sig.level = 0.05 ## power = 0.545751 ## ## NOTE: N is the number of observations La potencia del contraste ha sido de, aproximadamente, un 55%. Para obtener solo la potencia, podemos usar el sufijo $power: pwr.chisq.test(N=40, df=5, sig.level=0.05, w=sqrt(7.7/40))$power ## [1] 0.545751 Ejemplo 7.2 Vamos a calcular la potencia del contraste de normalidad de las longitudes de los sépalos de flores iris del Ejemplo 6.8. En ese ejemplo el tamaño de muestra fue \\(N=150\\); como usamos 7 clases, pero estimamos 2 parámetros, el número de grados de libertad fue 4; obtuvimos que \\(X^2=11.0637\\), por lo que \\(w=\\sqrt{11.0637/150}\\); y ahora, por variar, tomaremos \\(\\alpha=0.1\\). pwr.chisq.test(N=150, df=4, sig.level=0.1, w=sqrt(11.0637/15))$power ## [1] 1 La potencia da 1: la probabilidad de que aceptáramos que la muestra seguía una distribución normal si no fuera verdad es prácticamente 0. Ejemplo 7.3 En el contraste de homogeneidad de la Sección 7.3 hemos tomado tres muestras de 50 individuos cada una, en total 150 individuos. El estadístico de contraste ha valido \\(X^2=28.59\\), por lo que la magnitud del efecto en ese test ha sido de \\(w=\\sqrt{28.59/150}=0.4366\\), entre mediana y grande según la función cohen.ES, que nos da las magnitudes del efecto que por convención se entienden como pequeñas, medianas o grandes para los diferentes tests considerados en el paquete pwr: cohen.ES(test=&quot;chisq&quot;, size=&quot;medium&quot;)$effect.size ## [1] 0.3 cohen.ES(test=&quot;chisq&quot;, size=&quot;large&quot;)$effect.size ## [1] 0.5 ¿De qué tamaño deberíamos haber tomado las muestras para garantizar una potencia del 90%, suponiendo que esperásemos una magnitud del efecto mediana y tomásemos un nivel de significación \\(\\alpha=0.05\\)? pwr.chisq.test(df=6, sig.level=0.05, w=0.3, power=0.9) ## ## Chi squared power calculation ## ## w = 0.3 ## N = 193.5425 ## df = 6 ## sig.level = 0.05 ## power = 0.9 ## ## NOTE: N is the number of observations Hubiéramos necesitado como mínimo un total de unos 194 individuos: si queríamos tomar las tres muestras del mismo tamaño, esto significa tres muestras de como mínimo 65 individuos cada una. Para obtener solo el tamaño de la muestra, se puede añadir el sufijo $N: pwr.chisq.test(df=6, sig.level=0.05, w=0.3, power=0.9)$N ## [1] 193.5425 7.5 Guía rápida table calcula tablas de contingencia de frecuencias absolutas. prop.table calcula tablas de contingencia de frecuencias relativas. addmargins sirve para añadir a una table una fila o una columna obtenidas aplicando una función a todas las columnas o a todas las filas de la tabla, respectivamente. Sus parámetros principales son: margin: igualado a 1, se aplica la función por columnas, añadiendo una nueva fila; igualado a 2, se aplica la función por filas, añadiendo una nueva columna; igualado a c(1,2), que es su valor por defecto, hace ambas cosas. FUN: la función que se aplica a las filas o columnas; su valor por defecto es sum. colSums calcula un vector con las sumas de las columnas de una matriz o una tabla. rowSums calcula un vector con las sumas de las filas de una matriz o una tabla. chisq.test sirve para realizar tests \\(\\chi^2\\) de independencia y homogeneidad. El resultado es una list formada, entre otros, por los objetos siguientes: statistic (el valor del estadístico \\(X^2\\)), parameter (los grados de libertad) y p.value (el p-valor). Sus parámetros principales en el contexto de esta lección son: simulate.p.value: igualado a TRUE, calcula el p-valor mediante simulaciones. B: en este último caso, permite especificar el número de simulaciones. pwr.chisq.test, del paquete pwr, sirve para calcular uno de los parámetros siguientes a partir de los otros cuatro: N: el tamaño de la muestra. df: el número de grados de libertad del contraste. sig.level: el nivel de significación \\(\\alpha\\). power: la potencia \\(1-\\beta\\). w: la magnitud del efecto. 7.6 Ejercicios Modelo de test (1) Hemos observado dos variables cualitativas en una muestra de una población. Cada variable tiene 3 niveles. La tabla de contingencia resultante ha sido la siguiente: \\[ \\begin{array}{c|ccc} &amp;X&amp;Y&amp;Z\\cr\\hline A &amp; 2 &amp; 17 &amp; 11\\cr B &amp; 8 &amp; 10 &amp; 25\\cr C &amp; 3 &amp; 14 &amp; 5 \\end{array} \\] ¿Es verdad que, si estas variables aleatorias fueran independientes, las frecuencias esperadas de cada combinación de niveles, uno de cada variable, serían todas \\(\\geq 5\\)? Tenéis que contestar SI, en mayúsculas y sin acento, o NO. (2) Hemos observado dos variables cualitativas en una muestra de una población. Una variable tiene 4 niveles y la otra 3. La tabla de contingencia resultante ha sido la siguiente: \\[ \\begin{array}{c|cccc} &amp;A&amp;B&amp;C &amp; D\\cr\\hline X &amp; 50 &amp; 19&amp;17 &amp; 21\\cr Y &amp;69 &amp; 47 &amp; 56 &amp; 37 \\cr Z &amp;33 &amp; 23 &amp; 18 &amp; 21 \\end{array} \\] Emplead la función chisq.test para contrastar si estas dos variables son independientes o no. Tenéis que dar el p-valor del test (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decir (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.1\\), podríamos rechazar la hipótesis nula de que estas dos variables son independientes. Dad las dos respuestas en este orden y separadas por un único espacio en blanco. (3) Hemos realizado un test \\(\\chi^2\\) de independencia sobre una muestra de 200 individuos, con un nivel de significación de 0.1. Las variables objeto de estudio tenían 5 y 6 niveles, respectivamente. El estadístico de contraste ha valido \\(16.56\\). ¿Cuál es el p-valor del contraste? ¿Cuál es la potencia del contraste realizado? Tenéis que dar ambos valores en este orden, redondeados a 3 cifras decimales sin ceros innecesarios a la derecha, y separados por un único espacio en blanco. Ejercicios (1) En un estudio para determinar si la tensión arterial depende del grupo sanguíneo, se tomó una muestra aleatoria de 1500 personas, se determinó su grupo sanguíneo y se les midió la tensión, clasificándola en baja, normal o alta. Los resultados fueron los de la tabla siguiente: \\[ \\begin{array}{l} \\hphantom{TensionAAa}\\text{Grupo sanguíneo}\\\\ \\begin{array}{l|cccc} \\text{Tensión} &amp; \\text{A} &amp; \\text{B} &amp; \\text{AB} &amp; 0\\\\ \\hline \\text{Baja} &amp; 28 &amp;\t9 &amp;7 &amp;\t31\\\\ \\text{Normal} &amp;\t543 &amp;\t211 &amp;\t90 &amp;\t476\\\\ \\text{Alta} &amp; 44 &amp;22 &amp;8 &amp;\t31\\\\ \\hline \\end{array} \\end{array} \\] Como hemos dicho, a partir de estos datos queremos decidir si hay asociación entre la tensión arterial y el grupo sanguíneo. (a) Según el diseño del experimento, ¿qué tipo de contraste estamos realizando: de independencia o de homogeneidad? (b) Comprobad si se dan las condiciones para poder efectuar un test \\(\\chi^2\\). En caso afirmativo, llevadlo a cabo. En caso negativo, usad el método de Montecarlo. Respuestas al test (1) NO Nosotros lo hemos mirado con A=matrix(c(2,17,11,8,10,25,3,14,5),nrow=3,byrow=TRUE) RS=rowSums(A) CS=colSums(A) frec.esperadas=RS%*%t(CS)/sum(A) frec.esperadas ## [,1] [,2] [,3] ## [1,] 4.105263 12.947368 12.947368 ## [2,] 5.884211 18.557895 18.557895 ## [3,] 3.010526 9.494737 9.494737 (2) 0.128 NO Nosotros lo hemos resuelto con X=matrix(c(50,19,17,21,69,47,56,37,33,23,18,21),nrow=3,byrow=T) round(chisq.test(X)$p.value,3) ## [1] 0.128 (3) 0.681 0.777 Nosotros lo hemos resuelto con p.valor=1-pchisq(16.56,(5-1)*(6-1)) potencia=pwr.chisq.test(N=200, df=(5-1)*(6-1), sig.level=0.1, w=sqrt(16.56/200))$power round(c(p.valor,potencia),3) ## [1] 0.681 0.777 Soluciones sucintas de los problemas (a) De independencia, porque la muestra es transversal (b) Datos=rbind(c(28,9,7,31),c(543,211,90,476),c(44,22,8,31)) Datos 28 9 7 31 543 211 90 476 44 22 8 31 n=sum(Datos) n ## [1] 1500 freq.esp.tipo=colSums(Datos) freq.esp.tensión=rowSums(Datos) frec.esperadas=freq.esp.tensión%*%t(freq.esp.tipo)/n frec.esperadas 30.75 12.10 5.25 26.90 541.20 212.96 92.40 473.44 43.05 16.94 7.35 37.66 chisq.test(Datos) ## ## Pearson&#39;s Chi-squared test ## ## data: Datos ## X-squared = 5.1163, df = 6, p-value = 0.529 El valor por defecto de margin es el vector de todas las dimensiones de la tabla. Hay que recordar que, aunque ahora sólo tratamos con tablas bidimensionales, con table se pueden especificar tablas de contingencia de un número arbitrario de dimensiones.↩︎ "],
["chap-estmult.html", "Lección 8 Introducción a la estadística descriptiva multidimensional 8.1 Matrices de datos cuantitativos 8.2 Transformaciones lineales 8.3 Covarianzas y correlaciones 8.4 Correlación de Spearman 8.5 Contrastes de correlación 8.6 Un ejemplo 8.7 Representación gráfica de datos multidimensionales 8.8 Guía rápida 8.9 Ejercicios", " Lección 8 Introducción a la estadística descriptiva multidimensional En general, los datos que se recogen en experimentos son multidimensionales: medimos varias variables aleatorias sobre una misma muestra de individuos, y organizamos esta información en tablas de datos en las que las filas representan los individuos observados y cada columna corresponde a una variable diferente. En las lecciones finales de la primera parte ya aparecieron datos cualitativos y ordinales multidimensionales, para los que calculamos y representamos gráficamente sus frecuencias globales y marginales; en esta lección estudiamos algunos estadísticos específicos para resumir y representar la relación existente entre diversas variables cuantitativas. 8.1 Matrices de datos cuantitativos Supongamos que hemos medido los valores de \\(p\\) variables aleatorias \\(X_1,\\ldots,X_p\\) sobre un conjunto de \\(n\\) individuos u objetos. Es decir, tenemos \\(n\\) observaciones de \\(p\\) variables. En cada observación, los valores que toman estas variables forman un vector que será una realización del vector aleatorio \\(\\underline{X}=(X_1,X_2,\\ldots,X_p)\\). Para trabajar con estas observaciones, las dispondremos en una tabla de datos donde cada fila corresponde a un individuo y cada columna, a una variable. En R, lo más conveniente es definir esta tabla en forma de data frame, pero, por conveniencia de lenguaje, en el texto de esta lección la representaremos como una matriz \\[ {X}=\\begin{pmatrix} x_{1 1} &amp; x_{1 2} &amp;\\ldots &amp; x_{1 p}\\\\ x_{2 1} &amp; x_{2 2} &amp;\\ldots &amp; x_{2 p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots\\\\ x_{n 1} &amp; x_{n 2} &amp;\\ldots &amp; x_{n p} \\end{pmatrix}. \\] Utilizaremos las notaciones siguientes: Denotaremos la \\(i\\)-ésima fila de \\(X\\) por \\[ {x}_{i\\bullet}=(x_{i 1}, x_{i 2}, \\ldots, x_{i p}). \\] Este vector está compuesto por las observaciones de las \\(p\\) variables sobre el \\(i\\)-ésimo individuo. Denotaremos la \\(j\\)-ésima columna de \\(X\\) por \\[ x_{\\bullet j}=\\begin{pmatrix}x_{1 j} \\\\ x_{2 j}\\\\ \\vdots \\\\ x_{n j} \\end{pmatrix}. \\] Esta columna está formada por todos los valores de la \\(j\\)-ésima variable, es decir, es una muestra de \\(X_j\\). Observad que, en cada caso, la bolita \\(\\bullet\\) en el subíndice representa el índice “variable” de los elementos del vector o de la columna. De esta manera, podremos expresar la matriz de datos \\(X\\) tanto por filas (individuos) como por columnas (muestras de variables): \\[ {X}=\\begin{pmatrix}{x}_{1\\bullet}\\\\x_{2\\bullet}\\\\\\vdots \\\\ {x}_{n\\bullet}\\end{pmatrix}=({x}_{\\bullet1}, {x}_{\\bullet 2}, \\ldots, {x}_{\\bullet p}). \\] Con estas notaciones, podemos generalizar al caso multidimensional los estadísticos de una variable cuantitativa, definiéndolos como los vectores que se obtienen aplicando el estadístico concreto a cada columna de la tabla de datos. Así: El vector de medias de \\(X\\) es el vector formado por las medias aritméticas de sus columnas: \\[ \\overline{X}=(\\overline{{{x}}}_{\\bullet1}, \\overline{{x}}_{\\bullet 2}, \\ldots, \\overline{{x}}_{\\bullet p}), \\] donde, para cada \\(j=1, \\ldots, p\\), \\[ \\overline{{x}}_{\\bullet j}=\\frac{1}{n}\\sum\\limits_{i=1}^n x_{i j}. \\] Observemos que \\[ \\begin{array}{rl} \\overline{X} &amp; \\displaystyle = (\\overline{{{x}}}_{\\bullet1}, \\overline{x}_{\\bullet 2},\\ldots,\\overline{x}_{\\bullet p}) = \\frac{1}{n} \\Big(\\sum_{i=1}^n x_{i 1}, \\sum_{i=1}^n x_{i 2},\\ldots, \\sum_{i=1}^n x_{i p}\\Big)\\\\[1ex] &amp; \\displaystyle =\\frac{1}{n} \\sum_{i=1}^n (x_{i 1}, x_{i 2},\\ldots,x_{i p} ) = \\frac{1}{n} \\sum_{i=1}^n {{x}_{i\\bullet}} \\end{array} \\] Es decir, el vector de medias de \\(X\\) es la media aritmética de sus vectores fila. El vector de varianzas de \\(X\\) es el vector formado por las varianzas de sus columnas: \\[ s^2_{X}=(s^2_{1}, s^2_2, \\ldots, s^2_p), \\] donde \\[ s_j^2=\\frac{1}{n}\\sum_{i=1}^n {(x_{ij}-\\overline{{x}}_{\\bullet j})^2}. \\] El vector de varianzas muestrales de \\(X\\) está formado por las varianzas muestrales de sus columnas: \\[ \\widetilde{s}^2_{X}=(\\widetilde{s}^2_{1}, \\widetilde{s}^2_2, \\ldots, \\widetilde{s}^2_p), \\] donde \\[ \\widetilde{s}_j^2=\\frac{1}{n-1}\\sum_{i=1}^n {(x_{ij}-\\overline{{x}}_{\\bullet j})^2}=\\frac{n}{n-1}s_j^2. \\] Los vectores de desviaciones típicas \\(s_{X}\\) y de desviaciones típicas muestrales \\(\\widetilde{s}_{X}\\) de \\(X\\) son los formados por las desviaciones típicas y las desviaciones típicas muestrales de sus columnas, respectivamente: \\[ \\begin{array}{l} s_{X}=(s_{1}, s_2, \\ldots, s_p)=(+\\sqrt{\\vphantom{s_p^2}{s}^2_{1}}, +\\sqrt{\\vphantom{s_p^2}{s}^2_2}, \\ldots, +\\sqrt{s_p^2})\\\\[1ex] \\widetilde{s}_{X}=(\\widetilde{s}_{1}, \\widetilde{s}_2, \\ldots, \\widetilde{s}_p)=(+\\sqrt{\\vphantom{s_p^2}\\widetilde{s}^2_{1}}, +\\sqrt{\\vphantom{s_p^2}\\widetilde{s}^2_2}, \\ldots, +\\sqrt{\\widetilde{s}^2_p}) \\end{array} \\] Como en el caso unidimensional, \\(\\overline{X}\\) es un estimador insesgado de la esperanza \\(E(\\underline{X})=\\boldsymbol\\mu\\) del vector aleatorio \\(\\underline{X}\\) del cual \\(X\\) es una muestra. Por lo que refiere a \\({s}^2_{X}\\) y \\(\\widetilde{s}^2_{X}\\), ambas son estimadores del vector de varianzas de \\(\\underline{X}\\): \\(\\widetilde{s}^2_{X}\\) es insesgado y, cuando todas las variables aleatorias del vector son normales, \\({s}^2_{X}\\) es el máximo verosímil. Estos vectores de estadísticos se pueden calcular con R aplicando la función correspondiente al estadístico a todas las columnas de la tabla de datos. La manera más sencilla de hacerlo en un solo paso es usando la función sapply, si tenemos guardada la tabla como un data frame, o apply con MARGIN=2, si la tenemos guardada en forma de matriz. Ejemplo 8.1 Consideremos la tabla de datos \\[ X=\\left(\\begin{array}{rrr} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{array}\\right) \\] formada por 4 observaciones de 3 variables; por lo tanto, \\(n=4\\) y \\(p=3\\). Vamos a guardarla en un data frame y a calcular sus estadísticos. X=data.frame(V1=c(1,1,2,3),V2=c(-1,0,3,0),V3=c(3,3,0,1)) X ## V1 V2 V3 ## 1 1 -1 3 ## 2 1 0 3 ## 3 2 3 0 ## 4 3 0 1 Su vector de medias es: sapply(X, mean) ## V1 V2 V3 ## 1.75 0.50 1.75 Su vector de varianzas muestrales es: sapply(X, var) ## V1 V2 V3 ## 0.9166667 3.0000000 2.2500000 Su vector de desviaciones típicas muestrales es: sapply(X, sd) ## V1 V2 V3 ## 0.9574271 1.7320508 1.5000000 Su vector de varianzas es: var_ver=function(x){var(x)*(length(x)-1)/length(x)} #Varianza &quot;verdadera&quot; sapply(X, var_ver) ## V1 V2 V3 ## 0.6875 2.2500 1.6875 Su vector de desviaciones típicas es: sd_ver=function(x){sqrt(var_ver(x))} #Desv. típica &quot;verdadera&quot; sapply(X, sd_ver) ## V1 V2 V3 ## 0.8291562 1.5000000 1.2990381 Nota. De ahora en adelante, supondremos que todos los vectores de datos cuantitativos que aparezcan en lo que queda de lección, incluidas las columnas de tablas de datos, son no constantes y, por lo tanto, tienen desviación típica no nula. 8.2 Transformaciones lineales A veces es conveniente aplicar una transformación lineal a una tabla de datos \\(X\\), sumando a cada columna un valor y luego multiplicando cada columna resultante por otro valor. Los dos ejemplos más comunes de trasformación lineal son el centrado y la tipificación de datos. Para centrar una matriz de datos \\(X\\), se resta a cada columna su media aritmética: \\[ \\widetilde{X}= \\begin{pmatrix} x_{1 1}- \\overline{x}_{\\bullet 1}&amp; x_{1 2}- \\overline{x}_{\\bullet 2} &amp;\\ldots &amp; x_{1 p}- \\overline{x}_{\\bullet p}\\\\ x_{2 1} - \\overline{x}_{\\bullet 1}&amp; x_{2 2}- \\overline{x}_{\\bullet 2} &amp;\\ldots &amp; x_{2 p}- \\overline{x}_{\\bullet p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots\\\\ x_{n 1} - \\overline{x}_{\\bullet 1}&amp; x_{n 2}- \\overline{x}_{\\bullet 2} &amp;\\ldots &amp; x_{n p}- \\overline{x}_{\\bullet p} \\end{pmatrix}. \\] Llamaremos a esta matriz la matriz de datos centrados de \\(X\\). Ejemplo 8.2 Consideremos de nuevo la matriz de datos del Ejemplo 8.1, \\[ {X}=\\left(\\begin{array}{rrr} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{array}\\right) \\] Para centrarla, hemos de restar a cada columna su media. Ya hemos calculado estas medias hace un momento: sapply(X, mean) ## V1 V2 V3 ## 1.75 0.50 1.75 Por lo tanto, su matriz de datos centrados es \\[ \\widetilde{X}=\\left(\\begin{array}{rrr} 1-1.75&amp;-1-0.5&amp;3-1.75\\\\ 1-1.75&amp;0-0.5&amp;3-1.75\\\\ 2-1.75&amp;3-0.5&amp;0-1.75 \\\\ 3-1.75&amp;0-0.5&amp;1-1.75 \\end{array}\\right)= \\left(\\begin{array}{rrr} -0.75&amp;-1.5&amp;1.25\\\\ -0.75&amp;-0.5&amp;1.25\\\\ 0.25&amp;2.5&amp;-1.75\\\\ 1.25&amp;-0.5&amp;-0.75 \\end{array}\\right). \\] Dado un vector de datos formado por una muestra de una variable cuantitativa, su vector de datos tipificados es el vector que se obtiene restando a cada entrada la media aritmética del vector y dividiendo el resultado por su desviación típica. De esta manera, se obtiene un vector de datos de media aritmética 0 y varianza 1. Tipificar un vector de datos es conveniente cuando se quiere trabajar con estos datos sin que influyan ni su media ni las unidades en los que están medidos: al dividir por su desviación típica, los valores resultantes son adimensionales. Por lo tanto, tipificar las variables de una tabla de datos permite compararlas dejando de lado las diferencias que pueda haber entre sus valores medios o sus varianzas. La matriz tipificada de una matriz de datos \\(X\\) es la matriz \\(Z\\) que se obtiene tipificando cada columna; es decir, para tipificar una matriz de datos \\(X\\), restamos a cada columna su media y a continuación dividimos cada columna por la desviación típica de la columna original en \\(X\\) (que coincide con la desviación típica de la columna “centrada”, puesto que sumar o restar constantes no modifica la desviación típica): \\[ Z=\\begin{pmatrix} \\frac{x_{1 1}- \\overline{x}_{\\bullet 1}}{s_1}&amp; \\frac{x_{1 2}- \\overline{x}_{\\bullet 2}}{s_2} &amp;\\ldots &amp; \\frac{x_{1 p}- \\overline{x}_{\\bullet p}}{s_p}\\\\[2ex] \\frac{x_{2 1} - \\overline{x}_{\\bullet 1}}{s_1}&amp; \\frac{x_{2 2}- \\overline{x}_{\\bullet 2}}{s_2} &amp;\\ldots &amp; \\frac{x_{2 p}- \\overline{x}_{\\bullet p}}{s_p}\\\\[2ex] \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots\\\\ \\frac{x_{n 1} - \\overline{x}_{\\bullet 1}}{s_1}&amp; \\frac{x_{n 2}- \\overline{x}_{\\bullet 2}}{s_2} &amp;\\ldots &amp; \\frac{x_{n p}- \\overline{x}_{\\bullet p}}{s_p} \\end{pmatrix}. \\] Ejemplo 8.3 Vamos a tipificar a mano la tabla de datos \\[ {X}=\\left(\\begin{array}{rrr} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{array}\\right) \\] del Ejemplo 8.1. Ya la hemos centrado en el Ejemplo 8.2. Para tipificarla, tenemos que dividir cada columna de esta matriz centrada por la desviación típica de la columna correspondiente en la matriz original. Hemos calculado estas desviaciones típicas en el Ejemplo 8.1: sapply(X, sd_ver) ## V1 V2 V3 ## 0.8291562 1.5000000 1.2990381 Dividiendo cada columna de la matriz centrada \\(\\widetilde{X}\\) por la correspondiente desviación típica obtenemos: \\[ \\begin{array}{rl} Z &amp; =\\left(\\begin{array}{rrr} -0.75/0.8291562&amp;-1.5/1.5&amp;1.25/1.2990381\\\\ -0.75/0.8291562&amp;-0.5/1.5&amp;1.25/1.2990381\\\\ 0.25/0.8291562&amp;2.5/1.5&amp;-1.75/1.2990381\\\\ 1.25/0.8291562&amp;-0.5/1.5&amp;-0.75/1.2990381 \\end{array}\\right)\\\\ &amp; = \\left(\\begin{array}{rrr} -0.9045340&amp;-1.0000000&amp;0.9622504\\\\ -0.9045340&amp;-0.3333333&amp;0.9622504\\\\ 0.3015113&amp;1.6666667&amp;-1.3471506\\\\ 1.5075567&amp;-0.3333333&amp;-0.5773503 \\end{array}\\right) \\end{array} \\] La manera más sencilla de aplicar con R una transformación lineal a una tabla de datos \\(X\\), y en particular de centrarla o tipificarla, es usando la instrucción scale(X, center=..., scale=...) donde: X puede ser tanto una matriz como un data frame; el resultado será siempre una matriz. El valor del parámetro center es el vector que restamos a sus columnas, en el sentido de que cada entrada de este vector se restará a todas las entradas de la columna correspondiente. Su valor por defecto (que no es necesario especificar, aunque también se puede especificar con center=TRUE) es el vector \\(\\overline{X}\\) de medias de \\(X\\); para indicar que no se reste nada, podemos usar center=FALSE. El valor del parámetro scale es el vector por el que dividimos las columnas de \\(X\\): cada columna se divide por la entrada correspondiente de este vector. Su valor por defecto (que, de nuevo, se puede especificar igualando el parámetro a TRUE) es el vector \\(\\widetilde{s}_X\\) de desviaciones típicas muestrales; para indicar que no se divida por nada, podemos usar scale=FALSE. En particular, la instrucción scale(X) centra la tabla de datos \\(X\\) y divide sus columnas por sus desviaciones típicas muestrales; por lo tanto, no la tipifica según nuestra definición, ya que no las divide por sus desviaciones típicas “verdaderas”. Ejemplo 8.4 Recordemos la tabla de datos \\(X\\) del Ejemplo 8.1. X ## V1 V2 V3 ## 1 1 -1 3 ## 2 1 0 3 ## 3 2 3 0 ## 4 3 0 1 Su matriz centrada es: X_centrada=scale(X, center=TRUE, scale=FALSE) X_centrada ## V1 V2 V3 ## [1,] -0.75 -1.5 1.25 ## [2,] -0.75 -0.5 1.25 ## [3,] 0.25 2.5 -1.75 ## [4,] 1.25 -0.5 -0.75 ## attr(,&quot;scaled:center&quot;) ## V1 V2 V3 ## 1.75 0.50 1.75 Coincide con la matriz obtenida en el Ejemplo 8.2. Observad la estructura del resultado: en primer lugar nos da la matriz centrada, y a continuación nos dice que tiene un atributo llamado \"scaled:center\" cuyo valor es el vector usado para centrarla. Este atributo no interferirá para nada en las operaciones que realicéis con la matriz centrada, pero, si os molesta, recordad que se puede eliminar sustituyendo el resultado de centrar la matriz en los puntos suspensivos de la instrucción siguiente: attr(... , &quot;scaled:center&quot;)=NULL En nuestro ejemplo: attr(X_centrada, &quot;scaled:center&quot;)=NULL X_centrada ## V1 V2 V3 ## [1,] -0.75 -1.5 1.25 ## [2,] -0.75 -0.5 1.25 ## [3,] 0.25 2.5 -1.75 ## [4,] 1.25 -0.5 -0.75 Como ya hemos avisado, para tipificar esta tabla de datos no podemos hacer lo siguiente: X_tip=scale(X) X_tip ## V1 V2 V3 ## [1,] -0.7833495 -0.8660254 0.8333333 ## [2,] -0.7833495 -0.2886751 0.8333333 ## [3,] 0.2611165 1.4433757 -1.1666667 ## [4,] 1.3055824 -0.2886751 -0.5000000 ## attr(,&quot;scaled:center&quot;) ## V1 V2 V3 ## 1.75 0.50 1.75 ## attr(,&quot;scaled:scale&quot;) ## V1 V2 V3 ## 0.9574271 1.7320508 1.5000000 Para hacerlo bien según la definición que hemos dado, tenemos dos opciones. Una es multiplicar la matriz anterior por \\(\\sqrt{n/(n-1)}\\), donde \\(n\\) es el número de filas de la tabla. (El motivo es que, como \\(\\widetilde{s}_X=\\sqrt{\\frac{n}{n-1}}\\cdot s_X\\), se tiene que \\(\\frac{1}{s_X}=\\sqrt{\\frac{n}{n-1}}\\cdot \\frac{1}{\\widetilde{s}_X}\\); por lo tanto, si queríamos dividir por \\(s_X\\) y scale(X) ha dividido por \\(\\widetilde{s}_X\\), basta multiplicar su resultado por \\(\\sqrt{\\frac{n}{n-1}}\\) para obtener el efecto deseado.) n=dim(X)[1] #Número de filas de X X_tip=scale(X)*sqrt(n/(n-1)) X_tip V1 V2 V3 -0.9045340 -1.0000000 0.9622504 -0.9045340 -0.3333333 0.9622504 0.3015113 1.6666667 -1.3471506 1.5075567 -0.3333333 -0.5773503 Ahora sí que coincide con la matriz obtenida “a mano” en el Ejemplo 8.3. Otra posibilidad es usar, como valor del parámetro scale, el vector \\(s_X\\) de desviaciones típicas de las columnas. X_tip1=scale(X, scale=sapply(X, sd_ver)) X_tip1 ## V1 V2 V3 ## [1,] -0.9045340 -1.0000000 0.9622504 ## [2,] -0.9045340 -0.3333333 0.9622504 ## [3,] 0.3015113 1.6666667 -1.3471506 ## [4,] 1.5075567 -0.3333333 -0.5773503 ## attr(,&quot;scaled:center&quot;) ## V1 V2 V3 ## 1.75 0.50 1.75 ## attr(,&quot;scaled:scale&quot;) ## V1 V2 V3 ## 0.8291562 1.5000000 1.2990381 Observaréis que la matriz resultante es la misma, pero el atributo que indica el vector por el que hemos dividido las columnas es diferente: en este caso, es el de desviaciones típicas. Ahora, en ambos casos, podemos usar la función attr para eliminar los dos atributos, \"scaled:center\" y \"scaled:scale\", que se han añadido a la matriz tipificada. Por ejemplo: attr(X_tip, &quot;scaled:center&quot;)=NULL attr(X_tip, &quot;scaled:scale&quot;)=NULL X_tip ## V1 V2 V3 ## [1,] -0.9045340 -1.0000000 0.9622504 ## [2,] -0.9045340 -0.3333333 0.9622504 ## [3,] 0.3015113 1.6666667 -1.3471506 ## [4,] 1.5075567 -0.3333333 -0.5773503 8.3 Covarianzas y correlaciones La covarianza entre dos variables es una medida de la tendencia que tienen ambas variables a variar conjuntamente. Cuando la covarianza es positiva, si una de las dos variables crece o decrece, la otra tiene el mismo comportamiento; en cambio, cuando la covarianza es negativa, esta tendencia se invierte: si una variable crece, la otra decrece y viceversa. Puesto que interpretar el valor de la covarianza más allá de su signo es difícil, se suele usar una versión “normalizada” de la misma, la correlación de Pearson, que mide de manera más precisa la relación lineal entre dos variables. La covarianza generaliza la varianza, en el sentido de que la varianza de una variable es su covarianza consigo misma. Y como en el caso de la varianza, definiremos dos versiones de la covarianza: la “verdadera” y la muestral. La diferencia estará de nuevo en el denominador. Formalmente, la covarianza de las variables \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\) de una matriz de datos \\(X\\) es \\[ s_{i j}=\\frac{1}{n} \\sum_{k =1}^n\\big((x_{k i}-\\overline{{x}}_{\\bullet i})(x_{kj}-\\overline{{x}}_{\\bullet j})\\big)= \\frac{1}{n} \\Big(\\sum_{k =1}^n x_{k i} x_{k j}\\Big) - \\overline{{x}}_{\\bullet i} \\overline{{x}}_{\\bullet j}, \\] y su covarianza muestral es \\[ \\widetilde{s}_{ij} = \\frac{1}{n-1} \\sum_{k =1}^n\\big((x_{k i}-\\overline{{x}}_{\\bullet i})(x_{kj}-\\overline{{x}}_{\\bullet j})\\big)= \\frac{n}{n-1} s_{ij}. \\] El estadístico \\(\\tilde{s}_{ij}\\) es siempre un estimador insesgado de la covarianza \\(\\sigma_{i j}\\) de las variables aleatorias \\(X_i\\) y \\(X_j\\) de las que \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\) son muestras, mientras que \\(s_{i j}\\) es su estimador máximo verosímil cuando la distribución conjunta de \\(X_i\\) y \\(X_j\\) es normal bivariante. Es inmediato comprobar a partir de sus definiciones que ambas covarianzas son simétricas, y que la covarianza de una variable consigo misma es su varianza: \\[ s_{i j}= s_{j i}, \\quad \\widetilde{s}_{i j}= \\widetilde{s}_{j i}, \\quad s_{i i}=s_{i}^2, \\quad \\widetilde{s}_{ii}=\\widetilde{s}_i^2. \\] Ejemplo 8.5 La covarianza de las dos primeras columnas de la matriz de datos \\[ X=\\left(\\begin{array}{rrr} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{array}\\right) \\] del Ejemplo 8.1 se calcularía de la manera siguiente: \\[ s_{12}=\\frac{1}{4}(1\\cdot (-1)+1\\cdot 0+2\\cdot 3+3\\cdot 0)-1.75\\cdot 0.5= 1.25-0.875=0.375 \\] Su covarianza muestral se obtendría multiplicando por \\(4/3\\) este valor: \\[ \\widetilde{s}_{12} = \\frac{4}{3} s_{12}=0.5. \\] La covarianza muestral de dos vectores numéricos de la misma longitud \\(n\\) se puede calcular con R mediante la función cov. Para obtener su covarianza “verdadera”, hay que multiplicar el resultado de cov por \\((n-1)/n\\). Ejemplo 8.6 La covarianza muestral de las dos primeras columnas de la tabla de datos \\(X\\), que tenemos guardada en el data frame X, es: cov(X$V1, X$V2) ## [1] 0.5 y su covarianza “verdadera” es: n=dim(X)[1] ((n-1)/n)*cov(X$V1, X$V2) ## [1] 0.375 Queremos recalcar que, como en el caso de la varianza con var, R calcula con cov la versión muestral de la covarianza. Las matrices de covarianzas y de covarianzas muestrales de una tabla de datos \\(X\\) son, respectivamente, \\[ {S}= \\begin{pmatrix} s_{1 1} &amp; s_{1 2} &amp; \\ldots &amp; s_{1 p}\\\\ s_{2 1} &amp; s_{2 2} &amp; \\ldots &amp; s_{2 p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ s_{p 1} &amp; s_{p 2} &amp; \\ldots &amp; s_{p p} \\end{pmatrix},\\ \\widetilde{{S}}= \\begin{pmatrix} \\widetilde{s}_{1 1} &amp; \\widetilde{s}_{1 2} &amp; \\ldots &amp; \\widetilde{s}_{1 p}\\\\ \\widetilde{s}_{2 1} &amp; \\widetilde{s}_{2 2} &amp; \\ldots &amp; \\widetilde{s}_{2 p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\widetilde{s}_{p 1} &amp; \\widetilde{s}_{p 2} &amp; \\ldots &amp; \\widetilde{s}_{p p} \\end{pmatrix}, \\] donde cada \\(s_{i j}\\) y cada \\(\\widetilde{s}_{i j}\\) son, respectivamente, la covarianza y la covarianza muestral de las correspondientes columnas \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\). Estas matrices de covarianzas miden la tendencia a la variabilidad conjunta de los datos de \\(X\\) y, si \\(n\\) es el número de filas de \\(X\\), se tiene que \\[ S=\\frac{n-1}{n}\\widetilde{{S}}. \\] La matriz de covarianzas muestrales \\(\\widetilde{{S}}\\) es un estimador insesgado de la matriz de covarianzas \\(\\Sigma\\) del vector de variables aleatorias \\(\\underline{X}\\), y si este tiene distribución normal multivariante, \\(S\\) es un estimador máximo verosímil de \\(\\Sigma\\). Ambas matrices de covarianzas son simétricas, puesto que \\(s_{i j}=s_{j i}\\), y tienen todos sus valores propios \\(\\geq 0\\). La matriz de covarianzas muestrales de una tabla de datos se calcula aplicando la función cov al data frame o a la matriz que contenga dicha tabla. Para obtener su matriz de covarianzas “verdaderas”, es suficiente multiplicar el resultado de cov por \\((n-1)/n\\), donde \\(n\\) es el número de filas de la tabla de datos. Ejemplo 8.7 Continuemos con la matriz \\[ X=\\left(\\begin{array}{rrr} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{array}\\right) \\] del Ejemplo 8.1. Su matriz de covarianzas muestrales es cov(X) ## V1 V2 V3 ## V1 0.9166667 0.500000 -1.083333 ## V2 0.5000000 3.000000 -2.166667 ## V3 -1.0833333 -2.166667 2.250000 y su matriz de covarianzas es n=dim(X)[1] ((n-1)/n)*cov(X) ## V1 V2 V3 ## V1 0.6875 0.375 -0.8125 ## V2 0.3750 2.250 -1.6250 ## V3 -0.8125 -1.625 1.6875 Como la matriz de covarianzas es difícil de interpretar como medida de variabilidad de una tabla de datos, debido a que no es una única cantidad sino toda una matriz, interesa cuantificar esta variabilidad mediante un único índice. No hay consenso sobre este índice, y entre los que se usan destacamos: La varianza total de \\(X\\): la suma de las varianzas de sus columnas. La varianza media de \\(X\\): la media de las varianzas de sus columnas, es decir, la varianza total partida por el número de columnas. La varianza generalizada de \\(X\\): el determinante de su matriz de covarianzas. La desviación típica generalizada de \\(X\\): la raíz cuadrada positiva de su varianza generalizada. De cada uno de estos índices se definen, naturalmente, una versión muestral y una “verdadera”, según el tipo de varianzas o covarianzas que se usen en su cálculo. Pasemos ahora a la correlación lineal de Pearson (o, de ahora en adelante, simplemente correlación de Pearson) de dos variables \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\) de \\(X\\), que se define como \\[ r_{i j}=\\frac{s_{i j}}{s_i\\cdot s_j}. \\] Observad que \\[ \\frac{\\widetilde{s}_{i j}}{\\widetilde{s}_i\\cdot \\widetilde{s}_j}= \\frac{\\frac{n}{n-1}\\cdot {s}_{i j}}{\\sqrt{\\frac{n}{n-1}}\\cdot {s}_i \\cdot\\sqrt{\\frac{n}{n-1}}\\cdot{s}_j}= \\frac{s_{i j}}{s_i \\cdot s_j}=r_{i j}, \\] y, por lo tanto, esta correlación se puede calcular también a partir de las versiones muestrales de la covarianza y las desviaciones típicas por medio de la misma fórmula. El estadístico \\(r_{ij}\\) es un estimador máximo verosímil de la correlación de Pearson \\(\\rho_{i j}=Cor(X_i,X_j)\\) de las variables aleatorias \\(X_i\\) y \\(X_j\\) cuando su distribución conjunta es normal bivariante, y aunque es sesgado, su sesgo tiende a 0 cuando \\(n\\) tiende a \\(\\infty\\). Las propiedades más importantes de \\(r_{i,j}\\) son las siguientes: Es simétrica: \\(r_{i j}=r_{j i}\\). \\(-1\\leq r_{i j}\\leq 1\\). \\(r_{i i}=1\\). \\(r_{i j}\\) tiene el mismo signo que \\(s_{i j}\\). \\(r_{i j}=\\pm 1\\) si y, sólo si, existe una relación lineal perfecta entre las variables \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\): es decir, si, y sólo si, existen valores \\(a, b\\in \\mathbb{R}\\) tales que \\[ \\left(\\begin{array}{c} x_{1j}\\\\ \\vdots \\\\ x_{nj}\\end{array}\\right)= a\\cdot \\left(\\begin{array}{c} x_{1i}\\\\ \\vdots \\\\ x_{ni}\\end{array}\\right) +b. \\] La pendiente \\(a\\) de esta relación lineal tiene el mismo signo que \\(r_{i j}\\). El coeficiente de determinación \\(R^2\\) de la regresión lineal por mínimos cuadrados de \\({x}_{\\bullet j}\\) respecto de \\({x}_{\\bullet i}\\) es igual al cuadrado de su correlación de Pearson, \\(r_{i j}^2\\); por lo tanto, cuánto más se aproxime el valor absoluto de \\(r_{ij}\\) a 1, más se acercan las variables \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\) a depender linealmente la una de la otra. Así pues, la correlación de Pearson entre dos variables viene a ser una covarianza “normalizada”, ya que, como vemos, su valor está entre -1 y 1, y mide la tendencia de las variables a estar relacionadas según una función lineal. En concreto, cuanto más se acerca dicha correlación a 1 (respectivamente, a -1), más se acerca una (cualquiera) de las variables a ser función lineal creciente (respectivamente, decreciente) de la otra. Con R, la correlación de Pearson de dos vectores se puede calcular aplicándoles la función cor. Ejemplo 8.8 En ejemplos anteriores hemos calculado la covarianza y las varianzas de las dos primeras columnas de la matriz de datos \\[ {X}=\\left(\\begin{array}{rrr} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{array}\\right) \\] Hemos obtenido los valores siguientes \\[ s_{12}=0.375,\\quad s_1=0.8291562,\\quad s_2=1.5. \\] Por lo tanto, su correlación de Pearson es \\[ r_{1 2}=\\frac{0.375}{0.8291562\\cdot 1.5}=0.3015113. \\] Ahora vamos a calcularla con R, y aprovecharemos para confirmar su relación con el valor de \\(R^2\\) de la regresión lineal de la segunda columna respecto de la primera. Recordemos que esta tabla de datos sigue guardada en el data frame X. X ## V1 V2 V3 ## 1 1 -1 3 ## 2 1 0 3 ## 3 2 3 0 ## 4 3 0 1 La correlación de sus dos primeras columnas es: cor(X$V1, X$V2) ## [1] 0.3015113 que coincide con el valor obtenido “a mano”. Comprobemos ahora que su cuadrado es igual al valor de \\(R^2\\) de la regresión lineal: cor(X$V1, X$V2)^2 ## [1] 0.09090909 summary(lm(X$V2~X$V1))$r.squared ## [1] 0.09090909 La matriz de correlaciones de Pearson de \\(X\\) es \\[ {R}= \\begin{pmatrix} 1 &amp; r_{1 2} &amp; \\ldots &amp; r_{1 p}\\\\ r_{2 1} &amp; 1 &amp; \\ldots &amp; r_{2 p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ r_{p 1} &amp; r_{p 2} &amp; \\ldots &amp; 1 \\end{pmatrix}, \\] donde cada \\(r_{i j}\\) es la correlación de Pearson de las columnas correspondientes de \\(X\\). Esta matriz de correlaciones tiene siempre determinante entre 0 y 1 (ambos extremos incluidos) y todos sus valores propios son mayores o iguales que 0, y con R se puede calcular aplicando la misma instrucción cor a la tabla de datos, sea en forma de matriz o de data frame. Así, la matriz de correlaciones de nuestra tabla de datos \\(X\\) es: cor(X) ## V1 V2 V3 ## V1 1.0000000 0.3015113 -0.7543365 ## V2 0.3015113 1.0000000 -0.8339504 ## V3 -0.7543365 -0.8339504 1.0000000 Se tiene el teorema siguiente, que se puede demostrar mediante un simple, aunque farragoso, cálculo algebraico: Teorema 8.1 La matriz de correlaciones de Pearson de \\(X\\) es igual a: La matriz de covarianzas de su matriz tipificada. La matriz de covarianzas muestrales de su matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales en vez de por las “verdaderas”. La importancia de este resultado es que, si la tabla de datos es muy grande, suele ser más eficiente calcular la matriz de covarianzas de su matriz tipificada que la matriz de correlaciones de Pearson de la tabla original. Observad, por otro lado, que las dos matrices de covarianzas mencionadas en el enunciado coinciden, puesto que la matriz tipificada se obtiene multiplicando por \\(\\sqrt{n/(n-1)}\\) la matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales. Esto implica que la matriz de covarianzas muestrales de la matriz tipificada se obtiene multiplicando por \\(n/(n-1)\\) la matriz de covarianzas muestrales de la matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales. Finalmente, la matriz de covarianzas se obtiene a partir de la de covarianzas muestrales multiplicándola por \\((n-1)/n\\). Entonces, los factores \\(n/(n-1)\\) y \\((n-1)/n\\) se compensan y resulta que la matriz de covarianzas de la matriz tipificada coincide con la matriz de covarianzas muestrales de la matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales. Recordemos que si aplicamos la función scale a una tabla de datos \\(X\\), la tipifica dividiendo por las desviaciones típicas muestrales. Por lo tanto, otra manera de reformular el teorema anterior es decir que cor(X) da lo mismo que cov(scale(X)). Comprobemos esta igualdad para nuestra matriz de datos \\(X\\). cor(X) ## V1 V2 V3 ## V1 1.0000000 0.3015113 -0.7543365 ## V2 0.3015113 1.0000000 -0.8339504 ## V3 -0.7543365 -0.8339504 1.0000000 cov(scale(X)) ## V1 V2 V3 ## V1 1.0000000 0.3015113 -0.7543365 ## V2 0.3015113 1.0000000 -0.8339504 ## V3 -0.7543365 -0.8339504 1.0000000 Cuando se calcula la covarianza o la correlación de Pearson de dos vectores que contienen valores NA, lo usual es no tenerlos en cuenta: es decir, si un vector contiene un NA en una posición, se eliminan de los dos vectores sus entradas en dicha posición. De esta manera, se tomaría como covarianza de \\[ \\left(\\begin{array}{c} 1\\\\ 2\\\\ NA\\\\ 4\\\\ 6\\\\ 2\\end{array}\\right)\\mbox{ y } \\left(\\begin{array}{c} 2\\\\ 4\\\\ -3\\\\ 5\\\\ 7\\\\ NA \\end{array}\\right) \\] la de \\[ \\left(\\begin{array}{c}1\\\\ 2\\\\ 4\\\\ 6\\end{array}\\right)\\mbox{ y } \\left(\\begin{array}{c} 2\\\\ 4\\\\ 5\\\\ 7\\end{array}\\right). \\] Como ya nos pasaba con las funciones de estadística descriptiva univariante com mean o var, cuando aplicamos cov o cor a un par de vectores que contengan entradas NA, obtenemos por defecto NA. En las funciones univariantes usábamos na.rm=TRUE para pedir a R que obviara los NA, pero esta solución ahora no es posible, porque las posiciones de los NA también cuentan, y si los borramos tal cual se desmonta el emparejamiento de los datos. Así que, si se quiere que R calcule el valor de cov o cor sin tener en cuenta los NA, se ha de especificar añadiendo el parámetro use=\"complete.obs\", que le indica que ha de usar las observaciones completas, es decir, las posiciones que no tienen NA en ninguno de los dos vectores. Veamos el efecto sobre los dos vectores anteriores. Llamémosles \\(x\\) e \\(y\\), y sean \\(x_1\\) e \\(y_1\\) los vectores que se obtienen eliminando las entradas que contienen un NA en alguno de los dos vectores. x=c(1,2,NA,4,6,2) y=c(2,4,-3,5,7,NA) x1=x[is.na(x)!=TRUE &amp; is.na(y)!=TRUE] y1=y[is.na(x)!=TRUE &amp; is.na(y)!=TRUE] x1 ## [1] 1 2 4 6 y1 ## [1] 2 4 5 7 Si calculamos la covarianza de \\(x\\) e \\(y\\) tal cual con la función cov, da NA: cov(x, y) ## [1] NA Usando use=\"complete.obs\", obtenemos la covarianza de \\(x_1\\) e \\(y_1\\): cov(x, y, use=&quot;complete.obs&quot;) ## [1] 4.5 cov(x1, y1) ## [1] 4.5 Lo mismo sucede con la función cor: cor(x, y) ## [1] NA cor(x, y, use=&quot;complete.obs&quot;) ## [1] 0.9749135 cor(x1, y1) ## [1] 0.9749135 Al calcular las matrices de covarianzas o correlaciones de una tabla de datos que contenga valores NA, se suele seguir una de las dos estrategias siguientes, según lo que interese al usuario: Para cada par de columnas, se calcula su covarianza o su correlación con la estrategia explicada más arriba para dos vectores, obviando el hecho de que forman parte de una tabla de datos mayor; es decir, al efectuar el cálculo para cada par de columnas concreto, se eliminan de cada una de ellas solo las entradas de las filas en las que alguna de las dos tiene un NA, sin tener en cuenta para nada las otras columnas. Esta opción se especifica dentro de la función cov o cor con el parámetro use=\"pairwise.complete.obs\". Antes de nada, se eliminan las filas de la tabla que contienen algún NA en alguna columna, dejando solo en la tabla las filas “completas”, las que no contienen ningún NA. Luego se calcula la matriz de covarianzas o de correlaciones de la tabla resultante. Esta opción se especifica con el parámetro use=\"complete.obs\". Veamos un ejemplo. Consideremos la matriz de datos \\(Y\\) siguiente, cuyas dos primeras columnas son los vectores \\(x\\) e \\(y\\) anteriores: Y=cbind(c(1,2,NA,4,6,2), c(2,4,-3,5,7,NA), c(-2,1,0,2,NA,0)) Y ## [,1] [,2] [,3] ## [1,] 1 2 -2 ## [2,] 2 4 1 ## [3,] NA -3 0 ## [4,] 4 5 2 ## [5,] 6 7 NA ## [6,] 2 NA 0 Supongamos que queremos calcular su matriz de correlaciones de Pearson. Como todas las filas de \\(Y\\) tienen entradas NA, todas las correlaciones fuera de la diagonal dan NA (R sabe que la correlación de un columna consigo misma siempre es 1, y ya no la calcula): cor(Y) ## [,1] [,2] [,3] ## [1,] 1 NA NA ## [2,] NA 1 NA ## [3,] NA NA 1 Una opción es calcular las correlaciones de Pearson de cada par de variables eliminando sus valores NA pero sin tener en cuenta los posibles valores NA de la otra variable: cor(Y, use=&quot;pairwise.complete.obs&quot;) ## [,1] [,2] [,3] ## [1,] 1.0000000 0.9749135 0.8919017 ## [2,] 0.9749135 1.0000000 0.4387268 ## [3,] 0.8919017 0.4387268 1.0000000 Observad que la entrada (1,2) de esta matriz es la correlación de los vectores \\(x\\) e \\(y\\) calculada con use=\"complete.obs\". Calculemos ahora la matriz de correlaciones de Pearson de la matriz con filas completas: cor(Y, use=&quot;complete.obs&quot;) ## [,1] [,2] [,3] ## [1,] 1.0000000 0.9285714 0.8910421 ## [2,] 0.9285714 1.0000000 0.9958706 ## [3,] 0.8910421 0.9958706 1.0000000 Veamos que efectivamente coincide con la matriz de correlaciones de Pearson de la matriz que se obtiene eliminando las filas que contienen algún NA. Esta matriz es: noNAs=is.na(Y[,1])!=TRUE &amp; is.na(Y[,2])!=TRUE &amp; is.na(Y[,3])!=TRUE Y1=Y[noNAs,] Y1 ## [,1] [,2] [,3] ## [1,] 1 2 -2 ## [2,] 2 4 1 ## [3,] 4 5 2 y su matriz de correlaciones de Pearson es: cor(Y1) ## [,1] [,2] [,3] ## [1,] 1.0000000 0.9285714 0.8910421 ## [2,] 0.9285714 1.0000000 0.9958706 ## [3,] 0.8910421 0.9958706 1.0000000 8.4 Correlación de Spearman La correlación de Pearson mide específicamente la tendencia de dos variables cuantitativas continuas a depender linealmente una de otra. En circunstancias en las que no esperemos esta dependencia lineal, o en las que nuestras variables sean cuantitativas discretas o simplemente cualitativas, usar la correlación de Pearson para analizar la relación entre dos variables no es lo más adecuado. Entre las propuestas alternativas, la más popular es la correlación de Spearman. Este índice asigna a cada valor de cada vector su rango (su posición en el vector ordenado de menor a mayor, y en caso de empates la media de las posiciones que ocuparían todos los empates) y calcula la correlación de Pearson de estos rangos. Con R, la correlación de Spearman se calcula directamente con la función cor entrándole el parámetro method=\"spearman\". (El valor por defecto del parámetro method es \"pearson\" y por eso no lo indicamos cuando calculamos la correlación de Pearson.) Ejemplo 8.9 Vamos a calcular la correlación de Spearman de las dos primeras columnas de la matriz de datos \\(X\\) que hemos venido usando en nuestros ejemplos. En la tabla siguiente calculamos los rangos de sus entradas: \\[ \\begin{array}{|c|c|c|c|} \\hline {x}_{\\bullet 1}&amp; rango &amp; {x}_{\\bullet 2}&amp; rango \\\\\\hline\\hline 1&amp; 1.5 &amp; -1&amp; 1 \\\\ 1&amp;1.5 &amp; 0 &amp; 2.5\\\\ 2&amp;3 &amp; 3&amp; 4 \\\\ 3&amp;4 &amp; 0&amp; 2.5\\\\\\hline \\end{array} \\] ¿Cómo hemos obtenido los rangos? Fijaos por ejemplo en la primera columna: los dos 1 ocuparían la posición 1 y 2, les asignamos a ambos como rango la media de estas posiciones, 1.5; el 2 ocuparía la posición 3 y el 3 ocuparía la posición 4, y estos son también sus rangos. Dejamos como ejercicio que comprobéis los rangos de los elementos de \\(x_{\\bullet 2}\\). Con R estos rangos se calculan con la función rank. Así, los rangos de los elementos de \\(x_{\\bullet 1}\\) son rank(X$V1) ## [1] 1.5 1.5 3.0 4.0 y los de los elementos de \\(x_{\\bullet 2}\\) son rank(X$V2) ## [1] 1.0 2.5 4.0 2.5 Por lo tanto, la correlación de Spearman de \\[ (1,1,2,3)\\mbox{ y }(-1,0,3,0) \\] es la correlación de Pearson de \\[ (1.5, 1.5, 3, 4)\\mbox{ y }(1, 2.5, 4, 2.5) \\] Veámoslo: cor(X$V1,X$V2,method=&quot;spearman&quot;) ## [1] 0.5 cor(rank(X$V1),rank(X$V2)) ## [1] 0.5 8.5 Contrastes de correlación Como ya hemos comentado, podemos usar la correlación de Pearson \\(r_{xy}\\) de dos vectores \\(x\\) e \\(y\\), formados por los valores de dos variables cuantitativas \\(X,Y\\) medidos sobre una misma muestra de individuos, para estimar la correlación \\(\\rho_{XY}\\) de estas variables poblacionales. Cuando además el vector aleatorio \\((X,Y)\\) tiene distribución normal bivariante, disponemos de una fórmula para calcular intervalos de confianza para la correlación poblacional y de un método para efectuar contrastes de hipótesis con hipótesis nula \\(H_0: \\rho_{XY}=0\\) (“no hay correlación entre \\(X\\) e \\(Y\\)”). No vamos a entrar en los detalles de las fórmulas ni de los teoremas en que se basan, pero es importante que recordéis que la función de R que lleva a cabo dichos contrastes “de correlación” es la función cor.test. En particular, esta función calcula el intervalo de confianza asociado a un contraste de estos: si el contraste es bilateral, es decir, con hipótesis alternativa \\(H_1: \\rho_{XY}\\neq 0\\), el intervalo que produce esta función es el intervalo de confianza usual para \\(\\rho_{XY}\\) con nivel de confianza correspondiente al nivel de significación del contraste. La sintaxis de cor.test es la misma que la del resto de funciones para realizar contrastes de hipótesis básicos: cor.test(x, y, alternative=..., conf.level=...) donde x e y son los dos vectores de datos, que también se pueden especificar mediante una fórmula. Estos dos vectores han de tener la misma longitud, puesto que se entiende que son mediciones sobre el mismo conjunto de individuos. El parámetro alternative puede tomar los tres valores usuales y su valor por defecto es, como siempre, \"two.sided\", que corresponde al contraste bilateral, con hipótesis alternativa \\(H_1: \\rho_{XY}\\neq 0\\). Los valores alternative=\"greater\" y alternative=\"less\" permiten contrastar si \\(X\\) e \\(Y\\) tienen correlación mayor o menor que 0, respectivamente. Como en el resto de funciones de contrastes, el resultado es una list que, entre otros objetos, contiene: p.value: El p-valor del test. conf.int: Un intervalo de confianza del nivel de confianza especificado. estimate: El valor de la correlación de Pearson (calculado con use=\"complete.obs\" si algún vector contiene valores NA). Ejemplo 8.10 Queremos contrastar si hay correlación positiva entre el peso de una madre en el momento de la concepción del hijo y el peso de su hijo en el momento de nacer. Para ello vamos a usar la tabla de datos birthwt incluida en el paquete MASS que ya usamos en una lección anterior, que contiene información sobre recién nacidos y sus madres, y que en particular dispone de las variables bwt, que da el peso del recién nacido en gramos, y lwt, que da el peso de la madre en libras en el momento de su última menstruación. Vamos a suponer que ambos pesos siguen distribuciones normales. Si denotamos por \\(X\\) e \\(Y\\) las correspondientes variables poblacionales, queremos realizar el contraste \\[ \\left\\{\\begin{array}{l} H_0: \\rho_{XY}=0\\\\ H_1: \\rho_{XY}&gt;0 \\end{array}\\right. \\] Vamos a usar la función cor.test. library(MASS) cor.test(birthwt$bwt, birthwt$lwt, alternative=&quot;greater&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: birthwt$bwt and birthwt$lwt ## t = 2.5848, df = 187, p-value = 0.005252 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.06720637 1.00000000 ## sample estimates: ## cor ## 0.1857333 El p-valor 0.005 nos da evidencia estadísticamente significativa de que, en efecto, hay una correlación positiva entre el peso de la madre y el peso del recién nacido. El último valor, el 0.1857333 bajo el cor, es la correlación de Pearson de los dos vectores de pesos, y el 95 percent confidence interval es el intervalo de confianza del 95% del contraste unilateral planteado y nos dice que tenemos un 95% de confianza en que la correlación entre el peso de la madre y el peso del recién nacido es superior a 0.067. Podríamos haber obtenido el p-valor del contraste de correlación anterior directamente con la instrucción cor.test(birthwt$bwt, birthwt$lwt, alternative=&quot;greater&quot;)$p.value ## [1] 0.005252088 Si hubiéramos querido calcular un intervalo de confianza del 95% para \\(\\rho_{XY}\\) que repartiera por igual a ambos lados el 5% de probabilidad de no contener su valor real, hubiéramos podido usar el intervalo de confianza del contraste bilateral: cor.test(birthwt$bwt, birthwt$lwt)$conf.int ## [1] 0.04417405 0.31998094 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 La potencia de un contraste de correlación se calcula con la función pwr.r.test del paquete pwr. En este caso, el tamaño del efecto es simplemente la correlación de Pearson, que se entra en la función mediante el parámetro r. Apliquémosla para calcular la potencia del contraste de correlación anterior: library(pwr) dim(birthwt) ## [1] 189 10 round(cor(birthwt$bwt,birthwt$lwt),4) ## [1] 0.1857 pwr.r.test(n=189,r=0.1857,sig.level=0.05,alternative=&quot;greater&quot;) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 189 ## r = 0.1857 ## sig.level = 0.05 ## power = 0.8223733 ## alternative = greater La probabilidad de error de tipo II en este contraste era de un poco menos del 18%. Si quisiéramos realizar este contraste de correlación con una potencia del 90% suponiendo que la magnitud del efecto va ser pequeña, usaríamos primero cohen.ES con test=\"r\" para determinar qué magnitud del efecto se considera pequeña y a continuación pwr.r.test dejando sin especificar la n: cohen.ES(test=&quot;r&quot;,size=&quot;small&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = r ## size = small ## effect.size = 0.1 pwr.r.test(power=0.9,r=0.1,sig.level=0.05,alternative=&quot;greater&quot;) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 852.6473 ## r = 0.1 ## sig.level = 0.05 ## power = 0.9 ## alternative = greater Hubiéramos necesitado datos de al menos 853 recién nacidos. 8.6 Un ejemplo Recordaréis el data frame iris, que tabulaba las longitudes y anchuras de los pétalos y los sépalos de una muestra de flores iris de tres especies. Vamos a extraer una subtabla con sus cuatro variables numéricas, que llamaremos iris_num, y calcularemos sus matrices de covarianzas y correlaciones. str(iris) ## &#39;data.frame&#39;:\t150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... iris_num=iris[, 1:4] n=dim(iris_num)[1] #Número de filas Su matriz de covarianzas muestrales es: cov(iris_num) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 0.6856935 -0.0424340 1.2743154 0.5162707 ## Sepal.Width -0.0424340 0.1899794 -0.3296564 -0.1216394 ## Petal.Length 1.2743154 -0.3296564 3.1162779 1.2956094 ## Petal.Width 0.5162707 -0.1216394 1.2956094 0.5810063 Su matriz de covarianzas “verdaderas” es: cov(iris_num)*(n-1)/n ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 0.68112222 -0.04215111 1.2658200 0.5128289 ## Sepal.Width -0.04215111 0.18871289 -0.3274587 -0.1208284 ## Petal.Length 1.26582000 -0.32745867 3.0955027 1.2869720 ## Petal.Width 0.51282889 -0.12082844 1.2869720 0.5771329 Su matriz de correlaciones de Pearson es: cor(iris_num) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 Observamos, por ejemplo, una gran correlación de Pearson positiva entre la longitud y la anchura de los pétalos, 0.963, lo que indica una estrecha relación lineal con pendiente positiva entre estas magnitudes. Valdría la pena, entonces, calcular la recta de regresión lineal de una de estas medidas en función de la otra. lm(Petal.Length~Petal.Width, data=iris_num) ## ## Call: ## lm(formula = Petal.Length ~ Petal.Width, data = iris_num) ## ## Coefficients: ## (Intercept) Petal.Width ## 1.084 2.230 En cambio, la correlación de Pearson entre la longitud y la anchura de los sépalos es -0.1175698, muy cercana a cero, lo que es señal de que la variación conjunta de las longitudes y anchuras de los sépalos no tiene una tendencia clara. Vamos a ordenar ahora los pares de variables numéricas de iris en orden decreciente de su correlación en valor absoluto, para saber cuáles están más correlacionadas (en positivo o negativo). Para ello, en primer lugar creamos un data frame cuyas filas están formadas por pares diferentes de variables numéricas de iris, su correlación de Pearson y el valor absoluto de esta última, y a continuación ordenamos las filas de este data frame en orden decreciente de estos valores absolutos. Todo esto lo llevamos a cabo en el siguiente bloque de código, que luego explicamos: medidas=names(iris_num) n=length(medidas) #En este caso, n=4 indices=upper.tri(diag(n)) medida1=matrix(rep(medidas, times=n), nrow=n, byrow=FALSE)[indices] medida2=matrix(rep(medidas, times=n), nrow=n, byrow=TRUE)[indices] corrs=as.vector(cor(iris_num))[indices] corrs.abs=abs(corrs) corrs_df=data.frame(medida1, medida2, corrs, corrs.abs) corrs_df_sort=corrs_df[order(corrs_df$corrs.abs, decreasing=TRUE), ] corrs_df_sort ## medida1 medida2 corrs corrs.abs ## 6 Petal.Length Petal.Width 0.9628654 0.9628654 ## 2 Sepal.Length Petal.Length 0.8717538 0.8717538 ## 4 Sepal.Length Petal.Width 0.8179411 0.8179411 ## 3 Sepal.Width Petal.Length -0.4284401 0.4284401 ## 5 Sepal.Width Petal.Width -0.3661259 0.3661259 ## 1 Sepal.Length Sepal.Width -0.1175698 0.1175698 Vemos que el par de variables con mayor correlación de Pearson en valor absoluto son Petal.Length y Petal.Width, como ya habíamos observado, seguidos por Petal.Length y Sepal.Length. Vamos a explicar el código. La función upper.tri, aplicada a una matriz cuadrada \\(M\\), produce la matriz triangular superior de valores lógicos del mismo orden que \\(M\\), cuyas entradas \\((i,j)\\) con \\(i&lt;j\\) son todas TRUE y el resto todas FALSE. Existe una función similar, lower.tri, para producir matrices triangulares inferiores de valores lógicos. upper.tri(diag(4)) ## [,1] [,2] [,3] [,4] ## [1,] FALSE TRUE TRUE TRUE ## [2,] FALSE FALSE TRUE TRUE ## [3,] FALSE FALSE FALSE TRUE ## [4,] FALSE FALSE FALSE FALSE lower.tri(diag(4)) ## [,1] [,2] [,3] [,4] ## [1,] FALSE FALSE FALSE FALSE ## [2,] TRUE FALSE FALSE FALSE ## [3,] TRUE TRUE FALSE FALSE ## [4,] TRUE TRUE TRUE FALSE Ambas funciones disponen del parámetro diag que, igualado a TRUE, define también como TRUE las entradas de la diagonal principal. upper.tri(diag(4), diag=TRUE) ## [,1] [,2] [,3] [,4] ## [1,] TRUE TRUE TRUE TRUE ## [2,] FALSE TRUE TRUE TRUE ## [3,] FALSE FALSE TRUE TRUE ## [4,] FALSE FALSE FALSE TRUE Si \\(M\\) es una matriz y \\(L\\) es una matriz de valores lógicos del mismo orden, M[L] produce el vector construido de la manera siguiente: de cada columna, se queda sólo con las entradas de \\(M\\) cuya entrada correspondiente en \\(L\\) es TRUE, y a continuación concatena estas columnas, de izquierda a derecha, en un vector. Así, por ejemplo, tomemos la matriz \\(M\\) siguiente: M=matrix(1:16, nrow=4, byrow=T) M ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ## [4,] 13 14 15 16 El vector formado por las entradas de su triángulo superior, concatenadas por columnas, se obtiene de la manera siguiente: M[upper.tri(diag(4))] ## [1] 2 3 7 4 8 12 Ahora definimos las matrices siguientes, formadas por 4 copias (la primera por columnas, la segunda, por filas) del vector, al que hemos llamado medidas, de nombres de las variables numéricas de iris: matrix(rep(medidas, times=4), nrow=4, byrow=FALSE) ## [,1] [,2] [,3] [,4] ## [1,] &quot;Sepal.Length&quot; &quot;Sepal.Length&quot; &quot;Sepal.Length&quot; &quot;Sepal.Length&quot; ## [2,] &quot;Sepal.Width&quot; &quot;Sepal.Width&quot; &quot;Sepal.Width&quot; &quot;Sepal.Width&quot; ## [3,] &quot;Petal.Length&quot; &quot;Petal.Length&quot; &quot;Petal.Length&quot; &quot;Petal.Length&quot; ## [4,] &quot;Petal.Width&quot; &quot;Petal.Width&quot; &quot;Petal.Width&quot; &quot;Petal.Width&quot; matrix(rep(medidas, times=4), nrow=4, byrow=TRUE) ## [,1] [,2] [,3] [,4] ## [1,] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [2,] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [3,] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [4,] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; Al aplicar estas dos matrices a la matriz de valores lógicos upper.tri(diag(4)) obtenemos los nombres de las variables correspondientes a las filas y las columnas del triángulo superior, respectivamente, y al aplicar la matriz de correlaciones a esta matriz de valores lógicos, obtenemos sus entradas en este triángulo; en los tres vectores, las entradas siguen el mismo orden. Esto nos permite construir el data frame corrs_df cuyas filas están formadas por pares diferentes de variables numéricas de iris, su correlación de Pearson (columna corrs) y, aplicando abs a esta última variable, dicha correlación en valor absoluto (columna corrs.abs). corrs_df ## medida1 medida2 corrs corrs.abs ## 1 Sepal.Length Sepal.Width -0.1175698 0.1175698 ## 2 Sepal.Length Petal.Length 0.8717538 0.8717538 ## 3 Sepal.Width Petal.Length -0.4284401 0.4284401 ## 4 Sepal.Length Petal.Width 0.8179411 0.8179411 ## 5 Sepal.Width Petal.Width -0.3661259 0.3661259 ## 6 Petal.Length Petal.Width 0.9628654 0.9628654 Finalmente, la función order ordena los valores del vector al que se aplica, en orden decreciente si se especifica el parámetro decreasing=TRUE. Cuando aplicamos un data frame a una de sus variables reordenada de esta manera, reordena sus filas según el orden de esta variable. En este caso hubiéramos conseguido lo mismo con la función sort, pero la función order se puede aplicar a más de una variable del data frame: esto permite ordenar las filas del data frame en el orden de la primera variable de manera que, en caso de empate, queden ordenadas por la segunda variable, y así sucesivamente. 8.7 Representación gráfica de datos multidimensionales La representación gráfica de tablas de datos multidimensionales tiene la dificultad de las dimensiones; para dos o tres variables es sencillo visualizar las relaciones entre las mismas, pero para más variables ya no nos bastan nuestras tres dimensiones espaciales y tenemos que usar algunos trucos, tales como representaciones gráficas conjuntas de pares de variables. La manera más sencilla de representar gráficamente una tabla de datos formada por dos variables numéricas es aplicando la función plot a la matriz de datos o al data frame. Esta función produce el diagrama de dispersión (scatter plot) de los datos: el gráfico de los puntos del plano definidos por las filas de la tabla. A modo de ejemplo, si extrajéramos de la tabla iris una subtabla conteniendo sólo las longitudes y anchuras de los pétalos y quisiéramos visualizar la relación entre estas dimensiones, podríamos dibujar su diagrama de dispersión con el código del bloque siguiente. El resultado es la Figura 8.1, que muestra una clara tendencia positiva: cuanto más largos son los pétalos, más anchos tienden a ser. Esto se corresponde con la correlación de Pearson de 0.963 que hemos obtenido en la sección anterior. iris.pet=iris[ ,c(&quot;Petal.Length&quot;,&quot;Petal.Width&quot;)] plot(iris.pet, pch=20, xlab=&quot;Largo&quot;, ylab=&quot;Ancho&quot;) Figura 8.1: Diagrama de dispersión de las longitudes y anchuras de los pétalos de las flores de la tabla iris. Para tablas de datos de tres columnas numéricas, podemos usar con un fin similar la instrucción scatterplot3d del paquete homónimo, que dibuja un diagrama de dispersión tridimensional. Como plot, se puede aplicar a un data frame o a una matriz; por ejemplo, para representar gráficamente las tres primeras variables numéricas de iris, podríamos usar el código siguiente y obtendríamos la Figura 8.2: library(scatterplot3d) scatterplot3d(iris[ , 1:3], pch=20) Figura 8.2: Diagrama de dispersión tridimensional de las tres primeras columnas de la tabla iris. Podéis consultar la Ayuda de la instrucción para saber cómo modificar su apariencia: cómo ponerle un título, poner nombres adecuados a los ejes, usar colores, cambiar el estilo del gráfico, etc. Una representación gráfica muy popular de las tablas de datos de tres o más columnas numéricas son las matrices formadas por los diagramas de dispersión de todos sus pares de columnas. Si la tabla de datos es un data frame, esta matriz de diagramas de dispersión se obtiene simplemente aplicando la función plot al data frame; por ejemplo, la instrucción plot(iris[ , 1:4]) produce el gráfico de la Figura 8.3. En este gráfico, los cuadrados en la diagonal indican a qué variables corresponden cada fila y cada columna, de manera que podamos identificar fácilmente qué variables compara cada diagrama de dispersión; así, en el diagrama de la primera fila y segunda columna de esta figura, las abscisas corresponden a anchuras de sépalos y las ordenadas a longitudes de sépalos. Observad que la nube de puntos no muestra una tendencia clara y en todo caso ligeramente negativa, lo que se corresponde con la correlación de Pearson entre estas variables de -0.118 que hemos obtenido en la sección anterior. Figura 8.3: Matriz de diagramas de dispersión de la tabla iris. Podemos usar los parámetros usuales de plot para mejorar el gráfico resultante; por ejemplo, podemos usar colores para distinguir las flores según su especie. Así, la instrucción siguiente produce el gráfico de la Figura 8.4. plot(iris[ , 1:4], col=iris$Species, pch=20, cex=0.7) Figura 8.4: Matriz de diagramas de dispersión de la tabla iris, con las especies distinguidas por colores. Para obtener la matriz de diagramas de dispersión de una tabla de datos multidimensional también se puede usar la función pairs: así, pairs(iris[, 1:4]) produce exactamente el mismo gráfico que plot(iris[, 1:4]). La ventaja principal de pairs es que se puede aplicar a una matriz para obtener la matriz de diagramas de dispersión de sus columnas, mientras que plot no. El paquete car incorpora una función que permite dibujar matrices de diagramas de dispersión enriquecidos con información descriptiva extra de las variables de la tabla de datos y que además facilita el control del gráfico resultante, por lo que os recomendamos su uso frente a las funciones básicas plot y pairs. Se trata de la función spm (abreviatura de scatterplotMatrix); por ejemplo, el código siguiente produce el gráfico la Figura 8.5. library(car) spm(iris[ , 1:4], var.labels=c(&quot;Long. Sep.&quot;,&quot;Ancho Sep.&quot;,&quot;Long. Pet.&quot;,&quot;Ancho Pet.&quot;)) Figura 8.5: Una matriz de diagramas de dispersión de la tabla iris producida con la función spm. Observad para empezar que hemos cambiado los nombres que identifican las variables en los cuadrados de la diagonal, con el parámetro var.labels, y que en dichos cuadrados aparecen además unas curvas: se trata de la curva de densidad estimada de la variable correspondiente de la que hablábamos en la Lección ?? de la primera parte del curso. La información gráfica contenida en estos cuadrados de la diagonal se puede modificar con el parámetro diagonal: podemos pedir, por ejemplo, que dibuje un histograma de cada variable (con diagonal=list(method =\"histogram\")) o su boxplot (con diagonal=list(method=\"boxplot\")) o un normal-plot (con diagonal=list(method=\"qqplot\")). Así, el código siguiente produce el gráfico la Figura 8.6. spm(iris[ , 1:4], var.labels=c(&quot;Long. Sep.&quot;,&quot;Ancho Sep.&quot;,&quot;Long. Pet.&quot;,&quot;Ancho Pet.&quot;), diagonal=list(method=&quot;boxplot&quot;), pch=20,cex=0.75) Figura 8.6: Matriz de diagramas de dispersión de la tabla iris con boxplots en la diagonal. Observad también que los diagramas de dispersión de la matriz producida con spm contienen algunas líneas. La línea recta es la recta de regresión por mínimos cuadrados y, sin entrar en detalle sobre su significado exacto, las curvas discontinuas representan la tendencia de los datos. Podéis eliminar la recta de regresión con regLine=FALSE (no os lo recomendamos) y las curvas discontinuas con smooth=FALSE; si las queréis mantener, consultad la Ayuda de la función para saber cómo cambiar su estilo, color, etc. A veces querremos agrupar los datos de las variables numéricas de una tabla de datos. Los motivos serán los mismos que cuando se trata de una sola variable: por ejemplo, si los datos son aproximaciones de valores reales, o si son muy heterogéneos. Cuando tenemos dos variables emparejadas agrupadas, se pueden representar gráficamente las frecuencias de sus pares de clases mediante un histograma bidimensional, que divide el conjunto de todos los pares de valores en rectángulos definidos por los pares de intervalos e indica sobre cada rectángulo su frecuencia absoluta, por ejemplo mediante colores o intensidades de gris (dibujar barras verticales sobre las regiones es una mala idea, las de delante pueden ocultar las de detrás). Hay muchos paquetes de R que ofrecen funciones para dibujar histogramas bidimensionales; aquí explicaremos la función hist2d del paquete gplots. Su sintaxis básica es hist2d(x,y, nbins=..., col=...) donde: x e y son los vectores de primeras y segundas coordenadas de los puntos. Si son las dos columnas de un data frame de dos variables numéricas, lo podemos entrar en su lugar. nbins sirve para indicar los números de clases: podemos igualarlo a un único valor, y tomará ese número de clases sobre cada vector, o a un vector de dos entradas que indiquen el número de clases de cada vector. col sirve para especificar los colores a usar. Por defecto, los rectángulos vacíos aparecen de color negro, y el resto se colorean con tonalidades de rojo, de manera que los tonos más cálidos indican frecuencias mayores. Además, podemos usar los parámetros usuales de plot para poner un título, etiquetar los ejes, etc. A modo de ejemplo, vamos a dibujar el histograma bidimensional de las longitudes y anchuras de los pétalos de las flores iris, agrupando ambas dimensiones en los números de clases que da la regla de Freedman-Diaconis (y que calcula la función nclass.FD): library(gplots) hist2d(iris$Petal.Length, iris$Petal.Width, nbins=c(nclass.FD(iris$Petal.Length),nclass.FD(iris$Petal.Width))) Obtenemos (junto con una serie de información en la consola que hemos omitido) la Figura 8.7, que podéis comparar con el diagrama de dispersión de los mismos datos de la Figura 8.1. Figura 8.7: Histograma bidimensional de longitudes y anchuras de pétalos de flores iris. En los histogramas bidimensionales con muchas regiones de diferentes frecuencias, es conveniente usar de manera adecuada los colores para representarlas. Una posibilidad es usar el paquete RColorBrewer, que permite elegir esquemas de colores bien diseñados. Las dos funciones básicas son: brewer.pal(n,\"paleta predefinida\"), que carga en un vector de colores (una paleta) una secuencia de \\(n\\) colores de la paleta predefinida en el paquete. Los nombres y contenidos de todas las paletas predefinidas que se pueden usar en esta función se obtienen, en la ventana de gráficos, ejecutando la instrucción display.brewer.all(). Por ejemplo, la paleta de colores de la Figura 8.8 se define con el código siguiente: brewer.pal(11,&quot;Spectral&quot;) Figura 8.8: Paleta brewer.pal(11,“Spectral”) colorRampPalette(brewer.pal(...))(m), produce una nueva paleta de \\(m\\) colores a partir del resultado de brewer.pal, interpolando nuevos colores. Luego se puede usar la función rev para invertir el orden de los colores, lo que es conveniente en los histogramas bidimensionales si queremos que las frecuencias bajas correspondan a tonos azules y las frecuencias altas a tonos rojos. Así, la paleta de colores que se define con rev(colorRampPalette(brewer.pal(11,&quot;Spectral&quot;))(50)) es la de la Figura 8.9. Figura 8.9: Paleta rev(colorRampPalette(brewer.pal(11,“Spectral”))(50)) Vamos a usar esta última paleta en un histograma bidimensional de la tabla de alturas de padres e hijos recogidas por Karl Pearson en 1903 y que tenemos guardada en el url https://raw.githubusercontent.com/AprendeR-UIB/Material/master/pearson.txt; el resultado es la Figura 8.10. df_pearson=read.table(&quot;https://raw.githubusercontent.com/AprendeR-UIB/Material/master/pearson.txt&quot;,header=TRUE) hist2d(df_pearson, nbins=30, col=rev(colorRampPalette(brewer.pal(11,&quot;Spectral&quot;))(50))) Figura 8.10: Histograma bidimensional de las alturas de padres e hijos recogidas por Karl Pearson. Para terminar, veamos cómo producir un gráfico conjunto de un histograma bidimensional y los dos histogramas unidimensionales. Se trata de una modificación del gráfico similar explicado en http://www.everydayanalytics.ca/2014/09/5-ways-to-do-2d-histograms-in-r.html, el cual a su vez se inspira en un gráfico de la p. 62 de Computational Actuarial Science with R de Arthur Charpentier (Chapman and Hall/CRC, 2014). Considerad la función siguiente, cuyos parámetros son un data frame df de dos variables y un número n de clases, común para las dos variables: hist.doble=function(df,n){ par.anterior=par() h1=hist(df[,1], breaks=n, plot=F) h2=hist(df[,2], breaks=n, plot=F) m=max(h1$counts, h2$counts) par(mar=c(3,3,1,1)) layout(matrix(c(2,0,1,3), nrow=2, byrow=T), heights=c(1,3), widths=c(3,1)) hist2d(df, nbins=n, col=rev(colorRampPalette(brewer.pal(11,&quot;Spectral&quot;))(50))) par(mar=c(0,2,1,0)) barplot(h1$counts, axes=F, ylim=c(0, m), col=&quot;red&quot;) par(mar=c(2,0,0.5,1)) barplot(h2$counts, axes=F, xlim=c(0, m), col=&quot;red&quot;, horiz=T) par.anterior} Entonces, la instrucción hist.doble(df_pearson,25) produce la Figura 8.11. Figura 8.11: Histograma bidimensional con histogramas unidimensionales de las alturas de padres e hijos recogidas por Karl Pearson. Algunas explicaciones sobre el código, por si lo queréis modificar: Hemos “simulado” los histogramas mediante diagramas de barras de sus frecuencias absolutas, para poder dibujar horizontal el de la segunda variable. El parámetro axes=FALSE en los barplot indica que no dibuje sus ejes de coordenadas. La función par establece los parámetros generales básicos de los gráficos. Como con esta función los modificamos, guardamos los parámetros anteriores en par.anterior y al final los restauramos. El parámetro mar de la función par sirve para especificar, por este orden, los márgenes inferior, izquierdo, superior y derecho de la próxima figura, en números de líneas. La instrucción layout divide la figura a producir en sectores con la misma estructura que la matriz de su primer argumento. Dentro de esta matriz, cada entrada indica qué figura de las próximas se ha de situar en ese sector. Las alturas y amplitudes relativas de los sectores se especifican con los parámetros heights y widths, respectivamente. Así, la instrucción layout(matrix(c(2,0,1,3),nrow=2,byrow=T), heights=c(1,3),widths=c(3,1)) divide la figura en 4 sectores. Los sectores de la izquierda serán el triple de anchos que los de la derecha (widths=c(3,1)), y los sectores inferiores serán el triple de altos que los superiores (heights=c(1,3)). En estos sectores, R dibujará los próximos gráficos según el esquema definido por la matriz del argumento: \\[ \\left(\\begin{array}{cc} \\mbox{segundo} &amp; \\mbox{ninguno}\\\\ \\mbox{primero} &amp; \\mbox{tercero} \\end{array}\\right). \\] 8.8 Guía rápida sapply(data_frame,función) aplica la función a las columnas del data_frame. scale sirve para aplicar una transformación lineal a una matriz o a un data frame. Sus parámetros son: center: especifica el vector que restamos a sus columnas; por defecto, el vector de medias muestrales. scale: especifica el vector por el que dividimos sus columnas; por defecto, el vector de desviaciones típicas muestrales. cov, aplicada a dos vectores, calcula su covarianza muestral; aplicada a un data frame o a una matriz, calcula su matriz de covarianzas muestrales. Dispone del parámetro use, que: Para dos vectores: * Igualado a `&quot;complete.obs&quot;`, calcula las covarianzas teniendo en cuenta sólo sus observaciones completas (las posiciones en las que ninguno de los dos vectores tiene un NA). Para más de dos vectores: Igualado a \"pairwise.complete.obs\", calcula la covarianza de cada par de columnas teniendo en cuenta sólo sus observaciones completas, independientemente del resto de la tabla; es decir, como si en el cálculo de la covarianza de cada par de columnas usáramos use=\"complete.obs\", sin tener en cuenta que forman parte de una tabla de datos con más columnas. Igualado a \"complete.obs\", calcula las covarianzas de las columnas teniendo en cuenta sólo las filas completas de toda la matriz. cor, aplicada a dos vectores, calcula su correlación de Pearson; aplicada a un data frame o a una matriz, calcula su matriz de correlaciones de Pearson. Se puede usar el parámetro use de cov. Usando el parámetro method=\"spearman\" calcula la correlación (o la matriz de correlaciones, si se aplica a un data frame o a una matriz) de Spearman. cor.test realiza un contraste de correlación, con hipótesis nula que la correlación poblacional sea 0. Su sintaxis es la usual en funciones de contrastes. pwr.r.test, del paquete pwr, sirve para calcular la potencia de un contraste de correlación. Sus parámetros son: n, el tamaño de las muestras; r, su correlación de Pearson; sig.level, el nivel de significación; power, la potencia; y alternative, el tipo de contraste. Si se entran los valores de tres de los cuatro primeros parámetros, se obtiene el cuarto. upper.tri, aplicada a una matriz cuadrada M, produce la matriz triangular superior de valores lógicos del mismo orden que M. Con el parámetro diag=TRUE se impone que el triángulo de valores TRUE incluya la diagonal principal. lower.tri, aplicada a una matriz cuadrada M, produce la matriz triangular inferior de valores lógicos del mismo orden que M. Dispone del mismo parámetro diag=TRUE. order ordena el primer vector al que se aplica, desempatando empates mediante el orden de los vectores subsiguientes a los que se aplica; el parámetro decreasing=TRUE sirve para especificar que sea en orden decreciente. plot, aplicado a un data frame de dos variables numéricas, dibuja su diagrama de dispersión; aplicado a un data frame de más de dos variables numéricas, produce la matriz formada por los diagramas de dispersión de todos sus pares de variables. pairs es equivalente a plot en el sentido anterior, y se puede aplicar a matrices. spm, del paquete cars, produce matrices de dispersión más informativas y fáciles de modificar. scatterplot3d, del paquete scatterplot3d, dibuja diagramas de dispersión tridimensionales. hist2d, del paquete gplots, dibuja histogramas bidimensionales. Dispone de los parámetros específicos siguientes: nbins: indica los números de clases. col: especifica la paleta de colores que ha de usar para representar las frecuencias. brewer.pal(n,\"paleta predefinida\"), del paquete RColorBrewer, carga en una paleta de colores una secuencia de n colores de la paleta predefinida en dicho paquete. colorRampPalette(brewer.pal(...))(m), del paquete RColorBrewer, genera una nueva paleta de \\(m\\) colores a partir del resultado de brewer.pal, interpolando nuevos colores. display.brewer.all(), del paquete RColorBrewer, muestra los nombres y contenidos de todas las paletas predefinidas en dicho paquete. par sirve para establecer los parámetros generales básicos de los gráficos. layout divide en sectores la figura a producir, para que pueda incluir varios gráficos independientes simultáneamente. 8.9 Ejercicios Modelo de test (1) Considerad la matriz de datos \\[\\left(\\begin{array}{ccc} 10.6 &amp; 2.4 &amp; 7.5 \\\\ 7.4 &amp; 3.7 &amp; 10.9\\\\ 10.7 &amp; 2.6 &amp; 9.6 \\\\ 8.4 &amp; 4.9 &amp; 9.9\\\\16.7 &amp; 6.2 &amp; 13.2 \\\\ 11.3 &amp; 4.3 &amp; 7.7\\end{array} \\right).\\] Calculad la entrada (4,2) de su matriz de datos tipificada, redondeada a 3 cifras decimales y sin ceros innecesarios a la derecha. (2) Considerad la matriz de datos \\[\\left(\\begin{array}{ccc} 10.6 &amp; 2.4 &amp; 7.5 \\\\ 7.4 &amp; 3.7 &amp; 10.9\\\\ 10.7 &amp; 2.6 &amp; 9.6 \\\\ 8.4 &amp; 4.9 &amp; 9.9\\\\16.7 &amp; 6.2 &amp; 13.2 \\\\ 11.3 &amp; 4.3 &amp; 7.7\\end{array}\\right).\\] Calculad la covarianza muestral \\(\\widetilde{s}_{3,2}\\), redondeada a 3 cifras decimales y sin ceros innecesarios a la derecha. (3) Considerad la matriz de datos \\[\\left(\\begin{array}{ccc} 10.6 &amp; 2.4 &amp; 7.5 \\\\ 7.4 &amp; 3.7 &amp; 10.9\\\\ 10.7 &amp; 2.6 &amp; 9.6 \\\\ 8.4 &amp; 4.9 &amp; 9.9\\\\16.7 &amp; 6.2 &amp; 13.2 \\\\ 11.3 &amp; 4.3 &amp; 7.7\\end{array}\\right).\\] Calculad la correlación de Pearson \\(r_{3,2}\\), redondeada a 3 cifras decimales y sin ceros innecesarios a la derecha. (4) Usando la función cor.test, realizad el contraste bilateral de correlación entre el perímetro del tronco y la altura de los cerezos negros americanos usando la muestra del dataframe trees que viene con la instalación básica de R. Dad el p-valor redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha, e indicad si la conclusión, con un nivel de significación del 5%, es que hay correlación o no entre estas dos variables, escribiendo SI o NO, según corresponda. Separad el p-valor de la conclusión con un único espacio en blanco. (5) Calculad la correlación de Spearman de los vectores \\(x=(4,8,6,9,5,9 ,4,7,10, 8)\\) e \\(y=(0,6,2,1,4,4,3,7,11,5)\\). Dad el resultado redondeado a 3 cifras decimales sin ceros innecesarios a la derecha. (6) ¿Cuál de las cuatro matrices siguientes es la matriz de covarianzas de una tabla de datos de 2 columnas y 5 filas? Solo hay una. \\[ \\begin{array}{l} A=\\left(\\begin{array}{cc} 0.7 &amp; 3 \\cr 3 &amp; 1.2 \\end{array}\\right)\\\\ B=\\left(\\begin{array}{cc} 0.6 &amp; 0.8\\cr -0.8 &amp; 0.6 \\end{array}\\right)\\\\ C=\\left(\\begin{array}{cc} 0.6 &amp; 0.8\\cr 0.8 &amp; -0.6 \\end{array}\\right)\\\\ D=\\left(\\begin{array}{ccccc} 0.7 &amp; 0.2 &amp; 1.3 &amp; 0.5&amp; -0.1\\cr 0.2 &amp; 0.7 &amp; -0.3 &amp; -0.1 &amp;-0.1\\cr 1.3 &amp; -0.3 &amp; 3.1 &amp; 1.3 &amp; 0.4\\cr 0.5&amp; -0.1&amp; 1.3 &amp; 0.6 &amp; 0.2\\cr -0.1 &amp; -0.1 &amp; 0.4 &amp; 0.2&amp; 2.9\\end{array}\\right) \\end{array} \\] Problemas (1) El fichero https://raw.githubusercontent.com/AprendeR-UIB/Material/master/NotasMatesI14.csv recoge las notas medias (sobre 100) obtenidas en las diferentes actividades de evaluación de la asignatura Matemáticas I del grado de Biología, en el curso 2013/14, por parte de los estudiantes que fueron considerados “presentados” en la primera convocatoria. Estas actividades consistieron en: Dos controles (columnas Control1 y Control2). Talleres de resolución de problemas (columna Talleres). Ejercicios para resolver en casa (columna Casa). Cuestionarios en línea sobre los contenidos de la asignatura y sobre R (columnas TestsCont y TestsR, respectivamente). Cargad este fichero en un data frame. (a) Calculad el vector de medias y el vector de desviaciones típicas de esta tabla de datos. ¿Cuáles son las actividades de evaluación cuyas notas presentan mayor y menor variabilidad? (b) Calculad las matrices de covarianzas “verdaderas” y de correlaciones de Pearson de esta tabla de datos. (c) ¿Qué par de variables tiene mayor correlación? ¿Qué par de variables tiene menor correlación? (d) Comprobad en esta tabla de datos que su matriz de correlaciones es igual a la matriz de covarianzas de su tabla tipificada. (e) Dibujad una matriz de diagramas de dispersión de estas notas añadiendo en cada uno la recta de regresión lineal por mínimos cuadrados (pero sin otras curvas que indiquen la tendencia de los datos). ¿Se pueden ver en este diagrama los pares de actividades de evaluación con mayor y menor correlación que habéis encontrado en el apartado (c)? Respuestas al test (1) 0.673 Nosotros lo hemos calculado con X=matrix(c(10.6,2.4,7.5,7.4,3.7,10.9,10.7,2.6,9.6,8.4,4.9,9.9,16.7,6.2,13.2,11.3,4.3,7.7), nrow=6, byrow=TRUE) n=dim(X)[1] X.tip.Ex=scale(X)*sqrt(n/(n-1)) round(X.tip.Ex[4,2],3) ## [1] 0.673 (2) 2.114 Nosotros lo hemos calculado con X=matrix(c(10.6,2.4,7.5,7.4,3.7,10.9,10.7,2.6,9.6,8.4,4.9,9.9,16.7,6.2,13.2,11.3,4.3,7.7), nrow=6, byrow=TRUE) round(cov(X)[3,2],3) ## [1] 2.114 (3) 0.692 Nosotros lo hemos calculado con X=matrix(c(10.6,2.4,7.5,7.4,3.7,10.9,10.7,2.6,9.6,8.4,4.9,9.9,16.7,6.2,13.2,11.3,4.3,7.7), nrow=6, byrow=TRUE) round(cor(X)[3,2],3) ## [1] 0.692 (4) 0.003 SI Nosotros lo hemos calculado con round(cor.test(trees$Height,trees$Girth)$p.value,3) ## [1] 0.003 (5) 0.488 Nosotros lo hemos calculado con x=c(4,8,6,9,5,9 ,4,7,10, 8) y=c(0,6,2,1,4,4,3,7,11,5) round(cor(x,y,method=&quot;spearman&quot;),3) ## [1] 0.488 (6) A Soluciones sucintas de los problemas (1) notas=read.table(&quot;https://raw.githubusercontent.com/AprendeR-UIB/Material/master/NotasMatesI14.csv&quot;,sep=&quot;,&quot;,header=TRUE) str(notas) ## &#39;data.frame&#39;:\t107 obs. of 6 variables: ## $ Control1 : int 22 30 30 38 30 38 40 24 78 46 ... ## $ Control2 : int 0 0 0 0 3 4 5 6 7 7 ... ## $ Talleres : int 49 51 59 63 56 56 51 59 56 57 ... ## $ Casa : int 3 39 19 22 48 42 42 26 26 49 ... ## $ TestsR : int 2 24 48 45 59 68 38 34 37 66 ... ## $ TestsCont: int 14 31 33 38 59 58 26 18 39 47 ... (a) Medias=round(sapply(notas,mean),1) sd_ver=function(x){sqrt(var(x)*(length(x)-1)/length(x))} Desv.Tip=round(sapply(notas,sd_ver),1) Medias ## Control1 Control2 Talleres Casa TestsR TestsCont ## 58.3 26.7 58.6 57.4 64.8 59.9 Desv.Tip ## Control1 Control2 Talleres Casa TestsR TestsCont ## 19.9 16.9 16.0 20.5 20.7 20.0 Desv.Tip[which.max(Desv.Tip)] ## TestsR ## 20.7 Desv.Tip[which.min(Desv.Tip)] ## Talleres ## 16 (b) n=dim(notas)[1] Covariancias=((n-1)/n)*cov(notas) Correlaciones=cor(notas) Covariancias ## Control1 Control2 Talleres Casa TestsR TestsCont ## Control1 394.8244 209.7321 168.6785 198.7234 225.4755 224.4155 ## Control2 209.7321 285.7027 137.4756 225.4590 228.6795 247.2568 ## Talleres 168.6785 137.4756 254.7333 170.6966 174.0035 182.4254 ## Casa 198.7234 225.4590 170.6966 419.2941 284.8189 333.0139 ## TestsR 225.4755 228.6795 174.0035 284.8189 426.6009 356.6300 ## TestsCont 224.4155 247.2568 182.4254 333.0139 356.6300 401.8417 Correlaciones ## Control1 Control2 Talleres Casa TestsR TestsCont ## Control1 1.0000000 0.6244617 0.5318814 0.4884135 0.5493973 0.5634084 ## Control2 0.6244617 1.0000000 0.5095954 0.6514049 0.6550271 0.7297323 ## Talleres 0.5318814 0.5095954 1.0000000 0.5223030 0.5278423 0.5701837 ## Casa 0.4884135 0.6514049 0.5223030 1.0000000 0.6734395 0.8112888 ## TestsR 0.5493973 0.6550271 0.5278423 0.6734395 1.0000000 0.8613496 ## TestsCont 0.5634084 0.7297323 0.5701837 0.8112888 0.8613496 1.0000000 (c) ¿Qué par de variables tiene mayor correlación? ¿Qué par de variables tiene menor correlación? which.max(Correlaciones[Correlaciones!=1]) ## [1] 25 # La entrada 25 sin contar la diagonal es (TestsCont,TestsR) Correlaciones[Correlaciones!=1][which.max(Correlaciones[Correlaciones!=1])] ## [1] 0.8613496 which.min(Correlaciones[Correlaciones!=1]) ## [1] 3 # La entrada 3 sin contar la diagonal es (Casa,Control1) Correlaciones[Correlaciones!=1][which.min(Correlaciones[Correlaciones!=1])] ## [1] 0.4884135 (d) cov(scale(notas)) ## Control1 Control2 Talleres Casa TestsR TestsCont ## Control1 1.0000000 0.6244617 0.5318814 0.4884135 0.5493973 0.5634084 ## Control2 0.6244617 1.0000000 0.5095954 0.6514049 0.6550271 0.7297323 ## Talleres 0.5318814 0.5095954 1.0000000 0.5223030 0.5278423 0.5701837 ## Casa 0.4884135 0.6514049 0.5223030 1.0000000 0.6734395 0.8112888 ## TestsR 0.5493973 0.6550271 0.5278423 0.6734395 1.0000000 0.8613496 ## TestsCont 0.5634084 0.7297323 0.5701837 0.8112888 0.8613496 1.0000000 ((n-1)/n)*cov(scale(notas,scale=sapply(notas,sd_ver))) ## Control1 Control2 Talleres Casa TestsR TestsCont ## Control1 1.0000000 0.6244617 0.5318814 0.4884135 0.5493973 0.5634084 ## Control2 0.6244617 1.0000000 0.5095954 0.6514049 0.6550271 0.7297323 ## Talleres 0.5318814 0.5095954 1.0000000 0.5223030 0.5278423 0.5701837 ## Casa 0.4884135 0.6514049 0.5223030 1.0000000 0.6734395 0.8112888 ## TestsR 0.5493973 0.6550271 0.5278423 0.6734395 1.0000000 0.8613496 ## TestsCont 0.5634084 0.7297323 0.5701837 0.8112888 0.8613496 1.0000000 Correlaciones ## Control1 Control2 Talleres Casa TestsR TestsCont ## Control1 1.0000000 0.6244617 0.5318814 0.4884135 0.5493973 0.5634084 ## Control2 0.6244617 1.0000000 0.5095954 0.6514049 0.6550271 0.7297323 ## Talleres 0.5318814 0.5095954 1.0000000 0.5223030 0.5278423 0.5701837 ## Casa 0.4884135 0.6514049 0.5223030 1.0000000 0.6734395 0.8112888 ## TestsR 0.5493973 0.6550271 0.5278423 0.6734395 1.0000000 0.8613496 ## TestsCont 0.5634084 0.7297323 0.5701837 0.8112888 0.8613496 1.0000000 (e) Dibujad una matriz de diagramas de dispersión de estas notas añadiendo en cada uno la recta de regresión lineal por mínimos cuadrados (pero sin otras curvas que indiquen la tendencia de los datos). ¿Se pueden ver en este diagrama los pares de actividades de evaluación con mayor y menor correlación que habéis encontrado en el apartado (c)? library(car) spm(notas, smooth=FALSE,pch=20,cex=0.75) "],
["chap-ANOVA.html", "Lección 9 ANOVA básico 9.1 Los modelos del ANOVA en R 9.2 ANOVA de un factor 9.3 ANOVA de bloques completos aleatorios 9.4 ANOVA de dos vías 9.5 Condiciones del ANOVA 9.6 Comparaciones de pares de medias 9.7 Métodos no paramétricos 9.8 Guía rápida 9.9 Ejercicios", " Lección 9 ANOVA básico En esta lección explicamos cómo efectuar con R los ANOVA básicos que se estudian en cursos introductorios de estadística inferencial: de uno y dos factores y de bloques completos aleatorios. El tema central de la lección son los aspectos técnicos del ANOVA con R y los tests posteriores de comparación de pares de medias. Incluimos además una sección con algunas instrucciones que permiten contrastar las condiciones necesarias sobre los datos para que un contraste ANOVA tenga sentido y una sección con algunos contrastes no paramétricos alternativos al ANOVA que se puedan usar justamente cuando no tiene sentido realizar un ANOVA. 9.1 Los modelos del ANOVA en R Los modelos a los que se aplica un ANOVA u otras muchas funciones, como por ejemplo la función lm para calcular la recta de regresión lineal, se especifican en R mediante fórmulas. El operador básico para construir una fórmula es la tilde, ~. Las fórmulas suelen tener la forma Y~modelo, donde la Y es un vector y el modelo es una combinación de vectores o factores que representa el modelo con el que queremos explicar el vector Y (en palabras técnicas, al que queremos ajustar los datos del vector Y). Por ejemplo, para calcular la recta de regresión por mínimos cuadrados de un vector Y respecto de un vector X, usamos lm(Y~X). Esto significa que aplicamos la función lm a la fórmula Y~X que indica que queremos explicar Y en función de X. En los ANOVA que consideramos en esta lección se usan cuatro tipos de fórmulas. Concretamente, si X es una variable numérica y F1 y F2 son dos factores: La fórmula X~F1 se usa para indicar el ANOVA de un factor, F1, de la variable X. La fórmula X~F1+F2 se usa para indicar el ANOVA de dos factores, F1 y F2, de la variable X, sin tener en cuenta la interacción entre los factores; es decir, suponiendo que sus efectos se suman, sin que haya interacción entre los mismos. Es el tipo de fórmula que se usa en los ANOVA de bloques. La fórmula X~F1*F2 se usa para indicar el ANOVA de dos factores, F1 y F2, de la variable X, teniendo en cuenta además la posible interacción entre estos factores. La fórmula X~F1:F2 se usa para indicar el ANOVA de un factor que tiene como niveles los pares ordenados de niveles de F1 y F2. La función básica de R para realizar un ANOVA es aov. Su sintaxis genérica es aov(fórmula, data=...) con los argumentos siguientes: fórmula: Una fórmula que especifique un modelo de ANOVA. data: Opcional, sirve para especificar, si es necesario, el data frame al que pertenecen las variables utilizadas en la fórmula. Así, por ejemplo, si tenemos un data frame llamado DF, con una variable numérica X y un factor Fact, para realizar el ANOVA de la variable X respecto del factor Fact con la función aov podríamos entrar aov(X~Fact, data=DF) o aov(DF$X~DF$Fact) Otra posibilidad, que por ahora no usaremos pero sí más adelante, es aplicar la función anova (no la confundáis con aov) al resultado de lm. La sintaxis sería entonces anova(lm(fórmula, data=...)) 9.2 ANOVA de un factor Para ilustrar el ANOVA de un factor con R utilizaremos un experimento en el que se quiso determinar si cuatro dietas concretas tenían alguna influencia en el tiempo de coagulación de la sangre en mamíferos: véase Statistics for Experimenters (2a edición), de G. P. Box, W. G. Hunter y J. S. Hunte (Wiley, 2005), p. 133. Para ello se escogieron 24 animales, se repartieron de manera aleatoria en 4 grupos de 6 ejemplares cada uno, y a cada grupo se le asignó de manera aleatoria una de las 4 dietas objeto de estudio, que indicaremos con A, B, C y D. Al cabo de un cierto tiempo se midió el tiempo de coagulación de la sangre en estos animales. Los resultados (redondeados a enteros) se muestran en la Tabla 9.1. Observad que estamos ante un diseño experimental de un solo factor: la dieta. Tabla 9.1: Tiempos de coagulación bajo diferentes dietas. A B C D 62 63 68 56 60 67 66 62 63 71 71 60 59 64 67 61 63 65 68 63 59 66 68 64 Para contrastar si los tiempos medios de coagulación son los mismos para las cuatro dietas o no, vamos a realizar un ANOVA de estos datos. Para ello, lo primero que tenemos que hacer es recoger los datos en un data frame, formado por una variable numérica con los valores de la tabla y un factor cuyos niveles sean las diferentes dietas, de manera que a cada valor en la variable numérica le corresponda la dieta con la que se obtuvo. Nosotros entraremos los datos de la tabla anterior por filas, y entonces el factor tendrá que ser A, B, C, D, A, B, C, D, A... Otra opción sería entrar los datos por columnas, y entonces entraríamos como factor A, A, A, A, A, A, B, B, B,... Entramos pues los datos de la tabla, por filas: coag=c(62,63,68,56,60,67,66,62,63,71,71,60,59,64,67,61,63,65,68,63,59,66,68,64) Definimos de manera adecuada el factor de las dietas, como 6 copias de la fila A,B,C,D: diet=rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), times=6) Finalmente, definimos el data frame y comprobamos que es correcto: coagulacion=data.frame(coag,diet) str(coagulacion) ## &#39;data.frame&#39;:\t24 obs. of 2 variables: ## $ coag: num 62 63 68 56 60 67 66 62 63 71 ... ## $ diet: Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;: 1 2 3 4 1 2 3 4 1 2 ... head(coagulacion) ## coag diet ## 1 62 A ## 2 63 B ## 3 68 C ## 4 56 D ## 5 60 A ## 6 67 B Para analizar la igualdad de los tiempos medios de coagulación bajo las cuatro dietas, realizaremos un ANOVA de la variable coag separándola según (ajustándola a) el factor diet. Antes de empezar, es conveniente visualizar los datos para hacernos una idea de su distribución; por ejemplo, por medio de un diagrama de cajas con una caja por cada nivel: boxplot(coag~diet, data=coagulacion) Observad que también hemos empleado una fórmula para indicar que queremos los diagramas de cajas de la variable coag separada por la variable diet del data frame coagulacion. Obtenemos la Figura 9.1, donde podemos ver que las medias muestrales para las dietas A y C son muy diferentes, y que en cambio seguramente no podríamos rechazar que las medias poblacionales de las dietas A y D sean iguales. Por lo tanto, el resultado que esperamos del ANOVA es que nos permita rechazar la hipótesis nula de que los cuatro tiempos medios de coagulación son iguales. Ahora bien, hasta que no realicemos el ANOVA no sabremos si las diferencias que observamos en este diagrama son estadísticamente significativas o no. Figura 9.1: Diagrama de cajas de los tiempos de coagulación según las diferentes dietas. Como hemos comentado, para realizar el ANOVA deseado, entramos: aov(coag~diet, data=coagulacion) ## Call: ## aov(formula = coag ~ diet, data = coagulacion) ## ## Terms: ## diet Residuals ## Sum of Squares 228 112 ## Deg. of Freedom 3 20 ## ## Residual standard error: 2.366432 ## Estimated effects may be unbalanced El resultado no es la tabla del ANOVA. Para obtenerla, hay que aplicar summary al resultado de aov: summary(aov(coag~diet, data=coagulacion)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## diet 3 228 76.0 13.57 4.66e-05 *** ## Residuals 20 112 5.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 El resultado de esta función summary es la tabla ANOVA usual: En la primera columna, dos etiquetas: el nombre del factor, en este caso diet, y Residuals, que representa los errores o residuos del ANOVA. La segunda columna, etiquetada Df, nos da los grados de libertad correspondientes al factor (su número de niveles menos 1) y a los residuos (el número de individuos en la tabla, menos el número de niveles del factor). La tercera columna, Sum Sq, nos muestra las sumas de los cuadrados del factor, \\(SS_{Tr}\\), y de los residuos, \\(SS_E\\). La cuarta columna, Mean Sq, contiene las medias de los cuadrados del factor, \\(MS_{Tr}\\), y de los residuos, \\(MS_E\\). La quinta columna, F value, nos da el valor del estadístico de contraste. En la sexta columna, Pr(&gt;F), aparece el p-valor del contraste. La séptima columna, sin etiqueta, indica el nivel de significación del p-valor según el código usual, explicado en la última línea del resultado. A mayor número de asteriscos, más significativo es el p-valor y por lo tanto es más fuerte la evidencia de que las medias comparadas no son todas iguales. En nuestro caso, hemos obtenido un p-valor del orden de \\(4.7\\times 10^{-5}\\). Esto nos permite rechazar la hipótesis nula y concluir que no todos los tiempos medios de coagulación para las diferentes dietas consideradas son iguales, como intuíamos. Recordad que esto no significa que hayamos obtenido evidencia de que todos los tiempos medios de coagulación son diferentes, solo de que hay al menos un par de dietas que dan tiempos medios diferentes. Si ahora queremos determinar de qué pares de dietas se trata, tendremos que realizar algún test de comparaciones de pares de medias: véase la Sección 9.6. Para ahorrar espacio vertical, en lo que queda de lección vamos a eliminar los símbolos que marcan los niveles de significación de los p-valores. Para ello entramos la siguiente instrucción: options(show.signif.stars=FALSE) A partir de ahora, y mientras no cerremos la sesión, estos símbolos no aparecerán más en las tablas ANOVA. summary(aov(coag~diet, data=coagulacion)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## diet 3 228 76.0 13.57 4.66e-05 ## Residuals 20 112 5.6 Si en algún momento de la sesión queréis volver a ver los códigos de significación, basta que entréis options(show.signif.stars=TRUE) Como hemos comentado, una manera alternativa de realizar un ANOVA es mediante anova(lm(...)). Con esta construcción obtenemos directamente la tabla, sin necesidad de aplicarle summary. anova(lm(coag~diet, data=coagulacion)) ## Analysis of Variance Table ## ## Response: coag ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## diet 3 228 76.0 13.571 4.658e-05 ## Residuals 20 112 5.6 Para extraer los datos de la tabla ANOVA obtenida con la función summary(aov(...)), y así poder operar directamente con ellos, hay que añadirle los sufijos adecuados. Consultemos su estructura. tabla=summary(aov(coag~diet, data=coagulacion)) str(tabla) ## List of 1 ## $ :Classes &#39;anova&#39; and &#39;data.frame&#39;:\t2 obs. of 5 variables: ## ..$ Df : num [1:2] 3 20 ## ..$ Sum Sq : num [1:2] 228 112 ## ..$ Mean Sq: num [1:2] 76 5.6 ## ..$ F value: num [1:2] 13.6 NA ## ..$ Pr(&gt;F) : num [1:2] 4.66e-05 NA ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;summary.aov&quot; &quot;listof&quot; Vemos que la tabla ANOVA obtenida con summary(aov(...)) es una list formada por un solo objeto (List of 1), que a su vez es un data frame cuyas variables son las columnas numéricas de la tabla. Por lo tanto, para obtener una columna de estas, primero hemos de añadir el sufijo [[1]], que extrae el data frame de la list, y a continuación el sufijo $columna correspondiente a la columna de la tabla ANOVA que nos interesa. Por ejemplo, la columna de las sumas de los cuadrados es: tabla[[1]]$&quot;Sum Sq&quot; ## [1] 228 112 De manera similar, como el p-valor es el primer elemento de la columna Pr(&gt;F), lo obtenemos con: tabla[[1]]$&quot;Pr(&gt;F)&quot;[1] ## [1] 4.658471e-05 Fijaos en que, como los nombres de estas columnas contienen espacios en blanco u otros símbolos no aceptados en los nombres de objetos de R (recordad la Lección 4 de la primera parte), hay que especificarlos entre comillas. El resultado de anova(lm(...)) ya es directamente un data frame, por lo que para extraer los valores de la tabla ANOVA que produce podemos usar la sintaxis usual de los data frames. tabla2=anova(lm(coag~diet, data=coagulacion)) str(tabla2) ## Classes &#39;anova&#39; and &#39;data.frame&#39;:\t2 obs. of 5 variables: ## $ Df : int 3 20 ## $ Sum Sq : num 228 112 ## $ Mean Sq: num 76 5.6 ## $ F value: num 13.6 NA ## $ Pr(&gt;F) : num 4.66e-05 NA ## - attr(*, &quot;heading&quot;)= chr &quot;Analysis of Variance Table\\n&quot; &quot;Response: coag&quot; tabla2$&quot;Sum Sq&quot; ## [1] 228 112 tabla2$&quot;Pr(&gt;F)&quot;[1] ## [1] 4.658471e-05 Veamos otro ejemplo de ANOVA de un factor. Ejemplo 9.1 En un experimento, se estudió el efecto de seis dietas sobre el crecimiento de crías de conejo doméstico: véase Experimental Design and Analysis, de M. Lentner y T. Bishop (Valley Book Co. 1986), p. 428. Los datos obtenidos están recogidos en la tabla de datos rabbit del paquete faraway. library(faraway) str(rabbit) ## &#39;data.frame&#39;:\t30 obs. of 3 variables: ## $ treat: Factor w/ 6 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,..: 6 2 3 3 1 2 3 6 4 1 ... ## $ gain : num 42.2 32.6 35.2 40.9 40.1 38.1 34.6 34.3 37.5 44.9 ... ## $ block: Factor w/ 10 levels &quot;b1&quot;,&quot;b10&quot;,&quot;b2&quot;,..: 1 1 1 3 3 3 4 4 4 5 ... Consultando la Ayuda de rabbit nos enteramos de que el factor treat indica la dieta, con niveles a,b,c,d,e,f, y de que la variable gain indica el aumento de peso; la variable block, que indica la camada, es irrelevante en este análisis concreto. Si dibujamos el diagrama de cajas de los crecimientos para cada dieta, obtenemos la Figura 9.2, donde no observamos grandes diferencias en los crecimientos medios. boxplot(gain~treat, data=rabbit) Figura 9.2: Diagrama de cajas de los crecimientos de crías de conejo según las diferentes dietas. Para determinar si hay diferencia en los aumentos medios de peso bajo las seis dietas, realizaremos un ANOVA de la variable gain ajustándola al factor treat. summary(aov(gain~treat, data=rabbit)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 5 293.4 58.68 1.886 0.134 ## Residuals 24 746.5 31.10 El p-valor es 0.134, lo que indica que, efectivamente, no hay evidencia de que las dietas den lugar a crecimientos medios diferentes. Si hubiéramos querido obtener solo el p-valor, podríamos haber entrado: summary(aov(gain~treat, data=rabbit))[[1]]$&quot;Pr(&gt;F)&quot;[1] ## [1] 0.1342124 Es muy importante tener presente que, al usar las instrucciones aov o anova(lm(...)) para realizar un ANOVA, se tiene que emplear un factor (o varios, en las próximas secciones) para separar la variable numérica en subpoblaciones. Veamos un ejemplo de lo que pasa si nos descuidamos en este punto. Ejemplo 9.2 En un experimento se estudió el efecto de la vitamina C en el crecimiento de los dientes: véase The Statistics of Bioassay de C. I. Bliss (Academic Press, 1952), p. 499-501. Se tomaron 60 cobayas y se trató cada uno de ellos con una combinación diferente de dosis de vitamina C (0.5, 1 o 2 mg) y método de suministro de la misma (mediante zumo de naranja o como ácido ascórbico) durante 6 semanas, y se cuantificó el crecimiento de sus dientes durante dicho período (más en concreto, se midió la longitud media de sus odontoblastos al final del mismo). El resultado es una tabla de 60 datos, que aparecen recogidos en el fichero ToothGrowth del paquete UsingR. library(UsingR) str(ToothGrowth) ## &#39;data.frame&#39;:\t60 obs. of 3 variables: ## $ len : num 4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ... ## $ supp: Factor w/ 2 levels &quot;OJ&quot;,&quot;VC&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ dose: num 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ... La Ayuda de ToothGrowth nos dice que la variable len contiene la longitud media final de los odontoblastos del animal, la variable supp el método de suministro (OJ indica zumo de naranja, orange juice, y VC indica ácido ascórbico puro, vitamine C), y la variable dose la dosis. Nos vamos a olvidar por el momento de la variable supp, y vamos a contrastar si la dosis de vitamina C tiene influencia en el crecimiento de los dientes. Para ello realizamos un ANOVA de un factor. summary(aov(len~dose, data=ToothGrowth)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dose 1 2224 2224.3 105.1 1.23e-14 ## Residuals 58 1228 21.2 ¿Veis algo raro? Hemos comentado que se usaron tres dosis diferentes, por lo que el número de grados de libertad en la fila del factor, dose, tendría que ser 2, y no 1. ¿Qué ha pasado? Muy sencillo: dose es una variable numérica, y para usar aov el factor ha de ser eso, un factor. Así que lo primero que tenemos que hacer es convertir esta variable en un factor. Los haremos sobre un duplicado de la tabla ToothGrowth original, a la que llamaremos ToothGrowth2. ToothGrowth2=ToothGrowth ToothGrowth2$dose=as.factor(ToothGrowth2$dose) str(ToothGrowth2) ## &#39;data.frame&#39;:\t60 obs. of 3 variables: ## $ len : num 4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ... ## $ supp: Factor w/ 2 levels &quot;OJ&quot;,&quot;VC&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ dose: Factor w/ 3 levels &quot;0.5&quot;,&quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... summary(aov(len~dose, data=ToothGrowth2)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dose 2 2426 1213 67.42 9.53e-16 ## Residuals 57 1026 18 Ahora está bien. El p-valor prácticamente 0 nos permite rechazar la hipótesis nula y concluir que hay dosis de vitamina C que dan lugar a diferentes crecimientos medios de los odontoblastos. 9.3 ANOVA de bloques completos aleatorios Para ilustrar este tipo de ANOVA, analizaremos un experimento sobre producción de penicilina: vVéase Practical Regression and Anova using R, de J. Faraway, p. 186 (este texto se puede descargar de la página web https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf). En dicho experimento, se evaluaron cuatro procesos diferentes para determinar si había diferencias en su efectividad. Los cuatro procesos usaban una técnica de cultivo sumergido y empleaban agua de macerado de maíz como fuente de nitrógeno orgánico. Como la composición de este líquido puede afectar la producción final de penicilina, para evaluar los cuatro procesos se prepararon 5 mezclas diferentes de agua de macerado de maíz, de cada mezcla se tomaron 4 muestras y se asignaron de manera aleatoria a los cuatro procesos de producción. Los resultados obtenidos son los de la Tabla 9.2, y aparecen recogidos en la tabla de datos penicillin del paquete faraway. Tabla 9.2: Producción de penicilina bajo diferentes procesos de producción y mezclas. Mezcla A B C D 1 89 88 97 94 2 84 77 92 79 3 81 87 87 85 4 87 92 89 84 5 79 81 80 88 Observad que estamos ante un diseño experimental de bloques completos aleatorios. Los bloques son las mezclas de agua de macerado de maíz y los tratamientos (los procesos de producción) se han asignado de manera aleatoria a las unidades experimentales (las muestras) de cada bloque, de tal manera que cada bloque contiene exactamente una unidad experimental para cada tratamiento. Este es un ejemplo paradigmático de uso de bloques: para evitar la influencia de la variable “extraña” dada por la composición del agua de macerado de maíz, que puede influir en la producción, se escogen al azar unas mezclas y se prueban todos los procesos de manera independiente sobre cada mezcla. Demos un vistazo a esta tabla de datos. str(penicillin) ## &#39;data.frame&#39;:\t20 obs. of 3 variables: ## $ treat: Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;: 1 2 3 4 1 2 3 4 1 2 ... ## $ blend: Factor w/ 5 levels &quot;Blend1&quot;,&quot;Blend2&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... ## $ yield: num 89 88 97 94 84 77 92 79 81 87 ... head(penicillin) ## treat blend yield ## 1 A Blend1 89 ## 2 B Blend1 88 ## 3 C Blend1 97 ## 4 D Blend1 94 ## 5 A Blend2 84 ## 6 B Blend2 77 Según la Ayuda de penicillin, la variable treat es el proceso de producción, con valores A, B, C y D; la variable blend es la mezcla usada (los bloques), con valores Blend1 a Blend5; y la variable numérica yield es un valor que cuantifica la producción de penicilina. Veamos cómo son los diagramas de cajas de la producción de penicilina separada por procesos de producción y por mezclas: boxplot(yield~treat, data=penicillin) Figura 9.3: Diagrama de cajas de la producción de penicilina bajo los diferentes procesos de producción. boxplot(yield~blend, data=penicillin) Figura 9.4: Diagrama de cajas de la producción de penicilina según las diferentes mezclas. Vemos que no hay mucha diferencia entre las producciones para los diferentes procesos (sin tener en cuenta los bloques), y que sí que hay algunas diferencias en las producciones según la composición del agua de macerado de maíz; ya hemos comentado que es bien sabido que su composición influye en la producción. Si realizamos un ANOVA de un factor para cada uno de estos dos factores por separado, obtenemos resultados consistentes con esta observación visual: summary(aov(yield~treat, data=penicillin)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 3 70 23.33 0.762 0.532 ## Residuals 16 490 30.62 summary(aov(yield~blend, data=penicillin)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## blend 4 264 66.00 3.345 0.038 ## Residuals 15 296 19.73 El p-valor del ANOVA separando por procesos de producción es 0.532, lo que indica que no podemos rechazar la hipótesis nula de que los procesos de producción tengan igual productividad media (sin tener en cuenta las mezclas), y el p-valor del ANOVA separando por mezclas es 0.038, lo que, con un nivel de significación del 5%, nos permite rechazar que todas las mezclas produzcan la misma cantidad media de penicilina. Pero precisamente, el hecho de que la mezcla influya en la producción es lo que hace que el primer ANOVA no sea fiable: a lo mejor el efecto de las mezclas enmascara las diferencias en las productividades de los procesos bajo estudio. Para determinarlo, vamos a realizar un ANOVA de bloques, es decir, de dos factores y efectos acumulados. summary(aov(yield~treat+blend, data=penicillin)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 3 70 23.33 1.239 0.3387 ## blend 4 264 66.00 3.504 0.0407 ## Residuals 12 226 18.83 El p-valor que nos interesa en esta tabla es el de la fila treat: es 0.34, por lo que si tenemos en cuenta los bloques tampoco detectamos evidencia de que haya diferencias en la productividad media de los procesos estudiados. El segundo p-valor de la tabla, 0.0407, es el del ANOVA de bloques que resulta de intercambiar los bloques y el tratamiento, tomando las mezclas como el factor a analizar y los procesos de producción como los bloques: summary(aov(yield~blend+treat, data=penicillin)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## blend 4 264 66.00 3.504 0.0407 ## treat 3 70 23.33 1.239 0.3387 ## Residuals 12 226 18.83 Podemos extraer los resultados de una tabla de un ANOVA de bloques generada por summary(aov(...)) añadiendo los mismos sufijos que en el caso de un factor. Así, por ejemplo, los p-valores son: tabla=summary(aov(yield~treat+blend, data=penicillin)) tabla[[1]]$&quot;Pr(&gt;F)&quot; ## [1] 0.33865812 0.04074617 NA y si solo nos interesa el del factor treat: tabla[[1]]$&quot;Pr(&gt;F)&quot;[1] ## [1] 0.3386581 Veamos otro ejemplo de ANOVA de bloques completos aleatorios. Ejemplo 9.3 Para estudiar si las diferentes actividades de evaluación llevadas a cabo en la asignatura de Matemáticas I tienen una dificultad similar (o mejor dicho, para mirar de confirmar nuestras sospechas de que no es así), escogimos una muestra aleatoria de 15 estudiantes de Biología o Bioquímica que se hubieran presentado al control 2 de dicha asignatura en el curso 2012-2013, y anotamos las notas obtenidas por estos estudiantes en los apartados de Tests, Talleres, Ejercicios de Casa, Control 1 y Control 2. Si obtenemos evidencia de que no todas las medias de las notas de las diferentes actividades fueron iguales, podremos concluir que no todas las actividades tienen la misma dificultad. Los datos obtenidos son los de la tabla 9.3. Tabla 9.3: Notas obtenidas por 15 estudiantes en diferentes actividades de evaluación. Estudiante Tests Casa Talleres Control.1 Control.2 1 48 42 85 31 20 2 94 86 100 52 48 3 98 94 93 93 90 4 60 58 71 66 46 5 52 56 79 64 24 6 78 73 84 80 95 7 84 84 94 83 70 8 83 76 99 51 55 9 75 55 70 85 48 10 24 49 57 19 55 11 47 47 64 44 35 12 53 42 78 50 8 13 82 93 92 66 30 14 66 62 74 24 56 15 49 40 74 35 21 Como podemos observar, se trata de un experimento de bloques completos aleatorios: hemos escogido de manera aleatoria unos bloques (los estudiantes) y para cada estudiante hemos apuntado el valor de cada uno de los niveles del factor a estudiar (las notas en las diferentes actividades). Este diseño es el adecuado para este problema, puesto que hay una gran variabilidad en las notas obtenidas por estudiantes diferentes, desde matrículas a suspensos. Al tomar bloques, es decir, al considerar todas las notas de un conjunto aleatorio fijo de estudiantes, eliminamos el efecto de esta variabilidad. Vamos a construir un data frame con estos datos. Este data frame tendrá tres variables: notas, con las notas obtenidas por los estudiantes; acts, con los tipos de actividades de evaluación realizados; y bloques, con el indicador de cada estudiante. Entraremos las notas siguiendo las filas de la tabla anterior, y por lo tanto tenemos que construir estos dos factores entrando sus niveles en el orden adecuado: el factor acts ha de estar formado por 15 copias de la fila de actividades, y el factor bloques ha de estar formado por 5 copias de 1, 5 copias de 2, y así hasta 5 copias de 15. Y recordad que bloques ha de ser un factor, puesto que queremos usarlo en un ANOVA. notas=c(48,42,85,31,20,94,86,100,52,48,98,94,93,93,90,60,58, 71,66,46,52,56,79,64,24,78,73,84,80,95,84,84,94,83,70,83,76, 99,51,55,75,55,70,85,48,24,49,57,19,55,47,47,64,44,35,53,42, 78,50,8,82,93,92,66,30,66,62,74,24,56,49,40,74,35,21) acts=rep(c(&quot;Tests&quot;,&quot;Casa&quot;,&quot;Talleres&quot;,&quot;Control 1&quot;,&quot;Control 2&quot;), times=15) bloques=as.factor(rep(1:15,each=5)) notas.bloques=data.frame(notas,acts,bloques) str(notas.bloques) ## &#39;data.frame&#39;:\t75 obs. of 3 variables: ## $ notas : num 48 42 85 31 20 94 86 100 52 48 ... ## $ acts : Factor w/ 5 levels &quot;Casa&quot;,&quot;Control 1&quot;,..: 5 1 4 2 3 5 1 4 2 3 ... ## $ bloques: Factor w/ 15 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 2 2 2 2 2 ... head(notas.bloques) ## notas acts bloques ## 1 48 Tests 1 ## 2 42 Casa 1 ## 3 85 Talleres 1 ## 4 31 Control 1 1 ## 5 20 Control 2 1 ## 6 94 Tests 2 Vamos a dibujar los diagramas de cajas de las notas de las diferentes actividades y de los diferentes estudiantes: boxplot(notas.bloques$notas~notas.bloques$acts) Figura 9.5: Diagrama de cajas de las notas en las diferentes actividades. boxplot(notas.bloques$notas~notas.bloques$bloques) Figura 9.6: Diagrama de cajas de las notas de los estudiantes. Podemos observar diferencias en las notas de algunas actividades: por ejemplo, las notas del control 2 son muy inferiores a las de los talleres. Asimismo, como nos temíamos, vemos una gran variabilidad en las notas de los estudiantes. Realicemos ahora el ANOVA. summary(aov(notas~acts+bloques, data=notas.bloques)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## acts 4 9646 2411.5 13.133 1.31e-07 ## bloques 14 19430 1387.9 7.558 1.47e-08 ## Residuals 56 10283 183.6 El p-valor que nos interesa es el de la primera fila, etiquetada con el factor cuyos niveles queremos comparar, en este caso acts. Este p-valor es muy pequeño, del orden de \\(10^{-7}\\), lo que es evidencia de que, como nos temíamos, no todas las notas medias de las actividades de evaluación son iguales. 9.4 ANOVA de dos vías Para ilustrar el ANOVA de dos vías, es decir, de dos factores que pueden interacccionar, analizaremos en primer lugar los resultados de un experimento sobre el efecto de venenos y antídotos: véase “An analysis of transformations”, de G. Box y D. Cox, J. Roy. Stat. Soc. Series B 26 (1964), pp. 211-252; véase también Practical Regression and Anova using R, de J. Faraway, p. 182. En este experimento se usaron tres venenos y cuatro antídotos, y cada combinación de veneno y antídoto se administró a cuatro ratas elegidas de manera aleatoria e independiente. A continuación, se anotó el tiempo de supervivencia de cada animal, en unidades de 10 horas. Se trata, pues, de un diseño experimental de dos factores, en el que cada individuo ha sido asignado al azar a cada nivel de los dos factores. Por otro lado, no podemos descartar a priori que haya interacción entre venenos y antídotos, puesto que un antídoto puede ser más efectivo para un veneno que para otro. El objetivo del experimento era contrastar la igualdad de los tiempos medios de supervivencia según el veneno, según el antídoto, y según la combinación veneno-antídoto. Los datos obtenidos en este experimento se han recogido en la tabla rats del paquete faraway. Pero ahora tenemos un problema con este data frame. Si cargamos en una misma sesión varios paquetes que contengan objetos con el mismo nombre, R entiende en cada momento que ese nombre refiere al objeto del paquete que se haya cargado más recientemente. Como en esta lección hemos cargado el paquete UsingR después del paquete faraway, en estos momentos rats refiere a una tabla de datos del paquete survival que se ha cargado con UsingR y que no tiene nada que ver con los datos de este experimento. Una solución posible sería en este punto volver a cargar el paquete faraway. Otra opción más rápida es definir rats como el objeto rats de faraway por medio de la instrucción rats=faraway::rats. La construcción paquete::objeto invoca el objeto (una función, una tabla de datos,…) del paquete y sirve para eliminar ambigüedades como la que nos encontramos aquí. Además, tiene la ventaja de que no es necesario cargar el paquete, basta que esté instalado Vamos a cargar y explorar estos datos. rats=faraway::rats str(rats) ## &#39;data.frame&#39;:\t48 obs. of 3 variables: ## $ time : num 0.31 0.82 0.43 0.45 0.45 1.1 0.45 0.71 0.46 0.88 ... ## $ poison: Factor w/ 3 levels &quot;I&quot;,&quot;II&quot;,&quot;III&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ treat : Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;: 1 2 3 4 1 2 3 4 1 2 ... head(rats) ## time poison treat ## 1 0.31 I A ## 2 0.82 I B ## 3 0.43 I C ## 4 0.45 I D ## 5 0.45 I A ## 6 1.10 I B En la Ayuda de rats nos enteramos de que el factor treat contiene el antídoto, con niveles A, B, C y D; el factor poison contiene el veneno, con niveles I, II y III; y la variable numérica time contiene el tiempo de supervivencia de las ratas. Veamos cómo son los diagramas de cajas de estos tiempos, separados por venenos y por antídotos. boxplot(time~poison, data=rats) Figura 9.7: Diagrama de cajas de los tiempos de supervivencia según los venenos. boxplot(time~treat, data=rats) Figura 9.8: Diagrama de cajas de los tiempos de supervivencia según los antídotos Parece que hay diferencias en los tiempos medios de supervivencia tanto para los diferentes venenos como para los diferentes antídotos. Podemos también dibujar un diagrama de cajas de los tiempos de supervivencia separándolos por combinaciones de veneno y antídoto. La instrucción para hacerlo es la siguiente (observad la fórmula del argumento): boxplot(time~poison:treat, data=rats) Figura 9.9: Diagrama de cajas de los tiempos de supervivencia según las combinaciones de veneno y antídoto. El código para efectuar el ANOVA de dos vías del tiempo de supervivencia de las ratas bajo los efectos combinados de los venenos y los antídotos es el siguiente: summary(aov(time~poison*treat, data=rats)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## poison 2 1.0330 0.5165 23.222 3.33e-07 ## treat 3 0.9212 0.3071 13.806 3.78e-06 ## poison:treat 6 0.2501 0.0417 1.874 0.112 ## Residuals 36 0.8007 0.0222 En la última columna de la tabla que obtenemos, el primer p-valor es el del contraste de la igualdad de tiempos medios de supervivencia según el veneno (poison): su valor, \\(3.3\\times 10^{-7}\\), nos permite concluir que estos tiempos medios no son todos iguales. El segundo p-valor es el del contraste de la igualdad de tiempos medios de supervivencia según el antídoto (treat): que valga \\(3.8\\times 10^{-6}\\) indica que estos tiempos medios tampoco son todos iguales. Finalmente, el tercer p-valor es el del contraste de interacción entre veneno y antídoto (poison:treat). Recordad que existe dicha interacción cuando los cambios en la variable de respuesta (en este ejemplo, el tiempo de supervivencia) originados por uno de los factores no son los mismos para diferentes niveles del otro factor: es decir, en el contexto de este experimento, si algún antídoto es más (o menos) efectivo contra algún veneno que contra otro. Como aquí este p-valor vale 0.112, no obtenemos evidencia estadísticamente significativa de dicha interacción: aceptamos que cada uno de los antídotos tiene un efecto similar sobre cada veneno (pero no todos los antídotos son igual de efectivos). Observaréis que la tabla del ANOVA que produce R con esta fórmula no contiene la fila que permite contrastar si hay diferencia entre las medias de las poblaciones definidas por combinaciones de niveles, uno por cada factor: en este ejemplo, por las combinaciones de veneno y antídoto. Si se desea obtener esta fila, basta realizar un ANOVA de un factor que combine los dos factores del experimento. summary(aov(time~poison:treat, data=rats)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## poison:treat 11 2.2044 0.20040 9.01 1.99e-07 ## Residuals 36 0.8007 0.02224 Vemos que también hay diferencias estadísticamente significativas entre las combinaciones veneno–antídoto. En el caso del ANOVA de dos vías, es interesante dibujar el gráfico de interacción entre factores. En un gráfico de interacción del factor \\(F_1\\) respecto del factor \\(F_2\\) en el ANOVA de una variable numérica \\(X\\) respecto de estos factores, se dibuja una línea quebrada para cada nivel de \\(F_1\\). Esta línea une, mediante segmentos, los valores medios que toma la variable \\(X\\) en nuestra muestra para cada nivel de \\(F_2\\) en el nivel de \\(F_1\\) correspondiente. Si no hay ninguna interacción entre estos factores, las líneas resultantes serán paralelas. Cuanto más se alejen de ser paralelas, más evidencia de interacción habrá entre estos dos factores. Estos gráficos de interacción se dibujan en R con la instrucción interaction.plot aplicada, por este orden, a los dos factores y la variable numérica. Así, pues, para obtener el gráfico de interacción de la variable antídoto respecto de la variable veneno en el ANOVA anterior, entraríamos la siguiente instrucción: interaction.plot(rats$treat, rats$poison, rats$time) Figura 9.10: Gráfico de interacción del veneno y el antídoto. En el gráfico resultante observamos que los tres niveles del veneno producen líneas casi paralelas, aunque se observa una ligera interacción: la pendiente de la recta correspondiente al veneno II entre los valores medios de C y D es mucho mayor que la de las rectas correspondientes a los otros dos venenos, lo que indica que en el experimento el antídoto D ha sido menos efectivo contra el veneno II que contra los otros dos. No obstante, la diferencia no ha sido estadísticamente significativa. Este diagrama no ha quedado muy bonito, ya que las variables aparecen en él con sus nombres completos, y los factores no aparecen en la leyenda ordenados alfabéticamente. Una posibilidad para mejorarlo es cambiar las etiquetas de los ejes y la leyenda (eliminándola con legend=FALSE y añadiéndola a nuestro gusto con la función legend). Ya que estamos, dibujaremos algo más gruesas las líneas. En general, la instrucción interaction.plot admite todos los parámetros de plot más algunos de específicos que podéis conocer consultando su Ayuda. interaction.plot(rats$treat, rats$poison, rats$time, legend=FALSE, xlab=&quot;Antídoto&quot;, ylab=&quot;Tiempo medio de supervivencia&quot;, lwd=c(2,2,2)) legend(&quot;topright&quot;, lty=1:3, cex=0.6, title=&quot;Venenos&quot;, legend=c(&quot;I&quot;,&quot;II&quot;,&quot;III&quot;)) Figura 9.11: Gráfico de interacción del veneno y el antídoto. Veamos otro ejemplo de ANOVA de dos vías. Ejemplo 9.4 En el Ejemplo 9.2 hablábamos de un cierto experimento sobre el efecto de la vitamina C en el crecimiento de los dientes, y de la tabla de datos ToothGrowth del paquete UsingR que recoge los datos obtenidos en ese experimento. Ahora vamos a realizar un ANOVA de dos factores sobre esta tabla de datos para contrastar la influencia en dicho crecimiento de la dosis de vitamina C y del método de suministrarla, teniendo en cuenta que sus efectos pueden interaccionar. Recordad de aquel ejemplo que, si queremos llevar a cabo un ANOVA sobre esta tabla que involucre la variable dose que contiene la dosis, primero hay que convertir esta variable en un factor. Volveremos a copiar la tabla ToothGrowth en una nueva tabla ToothGrowth2 y modificaremos en esta tabla dicha variable. ToothGrowth2=ToothGrowth ToothGrowth2$dose=as.factor(ToothGrowth2$dose) str(ToothGrowth2) ## &#39;data.frame&#39;:\t60 obs. of 3 variables: ## $ len : num 4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ... ## $ supp: Factor w/ 2 levels &quot;OJ&quot;,&quot;VC&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ dose: Factor w/ 3 levels &quot;0.5&quot;,&quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... Demos ahora una ojeada a los diagramas de cajas de esta tabla, separando las longitudes de los odontoblastos por dosis, por suministro y por ambos. boxplot(len~dose, data=ToothGrowth2) Figura 9.12: Diagrama de cajas de las longitudes de los odontoblastos según la dosis de vitamina C. boxplot(len~supp, data=ToothGrowth2) Figura 9.13: Diagrama de cajas de las longitudes de los odontoblastos según el método de suministro de vitamina C. boxplot(len~dose:supp, data=ToothGrowth2) Figura 9.14: Diagrama de cajas de las longitudes de los odontoblastos según la dosis y el método de suministro. Podemos observar diferencias tanto según la dosis como según el método de suministro. ¿Serán significativas? summary(aov(len~supp*dose, data=ToothGrowth2)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## supp 1 205.4 205.4 15.572 0.000231 ## dose 2 2426.4 1213.2 92.000 &lt; 2e-16 ## supp:dose 2 108.3 54.2 4.107 0.021860 ## Residuals 54 712.1 13.2 summary(aov(len~supp:dose, data=ToothGrowth2)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## supp:dose 5 2740.1 548.0 41.56 &lt;2e-16 ## Residuals 54 712.1 13.2 El resultado muestra que, efectivamente, hay diferencias significativas en el crecimiento de los dientes tanto según la dosis (p-valor prácticamente 0) como según el método de suministro (p-valor \\(2\\times 10^{-4}\\)) o la combinación de ambos (p-valor prácticamente 0), y también que hay evidencia estadística de interacción entre los dos factores (p-valor 0.022). El gráfico de interacción entre las dosis y el método de suministro que produce la instrucción siguiente refleja esta interacción: la línea correspondiente a la dosis de 2 mg no es paralela a las otras. interaction.plot(ToothGrowth2$supp, ToothGrowth2$dose, ToothGrowth2$len, legend=FALSE, xlab=&quot;Método de suministro&quot;, ylab=&quot;Crecimiento medio&quot;, lwd=c(2,2,2)) legend(&quot;topright&quot;,lty=1:3,cex=0.6,title=&quot;Dosis&quot;,legend=c(&quot;0.5&quot;,&quot;1&quot;,&quot;2&quot;)) Figura 9.15: Gráfico de interacción de la dosis de vitamina C y el método de suministro. El hecho de haber detectado interacción entre los dos factores hace que nos interese estudiar el efecto de los niveles de un factor en el otro. Esto se puede llevar a cabo mediante varios ANOVA de un factor. En primer lugar, vamos estudiar, para cada método de suministro de vitamina C, si hay diferencias entre los crecimientos medios de los odontoblastos según la dosis cuando la vitamina C se suministra mediante ese método. Para ello vamos a realizar dos ANOVA de un factor, la dosis, sobre dos data frames extraídos de ToothGrowth2, uno con las filas donde supp toma el valor OJ y otro con las filas donde supp toma el valor VC. Vamos extraer estas subtablas de datos usando la instrucción subset. Recordad que la construcción subset(df, condición) define un data frame con las filas del data frame df que cumplen la condición. TG.OJ=subset(ToothGrowth2, supp==&quot;OJ&quot;) TG.VC=subset(ToothGrowth2, supp==&quot;VC&quot;) Ahora realizamos los dos ANOVA: summary(aov(len~dose, data=TG.OJ)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dose 2 885.3 442.6 31.44 8.89e-08 ## Residuals 27 380.1 14.1 summary(aov(len~dose, data=TG.VC)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dose 2 1650 824.7 67.07 3.36e-11 ## Residuals 27 332 12.3 En ambos casos el p-valor es muy pequeño, lo que indica que para cada tipo de suministro de vitamina C, el crecimiento medio de los dientes varía con la dosis. En este tipo de análisis, hay que tener en cuenta que si realizamos \\(N\\) contrastes independientes en un mismo experimento, cada uno de ellos con un nivel de significación \\(\\alpha\\), la probabilidad de rechazar en alguno de ellos la hipótesis nula si en todos es verdadera es \\[ 1-(1-\\alpha)^N\\approx N\\alpha. \\] Por lo tanto, si queremos garantizar un nivel de significación global \\(\\alpha\\), hay que realizar cada contraste con nivel de significación \\(\\alpha/N\\). (Para ser exactos, si llamamos \\(\\alpha_c\\) al nivel de significación de cada contraste necesario para obtener un nivel de significación global \\(\\alpha\\), se cumple que \\(1-(1-\\alpha_c)^N=\\alpha\\), de donde podemos despejar \\(\\alpha_c\\) y obtenemos \\(\\alpha_c=1-\\sqrt[N]{1-\\alpha}\\), que es un poco mayor que \\(\\alpha/N\\). Por ejemplo, para \\(\\alpha=0.05\\) y \\(N=20\\) tenemos que \\(\\alpha_c=0.0341\\) mientras que \\(\\alpha/N=0.025\\).) En este ejemplo concreto, si hubiéramos deseado un nivel de significación global 0.05, deberíamos haber realizado cada uno de los ANOVA con un nivel de significación 0.025, lo que no afecta a las conclusiones, ya que los p-valores han sido realmente muy pequeños. También podemos estudiar, para cada dosis, si hay diferencias entre las medias de crecimiento según el método de suministro. Como solo hay dos métodos de suministro, para cada dosis tanto da realizar un ANOVA o un simple test t suponiendo que las varianzas son iguales (ya que es una de las condiciones para poder realizar el ANOVA). TG.0.5=subset(ToothGrowth2, dose==0.5) summary(aov(len~supp, data=TG.0.5)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## supp 1 137.8 137.81 10.05 0.0053 ## Residuals 18 246.9 13.72 TG.1=subset(ToothGrowth2, dose==1) summary(aov(len~supp, data=TG.1)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## supp 1 175.8 175.82 16.26 0.000781 ## Residuals 18 194.6 10.81 TG.2=subset(ToothGrowth2, dose==2) summary(aov(len~supp, data=TG.2)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## supp 1 0.03 0.032 0.002 0.964 ## Residuals 18 270.61 15.034 En este caso, para garantizar un nivel de significación global de 0.05, hay que realizar cada contraste con un nivel de significación de 0.05/3=0.017. Obtenemos evidencia de que hay diferencia en el crecimiento medio de los odontoblastos según el tipo de suministro solo para las dosis de 0.5 y 1 mg, que es donde hemos obtenido p-valores inferiores al nivel de significación. Si la variable dose hubiera tenido, pongamos, 20 niveles en vez de 3, definir una subtabla para cada nivel y efectuar el ANOVA correspondiente hubiera sido muy largo. Para automatizar cálculos como estos, lo mejor es definir una función que calcule el p-valor del ANOVA de una variable respecto de un factor sobre la subtabla definida por un nivel variable \\(x\\) de un segundo factor, y luego aplicar esta función al vector de los niveles de este segundo factor. En este ejemplo concreto usaríamos el código siguiente. Anova_dosis=function(x){ summary(aov(len~supp, data=subset(ToothGrowth2, dose==x)))[[1]]$&quot;Pr(&gt;F)&quot;[1] } round(sapply(levels(ToothGrowth2$dose), FUN=Anova_dosis),4) ## 0.5 1 2 ## 0.0053 0.0008 0.9637 9.5 Condiciones del ANOVA Los resultados de un ANOVA nos permiten extraer conclusiones solo si las poblaciones a las que se aplica cumplen una serie de condiciones; las dos básicas son la normalidad y la igualdad de varianzas. La condición de normalidad requiere que cada nivel de cada factor defina una distribución normal, es decir, que cada muestra aleatoria simple de un tratamiento provenga de una población normal. La condición de igualdad de varianzas requiere que todas estas distribuciones tengan la misma varianza. En la Lección 6 ya estudiamos algunos tests específicos de normalidad: los tests de Kolmogorov-Smirnov-Lillliefors, de Anderson-Darling, de Shapiro-Wilk y de D’Agostino-Pearson. Cada uno tiene sus ventajas y sus inconvenientes: el primero es muy popular, por ser una variante del test de Kolmogorov-Smirnov; el segundo, detecta mejor las discrepancias en los extremos; el test de Shapiro-Wilk es más potente para muestras grandes y menos sensible a repeticiones que los dos anteriores; y el test de D’Agostino-Pearson permite repeticiones de datos, pero no se puede usar con muestras de tamaño inferior a 20. Veamos algunos ejemplos de aplicación de tests de normalidad en el contexto de un ANOVA. Ejemplo 9.5 Queremos contrastar si los tiempos de coagulación bajo cuatro dietas dados en la Tabla 9.1 provienen de distribuciones normales. Como hay algunas repeticiones de valores en las columnas de la tabla y no llegan a los 20 valores cada una, usaremos el test de Shapiro-Wilk que, recordemos, está implementado en la función shapiro.test. Teníamos los datos guardados en un data frame llamado coagulacion. str(coagulacion) ## &#39;data.frame&#39;:\t24 obs. of 2 variables: ## $ coag: num 62 63 68 56 60 67 66 62 63 71 ... ## $ diet: Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;: 1 2 3 4 1 2 3 4 1 2 ... Apliquemos la función shapiro.test a los tiempos de coagulación para cada dieta: coagA=coagulacion[coagulacion$diet==&quot;A&quot;,]$coag shapiro.test(coagA)$p.value ## [1] 0.1130298 coagB=coagulacion[coagulacion$diet==&quot;B&quot;,]$coag shapiro.test(coagB)$p.value ## [1] 0.5227052 coagC=coagulacion[coagulacion$diet==&quot;C&quot;,]$coag shapiro.test(coagC)$p.value ## [1] 0.2375366 coagD=coagulacion[coagulacion$diet==&quot;D&quot;,]$coag shapiro.test(coagD)$p.value ## [1] 0.5227052 Los cuatro p-valores que obtenemos son grandes, mayores que 0.05, lo que quiere decir que en ninguno de los cuatro tests podemos rechazar la hipótesis nula: no hay evidencia de que los tiempos de coagulación bajo alguna de las cuatro dietas no sigan leyes normales, por lo que aceptamos que sí las siguen. Recordad además que, como hemos efectuado cuatro contrastes, para garantizar un nivel de significación global de 0.05 hemos de realizar cada uno con un nivel de significación de 0.05/4=0.0125: es decir, que con p-valores por encima de este nivel de significación no podríamos rechazar ninguna hipótesis nula. ¿Cómo podríamos haber automatizado el cálculo de estos p-valores, para no tener que definir “a mano” cada vector de tiempos de coagulación? Con el código siguiente, donde definimos una función Test.SW que calcula el p-valor del test de Shapiro-Wilk y la aplicamos con un aggregate a los tiempos de coagulación del data frame coagulacion separados según la dieta. De esta manera obtenemos un data frame cuya columna coag contiene, para cada nivel de diet, el p-valor del test de Shapiro-Wilk para los tiempos de coagulación de la dieta correspondiente. Test.SW=function(x){shapiro.test(x)$p.value} aggregate(coag~diet, data=coagulacion,FUN=Test.SW) ## diet coag ## 1 A 0.1130298 ## 2 B 0.5227052 ## 3 C 0.2375366 ## 4 D 0.5227052 Ejemplo 9.6 Vamos a contrastar si en el ANOVA de dos vías del ejemplo de venenos y antídotos explicado en la Sección 9.4 se satisface que cada combinación de veneno y antídoto define una población normal. Para ello, en principio deberíamos repetir 12 veces, una para cada combinación de veneno y antídoto, un par de instrucciones como las siguientes: time.I.A=rats[rats$poison==&quot;I&quot; &amp; rats$treat==&quot;A&quot;, ]$time shapiro.test(time.I.A)$p.value ## [1] 0.07414486 Para evitarnos trabajo, vamos a usar la estrategia explicada al final del ejemplo anterior: aplicaremos la función Test.SW que hemos definido en dicho ejemplo a la variable numérica time del data frame rats separándola por las combinaciones de veneno y antídoto. aggregate(time~poison:treat, data=rats, FUN=Test.SW) ## poison treat time ## 1 I A 0.07414486 ## 2 II A 0.84756406 ## 3 III A 0.57735490 ## 4 I B 0.69983383 ## 5 II B 0.70083721 ## 6 III B 0.17057001 ## 7 I C 0.40503490 ## 8 II C 0.92091109 ## 9 III C 0.97187706 ## 10 I D 0.42739119 ## 11 II D 0.90650963 ## 12 III D 0.68893644 Todos los p-valores que obtenemos son mayores que 0.05 (y de hecho, para garantizar un nivel de significación global de 0.05, deberíamos realizar cada test de Shapiro-Wilk con un nivel de significación 0.05/12=0.0042), por lo que no podemos rechazar que todas las combinaciones de veneno y antídoto definan unos tiempos de supervivencia que sigan leyes normales. Pasemos al contraste de igualdad de varianzas (su nombre técnico es contraste de homocedasticidad), en el que usaremos el llamado test de Bartlett. Supongamos, para fijar ideas, que tenemos \\(k\\) vectores numéricos \\[ x_i=(x_{i1},x_{i2},\\ldots,x_{in_i}),\\quad i=1,\\ldots, k, \\] cada uno de los cuales es una muestra aleatoria simple de una variable aleatoria normal (se supone que lo hemos contrastado previamente) de varianza \\(\\sigma_i^2\\). El contraste que queremos realizar es: \\[ \\left\\{ \\begin{array}{ll} H_0: &amp; \\sigma_1^2 = \\cdots = \\sigma_k^2\\\\ H_1: &amp; \\mbox{$\\sigma_i^2 \\neq \\sigma_j^2$ para algunos $i, j$} \\end{array} \\right. \\] Para realizar este contraste, se usa el estadístico de Bartlett: \\[ K^2 = \\frac{ (N-k)\\ln (\\widetilde{s}_p^2)-\\sum_{i=1}^k (n_i -1)\\ln (\\widetilde{s}_i^2)}{ 1+\\frac{1}{3(k-1)}\\left(\\sum_{i=1}^k \\left(\\frac{1}{n_i -1}\\right)-\\frac{1}{N-k}\\right)}, \\] donde \\[ N=\\sum_{i=1}^k n_i,\\quad \\widetilde{s}_p^2 = \\frac{\\sum_{i=1}^k (n_i -1) \\widetilde{s}_i^2}{N-k},\\quad \\widetilde{s}_i^2 =\\frac{\\sum_{j=1}^{n_i} (x_{ij}-\\overline{x}_{i})^2}{n_i -1},\\ i=1,\\ldots,k. \\] Si las variables poblacionales son normales, este estadístico sigue aproximadamente una distribución \\(\\chi^2_{k-1}\\), y si la hipótesis nula es verdadera, su valor esperado es pequeño (puesto que, en este caso, los valores esperados de \\(\\widetilde{s}_1^2,\\ldots, \\widetilde{s}_k^2\\) y \\(\\widetilde{s}_p^2\\) son todos iguales). El p-valor del contraste es entonces \\(P(\\chi^2_{k-1} &gt; K^2)\\). R tiene una función que efectúa este test, bartlett.test, y puede aplicarse a una fórmula. Por ejemplo, para contrastar si las tres dietas en el ejemplo de la coagulación de la Sección 9.2 definen poblaciones con la misma varianza, podemos entrar bartlett.test(coag~diet, data=coagulacion) ## ## Bartlett test of homogeneity of variances ## ## data: coag by diet ## Bartlett&#39;s K-squared = 1.946, df = 3, p-value = 0.5837 Como el p-valor es alto, concluimos que las varianzas de los tiempos de coagulación bajo las tres dietas son la misma. En los ANOVA de dos vías, se ha de comprobar la igualdad de varianzas para las combinaciones de niveles de los dos factores. En la fórmula que se entra al bartlett.test esta combinación no se puede especificar con :, como hemos hecho hasta ahora, sino con interaction. Por ejemplo, para contrastar por medio del test de Bartlett si las 12 combinaciones veneno-dieta en el ejemplo de venenos y antídotos explicado en la Sección 9.4 tienen la misma varianza, tenemos que entrar: bartlett.test(time~interaction(poison,treat), data=rats) ## ## Bartlett test of homogeneity of variances ## ## data: time by interaction(poison, treat) ## Bartlett&#39;s K-squared = 45.137, df = 11, p-value = 4.59e-06 Obtenemos un p-valor de \\(4.6\\times 10^{-6}\\), por lo tanto, hemos de rechazar la igualdad de varianzas: las conclusiones de ese ANOVA de dos vías no tienen en principio ninguna validez, puesto que hay evidencia de que no se cumple una de las condiciones para realizarlo. Otros tests de igualdad de varianzas son el de Fligner-Killeen, implementado en la función fligner.test (y que tiene la misma particularidad que bartlett.test a la hora de especificar combinaciones de factores) y el test de Levene, implementado en la función leveneTest del paquete car (donde las combinaciones de factores sí que se pueden especificar con :). En cada caso, el test produce un p-valor con el significado usual. El test de Levene es muy popular, porque no requiere que las poblaciones sean normales, pero en el contexto del ANOVA esta propiedad no tiene ningún interés; además, cuando las poblaciones son normales, el test de Bartlett es más potente que el de Levene. 9.6 Comparaciones de pares de medias Si en un ANOVA hemos rechazado la hipótesis nula de la igualdad de todas las medias, es posible que nos interese determinar qué medias son diferentes. Para ello podemos llevar a cabo algunos tests. En esta sección explicamos las instrucciones de R para tres de ellos. 9.6.1 Tests t por parejas En los tests t por parejas, realizamos un contraste de medias para cada par de niveles del factor, teniendo en cuenta que, para garantizar un nivel de significación global \\(\\alpha\\), se ha de ajustar el nivel de significación de cada test. Hay diversos métodos para llevar a cabo este ajuste: los más populares son el método de Bonferroni, el de Holm, que es más potente, y el de Hochberg, que, para contrastes ANOVA que no sean de bloques, aún tiene mayor potencia. La función que efectúa tests t por parejas es pairwise.t.test. Su sintaxis es pairwise.t.test(var.numérica, factor, paired=..., p.adjust.method=...) El valor de p.adjust.method es el método de ajuste de p-valores que deseamos usar, y se ha de entrar entrecomillado. El valor por defecto es el de Holm, indicado por \"holm\" aunque no es necesario especificarlo. Para usar el de Bonferroni, hay que especificar p.adjust.method=\"bonferroni\", y para usar el de Hochberg, p.adjust.method=\"hochberg\". Si no queréis ningún ajuste, tenéis que usar p.adjust.method=\"none\". Para conocer qué otros métodos se pueden usar, sus ventajas e inconvenientes, consultad la Ayuda de p.adjust. Por lo que refiere al parámetro paired, sirve para indicar si las muestras son independientes (igualándolo a FALSE, que es su valor por defecto) o emparejadas (igualándolo a TRUE); cuando el ANOVA es de bloques, las muestras son emparejadas, y por lo tanto se ha de especificar paired=TRUE. Veamos algunos ejemplos de su uso. Ejemplo 9.7 Para determinar cuáles de las tres dietas en el ejemplo de la coagulación de la Sección 9.2 dan tiempos medios de coagulación diferentes, vamos a efectuar un test t por parejas de Bonferroni. pairwise.t.test(coagulacion$coag,coagulacion$diet,p.adjust.method=&quot;bonferroni&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: coagulacion$coag and coagulacion$diet ## ## A B C ## B 0.00934 - - ## C 0.00031 0.95266 - ## D 1.00000 0.00934 0.00031 ## ## P value adjustment method: bonferroni Los p-valores del resultado ya han sido ajustados, y por lo tanto se han de comparar directamente con el nivel de significación \\(\\alpha\\) global. Por ejemplo, con un nivel de significación \\(\\alpha=0.05\\), obtenemos evidencia de que los tiempos medios para los pares de dietas (A,B), (A,C), (B,D), (C,D) son diferentes, y en cambio no hay evidencia de que los tiempos medios para los pares (A,D) y (B,C) sean diferentes. Ejemplo 9.8 Para determinar qué actividades de evaluación de las consideradas en el Ejemplo 9.3 dan notas medias diferentes, vamos a realizar un test t por parejas de Holm. Primero vamos a comprobar que dichas actividades de evaluación definen poblaciones normales, condición necesaria para que los tests t por parejas tengan sentido. Contrastaremos la normalidad de las notas para cada actividad de evaluación con el test de Shapiro-Wilk, y para no trabajar más de la cuenta, usaremos la técnica explicada en el Ejemplo 9.5. Test.SW=function(x){round(shapiro.test(x)$p.value,4)} aggregate(notas~acts, data=notas.bloques, FUN=Test.SW) ## acts notas ## 1 Casa 0.1384 ## 2 Control 1 0.7054 ## 3 Control 2 0.5515 ## 4 Talleres 0.7612 ## 5 Tests 0.5984 Todos los p-valores son altos, los que nos permite aceptar que las notas de cada actividad provienen de una distribución normal. Así pues, podemos realizar el test t de Holm. En este caso tenemos que usar paired=TRUE, puesto que se trata de un ANOVA de bloques. pairwise.t.test(notas.bloques$notas, notas.bloques$acts, paired=TRUE) ## ## Pairwise comparisons using paired t tests ## ## data: notas.bloques$notas and notas.bloques$acts ## ## Casa Control 1 Control 2 Talleres ## Control 1 0.4562 - - - ## Control 2 0.0271 0.4562 - - ## Talleres 0.0012 0.0038 0.0012 - ## Tests 0.4562 0.1640 0.0271 0.0038 ## ## P value adjustment method: holm Los p-valores superiores a 0.05 son los de los pares de actividades “Ejercicios de casa”-“Control 1”, “Control 1”-“Control 2”, “Ejercicios de casa”-“Tests” y “Control 1”-“Tests”. Por lo tanto, a un nivel de significación global de 0.05, no podemos rechazar que las notas medias de estos pares de actividades sean iguales, mientras que en los otros tenemos evidencia de que son diferentes. 9.6.2 Test de Duncan Otro método popular para comparar los pares de medias es el test de Duncan. Con R se puede efectuar con la instrucción duncan.test del paquete agricolae. Su sintaxis básica es duncan.test(aov, &quot;factor&quot;, alpha=..., group=...)$sufijo donde aov es el resultado de la función aov (sin summary) con la que hemos calculado el ANOVA de partida. El factor es el factor del ANOVA, y se ha de entrar entrecomillado y con el mismo nombre que se ha usado en la fórmula del aov. El parámetro alpha sirve para entrar el nivel de significación \\(\\alpha\\): por defecto vale 0.05. group puede valer TRUE o FALSE, y hace que el resultado se presente de forma diferente. El sufijo tiene que ser groups si group=TRUE y comparison si group=FALSE. Este test no se puede usar en el ANOVA de bloques, puesto que supone que las muestras de los tratamientos son independientes. Ejemplo 9.9 Vamos a usar el test de Duncan para determinar qué pares de dietas en el ejemplo de la coagulación de la Sección 9.2 tienen tiempos medios de coagulación diferentes, con un nivel de significación global del 5%. En primer lugar, guardamos en un objeto el ANOVA calculado con aov, y luego aplicamos la función duncan.test a este objeto. Aquí lo haremos dos veces, una para cada posible valor del parámetro group, para poder comparar y comentar cómo se muestran los resultados. No entraremos el parámetro alpha, puesto que usamos su valor por defecto. library(agricolae) anova.coag=aov(coag~diet, data=coagulacion) duncan.test(anova.coag, &quot;diet&quot;, group=FALSE)$comparison ## difference pvalue signif. LCL UCL ## A - B -5 0.0016 ** -7.849968 -2.1500315 ## A - C -7 0.0001 *** -9.991509 -4.0084906 ## A - D 0 1.0000 -2.849968 2.8499685 ## B - C -2 0.1588 -4.849968 0.8499685 ## B - D 5 0.0021 ** 2.008491 7.9915094 ## C - D 7 0.0001 *** 3.918538 10.0814618 duncan.test(anova.coag, &quot;diet&quot;, group=TRUE)$groups ## coag groups ## C 68 a ## B 66 a ## A 61 b ## D 61 b Observemos los resultados: Con group=FALSE y sufijo comparison, obtenemos una tabla donde, para cada pareja de niveles del factor, se da, entre otros datos, un p-valor (la columna p value). Estos p-valores tienen el significado usual. En este ejemplo, de los p-valores se desprende que no podemos rechazar que las medias para los pares A-D y B-C sean iguales (los p-valores son grandes), mientras que, en cambio, hay evidencia significativa de que el resto de los pares de dietas tienen medias diferentes (los p-valores son pequeños). Las dos últimas columnas de la tabla definen intervalos de confianza (del nivel de confianza correspondiente al nivel de significación que hayamos entrado con alpha) para las diferencias de la medias en el orden especificado en la primera columna. Con group=TRUE y sufijo groups, obtenemos una tabla donde se agrupan los niveles según la igualdad de medias. En este caso, asigna las dietas B y C al grupo a y las dietas A y D al grupo b: dietas en el mismo grupo hemos de aceptar que tienen medias iguales, dietas en grupos diferentes podemos concluir que tienen medias diferentes. 9.6.3 Método de Tukey El método HSD de Tukey (las siglas HSD refieren a Honestly Significant Difference) es el método más preciso de comparación de parejas de medias para contrastes ANOVA de un factor en los que cada nivel tenga el mismo número de observaciones. Es un test similar al test t, pero usa otro estadístico con una distribución diferente, no vamos a entrar en detalles. Con R se efectúa aplicando la función TukeyHSD al resultado de aov. Para explicar qué información obtenemos con esta función, vamos a aplicar el método de Tukey al ANOVA del ejemplo sobre tiempos de coagulación de la Sección 9.2. anova.coag=aov(coag~diet, data=coagulacion) TukeyHSD(anova.coag) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = coag ~ diet, data = coagulacion) ## ## $diet ## diff lwr upr p adj ## B-A 5.000000e+00 1.175925 8.824075 0.0077978 ## C-A 7.000000e+00 3.175925 10.824075 0.0002804 ## D-A 1.421085e-14 -3.824075 3.824075 1.0000000 ## C-B 2.000000e+00 -1.824075 5.824075 0.4766005 ## D-B -5.000000e+00 -8.824075 -1.175925 0.0077978 ## D-C -7.000000e+00 -10.824075 -3.175925 0.0002804 De esta manera, para cada par de niveles obtenemos, entre otra información: La diferencia de sus medias muestrales, en la columna diff. Los extremos inferior y superior de un intervalo de confianza del 95% para la diferencia de medias poblacionales en el orden dado en la primera columna, en las columnas lwr y upr, respectivamente. El nivel de confianza se puede ajustar con el parámetro conf.level dentro de la función. El p-valor del contraste de igualdad de medias correspondiente, en la columna p adj. En nuestro ejemplo, los intervalos de confianza para las diferencias de medias de los pares A-B, A-C, B-D y C-D no contienen el valor 0 (y los p-valores correspondientes son pequeños), lo que indica que podemos rechazar que estas medias sean iguales. En cambio, los intervalos de confianza para las diferencias de medias de los pares A-D y B-C sí que contienen el 0 (y los p-valores correspondientes son grandes), lo que indica que podemos aceptar que estas medias son iguales. En este caso obtenemos, por lo tanto, la misma conclusión que con el test t de Bonferroni. 9.7 Métodos no paramétricos Cuando no se satisfacen las condiciones para poder aplicar un ANOVA, hay que usar algún otro método que no las requiera. En el caso del ANOVA de una vía, el método alternativo no paramétrico recomendado es el test de Kruskal-Wallis, que generaliza a más de dos muestras el test de Mann-Whitney-Wilcoxon para dos muestras independientes en el mismo sentido que el ANOVA generaliza el test t; en particular, un test de Kruskal-Wallis para un factor con solo dos niveles es equivalente al test de Mann-Whitney. Con R, el test de Kruskal-Wallis se lleva a cabo con la función kruskal.test, con la misma sintaxis que aov. Naturalmente, el resultado no es una tabla ANOVA, porque no estamos haciendo un ANOVA, pero da un p-valor que indica si podemos rechazar o no que las medias por niveles sean todas iguales. Por ejemplo, para efectuar el test de Kruskal-Wallis sobre la tabla de tiempos de coagulación del principio de la Sección 9.2, entraríamos: kruskal.test(coag~diet, data=coagulacion) ## ## Kruskal-Wallis rank sum test ## ## data: coag by diet ## Kruskal-Wallis chi-squared = 17.027, df = 3, p-value = 0.0006977 El p-valor nos da evidencia estadísticamente significativa de que no todos los tiempos medios de coagulación son iguales. Para determinar entonces qué pares de dietas dan tiempos medios diferentes, podríamos llevar a cabo un contraste por parejas. En principio no podremos usar un test t por parejas, ya que este presupone las condiciones de normalidad de las poblaciones e igualdad de las varianzas que son necesarias para el ANOVA y que suponemos que no se satisfacen (o habríamos hecho un ANOVA en vez de un test de Kruskal-Wallis). En su lugar, podemos usar un test de Mann-Whitney por parejas, con la función pairwise.wilcox.test. Su sintaxis es la misma que la de pairwise.t.test. En particular, hay que indicar con p.adjust.method el método de ajuste de los p-valores, siendo de nuevo \"holm\" su valor por defecto. Así, por ejemplo, para realizar un test de Mann-Whitney por parejas con ajuste de Bonferroni, entraríamos: pairwise.wilcox.test(coagulacion$coag,coagulacion$diet,p.adjust.method=&quot;bonferroni&quot;) ## ## Pairwise comparisons using Wilcoxon rank sum test ## ## data: coagulacion$coag and coagulacion$diet ## ## A B C ## B 0.046 - - ## C 0.028 0.630 - ## D 1.000 0.076 0.029 ## ## P value adjustment method: bonferroni Con un nivel de significación de 0.05, obtenemos evidencia de que los pares de dietas (A,B), (A,C) y (C,D) dan tiempos medios de coagulación diferentes, y no podemos rechazar que los otros pares de dietas den el mismo tiempo de coagulación. El resultado ha sido ligeramente diferente del test t por parejas, donde también obteníamos evidencia de diferencia de medias entre las dietas B y D. Recordad que, en general, los métodos no paramétricos tienen potencia menor y por lo tanto les cuesta más detectar diferencias. Por lo que refiere al ANOVA de bloques completos aleatorios, el método no paramétrico alternativo más popular es el test de Friedman, que generaliza a más de dos muestras el test de Wilcoxon para muestras emparejadas. Está implementado en la función friedman.test, cuya sintaxis es de nuevo la misma que la de aov excepto que la suma de los tratamientos y los bloques no se indica en la fórmula con un signo de suma sino con una barra vertical. Por ejemplo, para efectuar el test de Friedman sobre la tabla de datos de notas en actividades de evaluación del Ejemplo 9.3, entraríamos: friedman.test(notas~acts|bloques, data=notas.bloques) ## ## Friedman rank sum test ## ## data: notas and acts and bloques ## Friedman chi-squared = 30.047, df = 4, p-value = 4.787e-06 El p-valor nos indica que hay evidencia estadísticamente significativa de que algunas notas medias fueron diferentes. Para determinar cuáles, podemos llevar a cabo un test de Wilcoxon por parejas, usando la función pairwise.wilcox.test con paired=TRUE. Por ejemplo, usando de nuevo el ajuste de Bonferroni: pairwise.wilcox.test(notas.bloques$notas, notas.bloques$acts, paired=TRUE, p.adjust.method=&quot;bonferroni&quot;) ## ## Pairwise comparisons using Wilcoxon signed rank test ## ## data: notas.bloques$notas and notas.bloques$acts ## ## Casa Control 1 Control 2 Talleres ## Control 1 1.0000 - - - ## Control 2 0.0823 1.0000 - - ## Talleres 0.0133 0.0285 0.0031 - ## Tests 1.0000 0.5693 0.1816 0.0132 ## ## P value adjustment method: bonferroni Encontramos evidencia de que las notas medias de los Talleres fueron diferentes de las del resto de actividades, y no podemos rechazar que las notas medias de las otras actividades fueran todas iguales. Finalmente, en el caso de experimentos con dos o más factores, no hay una solución simple cuando no se satisfacen las condiciones necesarias para poder hacer un ANOVA. Una posible salida es usar un método adecuado de bootstrap. Sin entrar en detalles, una buena opción en un experimento de dos factores es usar la función pbad2way del paquete WRS2. Su sintaxis básica es pbad2way(fórmula, est=&quot;median&quot;, nboot=...) En el parámetro nboot se ha de entrar el número de muestras que se han de tomar; tened en cuenta que cuántas más toméis, mas tardará la ejecución. El parámetro est indica qué parámetro se quiere comparar, la media no está entre los valores disponibles así que aquí usaremos la mediana. Consultad la Ayuda de la función (y las referencias que se citan) para saber qué otros estadísticos podéis usar. Por ejemplo, para llevar a cabo por medio de un bootstrap el contraste del Ejemplo 9.4 sobre el efecto de la vitamina C en el crecimiento de los dientes (y recordando que la tabla ToothGrowth2 contiene los datos con los tratamientos codificados como factores) podríamos entrar: library(WRS2) set.seed(42) #Fijamos la semilla de aleatoriedad, para que sea reproducible pbad2way(len~supp*dose, data=ToothGrowth2, est=&quot;median&quot;, nboot=500) ## Call: ## pbad2way(formula = len ~ supp * dose, data = ToothGrowth2, est = &quot;median&quot;, ## nboot = 500) ## ## p.value ## supp 0.002 ## dose 0.000 ## supp:dose 0.034 Grosso modo, los p-valores obtenidos tienen el significado usual en un bootstrap: la fracción de muestras en las que obtenemos un valor de un cierto estadístico tan o más extremo que el de la muestra global. Un p-valor pequeño nos da evidencia de que podemos rechazar la hipótesis nula. Los p-valores de las filas con los nombres de los factores son los de los contrastes de los mismos y el p-valor de la última fila es el del contraste de interacción. Por lo tanto, con este contraste concluimos que hay evidencia de que tanto la dosis de vitamina C como el método de suministrarla influyen en el crecimiento de los odontoblastos (en concreto, en su mediana) y que hay interacción entre ambos factores. Como en el ANOVA, el contraste de la combinación de factores se ha de efectuar aparte: en este caso (ya que no podemos llevar a cabo un ANOVA, o no nos estaríamos complicando tanto la vida) una buena opción es usar un test de Kruskal-Wallis tomando como factor la combinación de factores (entrada en la fórmula por medio de interaction). kruskal.test(len~interaction(supp,dose),data=ToothGrowth2) ## ## Kruskal-Wallis rank sum test ## ## data: len by interaction(supp, dose) ## Kruskal-Wallis chi-squared = 45.806, df = 5, p-value = 9.948e-09 Concluimos que también hay evidencia de que la combinación de la dosis de vitamina C y el método de suministrarla influye en el crecimiento de los odontoblastos. Para terminar, una advertencia sobre pbad2way: a veces da un error numérico, como en la siguiente ejecución en el otro ejemplo de experimento de 2 vías de la Sección 9.4: pbad2way(time~poison*treat, data=rats, est=&quot;median&quot;, nboot=500) ## Error in solve.default(cov, ...): system is computationally singular: reciprocal condition number = 5.31267e-19 En este caso, el inventor del método recomienda emplear el parámetro pro.dis=TRUE, que modifica el método de cálculo de los estadísticos que se usan en el bootstrap y evita el error. set.seed(42) pbad2way(time~poison*treat, data=rats, est=&quot;median&quot;,nboot=500,pro.dis=TRUE) ## Call: ## pbad2way(formula = time ~ poison * treat, data = rats, est = &quot;median&quot;, ## nboot = 500, pro.dis = TRUE) ## ## p.value ## poison 0.000 ## treat 0.000 ## poison:treat 0.174 9.8 Guía rápida Fórmulas: X~F1 ajusta la variable X al factor F1. X~F1+F2 ajusta la variable X a los factores F1 y F2 sin tener en cuenta la interacción entre los factores. X~F1*F2 ajusta la variable X a los factores F1 y F2 teniendo en cuenta además la interacción entre los factores. X~F1:F2 ajusta la variable X a un factor que tiene como niveles los pares de niveles de F1 y F2. options(show.signif.stars=...) sirve para desactivar (usando FALSE) o activar (con TRUE) los códigos de significación por medio de estrellitas. aov(fórmula) efectúa el ANOVA indicado por la fórmula. El parámetro adicional data sirve para indicar el data frame del que se extraen los vectores y factores de la fórmula. summary(aov(...)) muestra la tabla del ANOVA efectuado con la función aov. Añadiendo el sufijo [[1]]$\"Pr(&gt;F)\" nos da directamente la columna de p-valores de la tabla. anova(lm(fórmula, data=...)) también produce la tabla del ANOVA indicado por la fórmula. Con el sufijo $\"Pr(&gt;F)\", nos da directamente la columna de p-valores de la tabla. interaction.plot(factor1,factor2,vector) produce el gráfico de interacción del vector respecto de los dos factores. subset(dataframe,condición) define un data frame con las filas del dataframe que cumplen la condición. shapiro.test(x) realiza el test de normalidad de Shapiro-Wilk sobre el vector numérico x. aggregate sirve para aplicar una función a una o varias variables de un data frame agrupando sus entradas por los niveles de uno o varios factores. bartlett.test, con la misma sintaxis que aov, efectúa el test de Bartlett de homocedasticidad. pairwise.t.test(x,factor) efectúa un test t por parejas de la variable x separada por los niveles del factor. Dos parámetros importantes: paired: se ha de igualar a TRUE en los ANOVA de bloques. p.adjust.method: sirve para indicar el método de ajuste de los p-valores. duncan.test(aov,factor)$sufijo del paquete agricolae, efectúa el test de Duncan del ANOVA aov. Sus dos parámetros más importantes para nosotros son alpha, que sirve para indicar el nivel de significación y group, que sirve para indicar cómo queremos que muestre el resultado. El sufijo ha de ser ser groups si group=TRUE y comparison si group=FALSE. TukeyHSD(aov) efectúa el test HSD de Tukey del ANOVA aov. kruskal.test efectúa el test de Kruskal-Wallis. Su sintaxis es la misma que la de aov. pairwise.wilcox.test efectúa un test de Wilcoxon (con paired=TRUE) o de Mann-Whitney-Wilcoxon (con paired=FALSE, el valor por defecto de este parámetro) por parejas. Su sintaxis es la misma que la de pairwise.t.test. friedman.test efectúa el test de Friedman. Su sintaxis es la misma que la de aov. pbad2way(fórmula, data=..., est=\"median\") del paquete WRS2 lleva a cabo un contraste bootstrap de medianas de los datos de un experimento factorial de 2 vías. El parámetro nboot sirve para indicar el número de muestras a usar. 9.9 Ejercicios Test (1) En un experimento queremos comparar el valor medio de una cierta variable en cuatro poblaciones, A, B, C y D. Hemos tomado una muestra de 5 individuos de cada población y hemos medido en ellos la variable en cuestión. Los resultados para cada población han sido los siguientes: A=(23.1,17.2,20.1,19.5,18.8), B=(22.6,15.5,19.3,18.5,16.2), C=(21.1,26.4,24.7,21.4,20.9) y D=(20.4,24,23.4,24.9,21.5). Realizad un ANOVA de 1 vía sobre estos datos. Tenéis que dar el p-valor del contraste de la igualdad de las medias redondeado a 3 cifras decimales (sin ceros innecesarios a la derecha) y decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%, la igualdad de todas las medias. Dad el p-valor y la conclusión en este orden, separados por un único espacio en blanco. (2) En un experimento queremos comparar la influencia de cuatro tratamientos, A, B, C y D, sobre una determinada variable fisiológica. Para ello hemos aplicado cada tratamiento a los mismos 5 individuos, pero en órdenes y momentos escogidos al azar, y hemos medido en ellos la variable en cuestión tras cada tratamiento. Los resultados para cada tratamiento (ordenados en cada uno en el mismo orden, correspondiente a los individuos) han sido los siguientes: A=(23.1,17.2,20.1,19.5,18.8), B=(22.6,15.5,19.3,18.5,16.2), C=(21.1,26.4,24.7,21.4,20.9) y D=(20.4,24,23.4,24.9,21.5). Realizad un ANOVA de bloques sobre estos datos. Tenéis que dar el p-valor del contraste de la igualdad de las medias de los tratamientos redondeado a 3 cifras decimales (sin ceros innecesarios a la derecha) y decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%, la igualdad de todas las medias de los tratamientos. Dad el p-valor y la conclusión en este orden, separados por un único espacio en blanco. (3) En un experimento queremos comparar el valor medio de una cierta variable en cuatro poblaciones, A, B, C y D; los individuos de cada población además pueden clasificarse en dos tipos, X e Y, según una determinada característica que creemos que será relevante en nuestro estudio. Hemos tomado una muestra de 5 individuos de cada combinación población-característica y hemos medido en ellos la variable en cuestión. Los resultados para cada combinación población-característica han sido los siguientes: A-X=(29.2,30.9,31.3,30.0,30.0), B-X=(29.1,31.3,31.4,34.3,32.3), C-X=(31.3,40.7,32.6,34.8, 38.3), D-X=(32.2,36.0,31.6,31.8,32.6), A-Y=(26.3,25.9,25.6,25.3,24.9), B-Y=(30.7,29.8, 27.9,28.8,28.1), C-Y=(26.2,26.7,27.2,29.3,28.4), y D-Y=(20.5,26.2,27.9,23.3,27.8). Realizad un ANOVA de 2 vías sobre estos datos. Dad el p-valor del contraste de interacción entre el tratamiento y la característica redondeado a 3 cifras decimales (sin ceros innecesarios a la derecha) y decid si podéis concluir (con un SI) o no (con un NO), con un nivel de significación del 5%, que hay interacción entre el tratamiento y la característica. Dad el p-valor y la conclusión en este orden, separados por un único espacio en blanco. (4) En un experimento queremos comparar el valor medio de una cierta variable en cuatro poblaciones, A, B, C y D. Hemos tomado una muestra de 5 individuos de cada población y hemos medido en ellos la variable en cuestión. Los resultados para cada población han sido los siguientes: A=(23.1,17.2,20.1,19.5,18.8), B=(22.6,15.5,19.3,18.5,16.2), C=(21.1,26.4,24.7,21.4,20.9) y D=(20.4,24,23.4,24.9,21.5). Realizad un test de Bartlett de homocedasticidad sobre estos datos, para saber si las varianzas poblacionales de las cuatro poblaciones son todas iguales o no. Tenéis que dar el p-valor del contraste de la igualdad de las varianzas redondeado a 3 cifras decimales (sin ceros innecesarios a la derecha) y decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%, la igualdad de todas las varianzas. Dad el p-valor y la conclusión en este orden, separados por un único espacio en blanco. (5) En un experimento hemos comparado el valor medio de una cierta variable en cuatro poblaciones, A, B, C y D. Hemos tomado una muestra de 5 individuos de cada población y hemos medido en ellos la variable en cuestión. Los resultados para cada población han sido los siguientes: A=(23.1,17.2,20.1,19.5,18.8), B=(22.6,15.5,19.3,18.5,16.2), C=(21.1,26.4,24.7,21.4,20.9) y D=(20.4,24,23.4,24.9,21.5). Realizad un test t por parejas de Bonferroni sobre estos datos con un nivel de significación del 5%, para determinar qué pares de poblaciones podemos concluir que tienen medias diferentes con este nivel de significación. Tenéis que decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%, la igualdad de las medias de las poblaciones B y C. 9.9.1 Ejercicio (1) La tabla de datos butterfat del paquete faraway contiene los porcentajes de contenido de grasa en la leche en muestras aleatorias de 5 razas. Para cada raza hay 20 vacas en la muestra: 10 vacas de dos años, 2year, y 10 de más de 4 años, Mature. Las variables de la tabla de datos son: Butterfat: Porcentaje de contenido de grasa en la leche. Breed: un factor que indica la raza de la vaca. Los niveles son: Ayrshire, Canadian, Guernsey, Holstein-Fresian y Jersey. Age: una variable factor con los valores ya comentados: 2year y Mature. Queremos usar estos datos para contrastar si el porcentaje medio de contenido de grasa en la leche depende de la raza o de la edad de la vaca. (a) Dibujad sendos gráficos con los diagrama de cajas de los porcentajes de contenido de grasa en la leche de las vacas de la muestra separados por la raza de la vaca y separados por su edad, respectivamente. Comentadlos. (b) Contrastad si cada muestra de una combinación de niveles (raza,edad) proviene de una distribución normal usando el test de Shapiro-Wilks con un nivel de significación \\(\\alpha =0.05\\). (c) Contrastad si todas las muestras de combinaciones de niveles (raza,edad) tienen la misma varianza, con un nivel de significación \\(\\alpha =0.05\\). (d) Se satisfacen las hipótesis para poder usar un ANOVA de 2 vías para comprobar si el porcentaje medio de grasa en la leche depende de la raza o de la edad de la vaca? (e) Sea cual sea vuestra respuesta a la pregunta anterior, realizad un test ANOVA de 2 vías para contrastar con un nivel de significación \\(\\alpha =0.05\\) si el porcentaje medio de grasa en la leche depende de la raza o de la edad de la vaca (no hace falta contrastar las combinaciones (raza,edad)). Hay interacción entre la raza de la vaca y la edad de la misma? (f) Sea cual sea vuestra respuesta a la pregunta (d), realizad también los contrastes de (e) mediante un test no paramétrico adecuado con un nivel de significación \\(\\alpha =0.05\\). Coinciden las conclusiones? Respuestas al test (1) 0.013 SI Fijaos en que en el enunciado nos dan la tabla de datos siguiente por columnas: A B C D 23.1 22.6 21.1 20.4 17.2 15.5 26.4 24.0 20.1 19.3 24.7 23.4 19.5 18.5 21.4 24.9 18.8 16.2 21.9 21.5 Por lo tanto para entrarla de manera correcta como un data frame copiando los datos del enunciado hay que usar: x=c(23.1,17.2,20.1,19.5,18.8, 22.6,15.5,19.3,18.5,16.2, 21.1,26.4,24.7,21.4,21.9, 20.4,24.0,23.4,24.9,21.5) pobl=rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),each=5) X=data.frame(x,pobl) A partir de aquí, la respuesta la obtenemos con summary(aov(x~pobl,data=X))[[1]]$&quot;Pr(&gt;F)&quot; ## [1] 0.01257876 NA (2) 0.026 NO Nosotros lo hemos resuelto con x=c(23.1,17.2,20.1,19.5,18.8, 22.6,15.5,19.3,18.5,16.2, 21.1,26.4,24.7,21.4,21.9, 20.4,24.0,23.4,24.9,21.5) pobl=rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), each=5) bloques=as.factor(rep(1:5,4)) X.b=data.frame(x,pobl,bloques) summary(aov(x~pobl+bloques,data=X.b))[[1]]$&quot;Pr(&gt;F)&quot;[1] ## [1] 0.02561791 (3) 0.0234 SI Nosotros lo hemos resuelto con AX=c(29.2,30.9,31.3,30.0,30.0) BX=c(29.1,31.3,31.4,34.3,32.3) CX=c(31.3,40.7,32.6,34.8,38.3) DX=c(32.2,36.0,31.6,31.8,32.6) AY=c(26.3,25.9,25.6,25.3,24.9) BY=c(30.7,29.8,27.9,28.8,28.1) CY=c(26.2,26.7,27.2,29.3,28.4) DY=c(20.5,26.2,27.9,23.3,27.8) y=c(AX,BX,CX,DX,AY,BY,CY,DY) fact.A=rep(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),each=5),2) fact.X=rep(c(&quot;X&quot;,&quot;Y&quot;),each=20) DF=data.frame(y,fact.A,fact.X) summary(aov(y~fact.A*fact.X)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## fact.A 3 74.7 24.9 5.487 0.00371 ## fact.X 1 330.1 330.1 72.700 9.62e-10 ## fact.A:fact.X 3 49.3 16.4 3.619 0.02344 ## Residuals 32 145.3 4.5 (4) 0.881 NO Nosotros lo hemos resuelto con bartlett.test(x~pobl,data=X) ## ## Bartlett test of homogeneity of variances ## ## data: x by pobl ## Bartlett&#39;s K-squared = 0.6667, df = 3, p-value = 0.881 (5) SI Nosotros lo hemos resuelto con pairwise.t.test(X$x,X$pobl,p.adjust.method=&quot;holm&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: X$x and X$pobl ## ## A B C ## B 0.762 - - ## C 0.143 0.034 - ## D 0.151 0.041 0.861 ## ## P value adjustment method: holm 9.9.2 Soluciones sucintas del ejercicio library(faraway) BF=butterfat str(BF) ## &#39;data.frame&#39;:\t100 obs. of 3 variables: ## $ Butterfat: num 3.74 4.01 3.77 3.78 4.1 4.06 4.27 3.94 4.11 4.25 ... ## $ Breed : Factor w/ 5 levels &quot;Ayrshire&quot;,&quot;Canadian&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Age : Factor w/ 2 levels &quot;2year&quot;,&quot;Mature&quot;: 2 1 2 1 2 1 2 1 2 1 ... (a) boxplot(Butterfat~Breed, data=BF) boxplot(Butterfat~Age, data=BF) (b) Test.SW=function(x){shapiro.test(x)$p.value} aggregate(Butterfat~Breed:Age, data=BF,FUN=Test.SW) ## Breed Age Butterfat ## 1 Ayrshire 2year 0.99921213 ## 2 Canadian 2year 0.20778457 ## 3 Guernsey 2year 0.95640414 ## 4 Holstein-Fresian 2year 0.16320779 ## 5 Jersey 2year 0.69510564 ## 6 Ayrshire Mature 0.21757333 ## 7 Canadian Mature 0.73627123 ## 8 Guernsey Mature 0.33518154 ## 9 Holstein-Fresian Mature 0.05739251 ## 10 Jersey Mature 0.10347415 Podemos concluir con un nivel de significación del 5% que todas las muestras provienen de variables normales. (c) bartlett.test(Butterfat~interaction(Breed,Age), data=BF) ## ## Bartlett test of homogeneity of variances ## ## data: Butterfat by interaction(Breed, Age) ## Bartlett&#39;s K-squared = 24.356, df = 9, p-value = 0.003772 No podemos aceptar que se dé homocedasticidad. (d) No. (e) summary(aov(Butterfat~Breed*Age,data=BF)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Breed 4 34.32 8.580 49.565 &lt;2e-16 *** ## Age 1 0.27 0.274 1.580 0.212 ## Breed:Age 4 0.51 0.128 0.742 0.566 ## Residuals 90 15.58 0.173 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (f) library(WRS2) pbad2way(Butterfat~Breed*Age,data=BF, est=&quot;median&quot;, nboot=500) ## Error in solve.default(cov, ...): system is computationally singular: reciprocal condition number = 4.97982e-19 ¡Ups! pbad2way(Butterfat~Breed*Age,data=BF, est=&quot;median&quot;, nboot=500,pro.dis=TRUE) ## Call: ## pbad2way(formula = Butterfat ~ Breed * Age, data = BF, est = &quot;median&quot;, ## nboot = 500, pro.dis = TRUE) ## ## p.value ## Breed 0.00 ## Age 0.28 ## Breed:Age 0.83 "],
["chap-regresion.html", "Lección 10 Regresión lineal 10.1 El modelo de regresión lineal en R 10.2 Intervalos de confianza en el modelo de regresión lineal 10.3 Selección del modelo en base al ajuste de los datos 10.4 Diagnósticos de regresión 10.5 Guía rápida 10.6 Ejercicios", " Lección 10 Regresión lineal En esta lección, explicamos el uso de R para realizar regresiones lineales, tanto simples como múltiples. Presentaremos algunos ejemplos para ilustrar el uso de las funciones de R específicas para esta técnica de modelado estadístico, así como la posterior validación y adecuación del modelo mediante el análisis de los residuos. Aunque en la Lección 3 de la primera parte del curso ya introdujimos la regresión lineal sin entrar en mucha profundidad, es en esta lección donde desarrollaremos más ámpliamente este tema. 10.1 El modelo de regresión lineal en R Uno de los problemas recurrentes en estadística es determinar a partir de un conjunto de observaciones de variables si existe alguna relación funcional entre una de las variables, llamada variable dependiente o de respuesta, y el resto de variables, conocidas como variables independientes o de control. Los objetivos de encontrar esta relación funcional son, por un lado, entender cómo los valores de la variable dependiente “dependen” de los de las variables independientes y, por otro lado, poder estimar el valor de la variable dependiente sobre un sujeto para el que conozcamos sus valores de las variables independientes. En esta lección nos centramos en el caso en que esta relación funcional es lineal. Formalmente, la situación es la siguiente. Sean \\(X_1\\), …, \\(X_k\\) k variables (no necesariamente aleatorias), que serán las independientes, y sea \\(Y\\) la variable dependiente. Llamemos \\(Y|{x_1,\\ldots, x_k}\\) a la variable aleatoria que nos da los valores de \\(Y\\) sobre los individuos en los que cada variable independiente \\(X_i\\) toma el correspondiente valor \\(x_i\\), y sea \\(\\mu_{Y|{x_1,\\ldots, x_k}}\\) el valor esperado de \\(Y|{x_1,\\ldots, x_k}\\). Vamos a suponer que \\(\\mu_{Y|{x_1,\\ldots, x_k}}\\) es una función lineal de \\(X_1\\), …, \\(X_k\\), y que por lo tanto “en la realidad” existen unos coeficientes reales \\(\\beta_0\\), \\(\\beta_1\\), …, \\(\\beta_k\\), que desconocemos, tales que, para cada posible vector de valores \\((x_1,\\ldots,x_k)\\) de las variables independientes \\(X_1\\), …,\\(X_k\\), se tiene que \\[ \\mu_{Y|{x_1,\\ldots, x_k}}=\\beta_0+\\beta_1x_1+\\cdots+\\beta_kx_k. \\] De esta manera, si denotamos por \\(E_{x_1,\\ldots,x_k}\\) la variable error \\[ Y|{x_1,\\ldots, x_k}-\\mu_{Y|{x_1,\\ldots, x_k}} \\] que, para cada individuo cuyas variables \\(X_i\\) valen \\(x_i\\), nos da la diferencia entre su valor de \\(Y\\) y el valor esperado de esta variable para los individuos con sus mismos valores de \\(X_1\\), …, \\(X_k\\), tenemos que \\[ Y|{x_1,\\ldots, x_k}=\\beta_0+\\beta_1x_1+\\cdots+\\beta_kx_k+E_{x_1,\\ldots,x_k}. \\] Esta ecuación nos dice que el valor de \\(Y\\) sobre un sujeto para el que \\(X_1=x_1\\), …, \\(X_k=x_k\\), viene dado por dos componentes: por un lado, una componente “fija” definida por el modelo lineal \\(\\mu_{Y|{x_1,\\ldots, x_k}}=\\beta_0+\\beta_1x_1+\\cdots+\\beta_kx_k\\), y por otro lado, el error aleatorio \\(E_{x_1,\\ldots,x_k}\\). Por la linealidad de las esperanzas, como el valor esperado de \\(Y|{x_1,\\ldots, x_k}\\) es \\(\\beta_0+\\beta_1x_1+\\cdots+\\beta_kx_k\\), el valor esperado del error \\(E_{x_1,\\ldots,x_k}\\) es 0. Si disponemos entonces de un conjunto de n datos \\[ (x_{i1},x_{i2},\\ldots,x_{ik},y)_{i=1,\\ldots,n} \\] donde cada vector \\((x_{i1},x_{i2},\\ldots,x_{ik},y)\\) está formado por los valores de las variables \\(X_1\\), …, \\(X_{k}\\) e \\(Y\\) sobre un individuo, podemos usar estos datos para estimar los valores poblacionales de \\(\\beta_0\\), \\(\\beta_1\\), …, \\(\\beta_k\\). Vamos a suponer que \\(n&gt;k\\), por lo que no podemos esperar encontrar los valores de \\(\\beta_0\\), \\(\\beta_1\\), …, \\(\\beta_k\\) simplemente resolviendo un sistema de ecuaciones. Sean \\(b_0\\), \\(b_1\\), …, \\(b_k\\) nuestras estimaciones de \\(\\beta_0\\), \\(\\beta_1\\), …, \\(\\beta_k\\), respectivamente, a partir de nuestro conjunto de datos. Podemos escribir entonces la función lineal de regresión \\[ \\widehat{y}=b_0+b_1x_{1}+\\cdots+b_kx_{k} \\] que nos permite estimar por medio de \\(\\widehat{y}\\) el valor de \\(Y\\) que esperamos que tenga un sujeto para el que las variables \\(X_1\\), …, \\(X_{k}\\) valgan, respectivamente, \\(x_1\\), …, \\(x_{k}\\). En particular, para cada \\(i=1,\\ldots,n\\), llamaremos \\(\\widehat{y}_i\\) al valor de \\(Y\\) que estimamos sobre el sujeto \\(i\\)-ésimo de nuestro conjunto de datos, en el que las variables \\(X_1\\), …, \\(X_{k}\\) valen, respectivamente, \\(x_{i1}\\), …, \\(x_{ik}\\): \\[ \\widehat{y}_i=b_0+b_1x_{i1}+\\cdots+b_kx_{ik}. \\] Para cada \\(i=1,\\ldots,n\\), denotaremos por \\(e_i\\) el error cometido con la estimación sobre el individuo i-ésimo de nuestro conjunto de datos, definido como la diferencia \\[ y_i-\\widehat{y}_i=y_i-(b_0+b_1x_{i1}+\\cdots+b_kx_{ik}) \\] entre el valor \\(y_i\\) de la variable \\(Y\\) observado sobre dicho sujeto i-ésimo y nuestra estimación \\(\\widehat{y}_i\\) de este valor por medio de la función lineal que hemos encontrado. La mayoría de las estrategias usadas para calcular las estimaciones \\(b_0\\), \\(b_1\\), …, \\(b_k\\) se basan en preestablecer un cierto criterio de “bondad” de la estimación de cada \\(y_i\\) por medio del correspondiente \\(\\widehat{y}_i\\) y encontrar entonces los coeficientes que dan las “mejores” estimaciones según este criterio. El criterio más utilizado en este sentido es el de mínimos cuadrados, en el que se minimiza la suma de los cuadrados de los errores cometidos sobre los sujetos de nuestro conjunto de datos. Es decir, en el método de mínimos cuadrados se toman como estimaciones de \\(\\beta_0\\), \\(\\beta_1\\), …, \\(\\beta_k\\) los valores \\(b_0\\), \\(b_1\\), …, \\(b_k\\) para los que la suma \\[ SS_e=\\sum_{i=1}^n e_i^2=\\sum_{i=1}^n(y_i-b_0-b_1x_{i1}-\\cdots-b_kx_{ik})^2 \\] toma su valor mínimo: de ahí la coletilla de mínimos cuadrados. Como es el único método que vamos a explicar, a partir de ahora siempre que hablemos de regresión lineal nos referiremos a esta regresión lineal por el método de mínimos cuadrados. Además, hablaremos de regresión lineal simple si \\(k=1\\) (es decir, cuando usamos una única variable independiente) y de regresión lineal múltiple si \\(k&gt;1\\) (es decir, cuando usamos más de una variable independiente). La función básica de R para realizar una regresión lineal es lm. Su sintaxis básica es lm(formula,data=..., subset=...) donde: formula refiere a una fórmula que relaciona la variable respuesta y las variables independientes del modelo, en el sentido de las fórmulas introducidas en la Sección 9.1. Su estructura en este caso ha de ser la siguiente. En primer lugar, se escribe la variable dependiente, que necesariamente tiene que ser una variable numérica. A continuación, el símbolo ~ que indica la dependencia de esta variable respecto de las variables que se indiquen a su derecha. Finalmente, se incluyen las variables independientes separadas por símbolos +. data es un parámetro opcional que sirve para especificar, si es necesario, el data frame al que pertenecen las variables utilizadas en la fórmula. subset es otro parámetro opcional que sirve para especificar que la regresión sólo tenga en cuenta un subconjunto de las observaciones, definido mediante alguna condición lógica. Así, por ejemplo, si tenemos un data frame llamado DF, con una variable numérica Y y tres variables independientes X1, X2 y X3, para realizar la regresión lineal de la variable Y respecto de X1, X2 y X3 se puede ejecutar lm(Y~X1+X2+X3,data=DF) o lm(DF$Y~DF$X1+DF$X2+DF$X3) Un atajo muy útil en la ejecución de lm cuando nuestro data frame DF tiene muchas variables y queremos obtener la funció de regresión lineal de una variable, Y, respecto de todas las otras es entrar lm(Y~.,data=DF) El punto a la derecha de la ~ es una abreviatura de “la suma de todas las variables de DF diferentes de Y”. El siguiente ejemplo ilustra el uso de la función lm y de la salida que proporciona. Ejemplo 10.1 La tabla de datos Davis del paquete car contiene datos del peso y la altura de 200 hombres y mujeres que realizan ejercicio habitualmente obtenidos de dos formas distintas: los valores medidos con los instrumentos adecuados (báscula y escaliómetro, respectivamente) y los valores que notificaron estas personas antes de realizarse las mediciones.8 Veamos su estructura. library(car) data(Davis) str(Davis) ## &#39;data.frame&#39;:\t200 obs. of 5 variables: ## $ sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 1 1 2 1 2 2 2 2 2 ... ## $ weight: int 77 58 53 68 59 76 76 69 71 65 ... ## $ height: int 182 161 161 177 157 170 167 186 178 171 ... ## $ repwt : int 77 51 54 70 59 76 77 73 71 64 ... ## $ repht : int 180 159 158 175 155 165 165 180 175 170 ... En la Ayuda del objeto nos enteramos de que el factor sex indica el sexo del individuo, con niveles “F” y “M”; las variables cuantitativas weight y height indican, respectivamente, el peso (en kg.) y la altura (en cm.) medidas; y las variables cuantitativas repwt y repht indican, respectivamente, el peso (en kg.) y la altura (en cm.) notificadas por el individuo. Además, en la Descripción se nos avisa de que hay valores que faltan. En este primer ejemplo, nos centramos en las variables weight y repwt y vamos a suponer que el peso esperado de una persona que realiza ejercicio habitualmente (nuestra población de interés) es función lineal del peso que dice que tiene, y vamos a estimar los coeficientes de esta función lineal. De esta manera, cuando una persona de la población de interés nos diga su peso, podremos estimar su peso real. Lo primero que haremos será extraer un data frame formado por estas dos columnas, a las que renombraremos pesosy pesosnotif, respectivamente. Además, por si estas variables tienen valores NA, aplicaremos al data frame la función na.omit para eliminar filas que no correspondan a observaciones completas. datospeso=data.frame(Davis$weight,Davis$repwt) colnames(datospeso)=c(&quot;peso&quot;,&quot;pesonotif&quot;) head(datospeso) ## peso pesonotif ## 1 77 77 ## 2 58 51 ## 3 53 54 ## 4 68 70 ## 5 59 59 ## 6 76 76 datospeso=na.omit(datospeso) dim(datospeso) ## [1] 183 2 Han sobrevivido 183 de las 200 filas originales. Para calcular la recta de regresión por mínimos cuadrados de la variable peso respecto de la variable pesonotif, usaremos la función lm. lm(peso~pesonotif,data=datospeso) ## ## Call: ## lm(formula = peso ~ pesonotif, data = datospeso) ## ## Coefficients: ## (Intercept) pesonotif ## 5.3363 0.9278 El resultado obtenido significa que la recta de regresión buscada tiene término independiente (Intercept, el valor en el que la recta resultante corta el eje de ordenadas) \\(b_0=5.3363\\) y coeficiente de la variable peso \\(b_1=0.9278\\). Es decir, tenemos \\[ \\widehat{\\hbox{peso}}=5.3363+0.9278\\cdot \\hbox{pesonotif}. \\] Vamos a representar gráficamente los puntos de la muestra conjuntamente con la recta de regresión encontrada: plot(datospeso$pesonotif, datospeso$peso, pch=20, xlab=&quot;peso notificado&quot;, ylab=&quot;peso medido&quot;) abline(lm(peso~pesonotif,data=datospeso), col=&quot;red&quot;) Figura 10.1: Gráfico de los pares de observaciones (pesonotif,peso) junto con la correspondiente recta de regresión. Como se puede observar, la recta de regresión se ajusta notablemente a los puntos de la muestra indicando un buen comportamiento del modelo a nivel visual. Sin embargo, destaca una de las observaciones que se encuentra muy alejada tanto del patrón seguido por el resto de puntos como de la recta de regresión. Para obtener a qué observación corresponde ese punto, podríamos utilizar la función identify de R. Si inmediatamente después de generar el gráfico anterior ejecutáis identify(datospeso$pesonotif,datospeso$peso) al pulsar con el cursor sobre un punto del gráfico podéis saber sus coordenadas. Como en este documento esto es imposible, lo que haremos será determinar ese punto anómalo como el que tiene ordenada mayor que 160: which(datospeso$peso&gt;160) ## [1] 12 datospeso[which(datospeso$peso&gt;160),] ## peso pesonotif ## 12 166 56 El punto anómalo, correspondiente al individuo 12 de la muestra, ¡dijo que pesaba 56 kg y en realidad pesaba 166! Más adelante, en la Sección 10.4, trataremos el procedimiento a seguir en estas situaciones. Veamos un segundo ejemplo. Ejemplo 10.2 El fichero de datos “USA2012.txt” que podéis descargar desde el enlace https://raw.githubusercontent.com/AprendeR-UIB/Material/master/USA2012.txt contiene datos demográficos, sociales y económicos de los 50 estados de los Estados Unidos más el distrito de Columbia correspondientes al año 2012, el año que Barack Obama ganó sus segundas elecciones presidenciales. Estos datos han sido recopilados de diversas fuentes como el United States Census Bureau, el Pew Research Center o el Bureau of Labor Statistics. Vamos a cargarlo en un data frame y consultar su estructura. library(RCurl) datos=getURL(&quot;https://raw.githubusercontent.com/AprendeR-UIB/Material/master/USA2012.txt&quot;) USA=read.table(text=datos,header=TRUE) str(USA) ## &#39;data.frame&#39;:\t51 obs. of 21 variables: ## $ estado : Factor w/ 51 levels &quot;Alabama&quot;,&quot;Alaska&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ region : int 3 4 4 3 4 4 1 3 3 3 ... ## $ densidad : num 94.4 1.2 56.3 56 239.1 ... ## $ veteranos : num 8.1 10.1 8.2 7.9 5 8.1 6 8.5 5.2 8.3 ... ## $ mujeres : num 51.5 48 50.3 50.9 50.3 49.9 51.3 51.6 52.8 51.1 ... ## $ grad_instituto: num 82.6 91.6 85.4 83.3 81 89.9 89 87.7 87.5 85.8 ... ## $ grad_univ : num 22.3 27.5 26.6 19.8 30.5 36.7 36.2 28.5 51.2 26.2 ... ## $ afro : num 26.2 3.3 4.1 15.4 6.2 4 10.1 21.4 50.7 16 ... ## $ asia : num 1.1 5.4 2.8 1.2 13 2.8 3.8 3.2 3.5 2.4 ... ## $ hispanos : num 3.9 5.5 29.6 6.4 37.6 20.7 13.4 8.2 9.1 22.5 ... ## $ blancos : num 67 64.1 57.8 74.5 40.1 70 71.2 65.3 34.8 57.9 ... ## $ evangelicos : int 49 26 23 53 18 23 9 15 15 25 ... ## $ protestantes : int 1 19 15 16 14 19 13 18 20 15 ... ## $ relig_afro : int 18 2 2 10 4 2 4 14 18 8 ... ## $ catolicos : int 6 14 25 5 31 19 43 27 18 26 ... ## $ mormones : int 1 4 4 0 2 2 1 0 0 0 ... ## $ jubilados : num 13.8 7.7 13.8 14.4 11.4 10.9 14.2 14.4 11.4 17.3 ... ## $ paro : num 8 7.6 8.4 7.6 10.4 7.8 8.3 7.2 9 8.5 ... ## $ salario : int 43464 63648 47044 39018 57020 57255 64247 48972 65246 46071 ... ## $ obama : num 38.4 40.8 44.6 36.9 60.2 ... ## $ diputados : int 9 3 11 6 55 9 7 3 3 29 ... Calculemos, por ejemplo, la recta de regresión del porcentaje de voto a Obama (variable obama) con respecto al porcentaje de graduados universitarios del estado (variable grad_univ): lm(obama~grad_univ,data=USA) ## ## Call: ## lm(formula = obama ~ grad_univ, data = USA) ## ## Coefficients: ## (Intercept) grad_univ ## 7.812 1.460 Se obtiene la recta \\[ \\widehat{\\hbox{obama}}=7.812+1.46\\cdot \\hbox{grad_univ}. \\] Como podemos observar, a mayor porcentaje de graduados universitarios, el modelo nos predice un mayor porcentaje de voto a Obama. Añadamos ahora al modelo la variable veteranos que indica el porcentaje de veteranos de guerra en la población del estado. lm(obama~grad_univ+veteranos,data=USA) ## ## Call: ## lm(formula = obama ~ grad_univ + veteranos, data = USA) ## ## Coefficients: ## (Intercept) grad_univ veteranos ## 13.7318 1.4104 -0.5981 En este caso, la ecuación de regresión es \\[ \\widehat{\\hbox{obama}}=13.732+1.41\\cdot \\hbox{grad_univ}-0.598\\cdot \\hbox{veteranos}. \\] Así, mientras que un mayor porcentaje de graduados universitarios sigue correspondiéndose a un mayor porcentaje de voto a Obama, un mayor porcentaje de veteranos en la población lleva asociada una disminución de este porcentaje de voto. Ya hemos visto que los coeficientes de la recta de regresión se pueden obtener simplemente ejecutando la función lm. Sin embargo, esta función nos puede proporcionar mucha información adicional, aplicando summary al resultado de lm. Volvamos al Ejemplo 10.1. summary(lm(peso~pesonotif,data=datospeso)) ## ## Call: ## lm(formula = peso ~ pesonotif, data = datospeso) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.048 -1.868 -0.728 0.601 108.705 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.3363 3.0369 1.757 0.0806 . ## pesonotif 0.9278 0.0453 20.484 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.419 on 181 degrees of freedom ## Multiple R-squared: 0.6986,\tAdjusted R-squared: 0.697 ## F-statistic: 419.6 on 1 and 181 DF, p-value: &lt; 2.2e-16 En esta salida encontramos la siguiente información: En Residuals, se proporciona un resumen descriptivo de los residuos o errores \\(e_i\\) del modelo, concretamente, sus valores mínimo y máximo y sus cuartiles. En la tabla de Coefficients, en la columna Estimate se dan las estimaciones de los coeficientes de cada variable de la función de regresión, junto a sus respectivos errores típicos en la columna Std. Error. A continuación, las columnas t value y Pr(&gt;|t|) proporcionan para cada coeficiente \\(\\beta_i\\) el valor del estadístico y el p-valor del contraste \\[ \\left\\{\\begin{array}{ll} H_0:&amp; \\beta_i=0\\\\ H_1:&amp; \\beta_i\\neq 0 \\end{array}\\right. \\] R indica el significado estadístico de cada p-valor a su derecha mediante el código usual de estrellas. Después de la tabla, encontramos el Residual standard error que corresponde a la raíz del valor estimado de la varianza común de los residuos \\(S=\\sqrt{\\frac{SS_E}{n-k-1}}\\), junto con los grados de libertad \\(n-k-1\\). En la siguiente fila, tenemos los valores Multiple R-squared y Adjusted R-squared, es decir, los coeficientes de determinación \\(R^2\\) y de determinación ajustado \\(R^2_{adj}\\), respectivamente. Recordemos que \\[ R^2=\\dfrac{s_{\\widehat{y}}^2}{s_{y}^2}, \\quad R^2_{adj}=\\frac{(n-1)R^2-k}{n-k-1} \\] donde \\(s_{y}^2\\) y \\(s_{\\widehat{y}}^2\\) denotan la varianza de los valores de \\(y\\) en nuestra muestra y de los valores de \\(y\\) que predice nuestro modelo sobre los puntos de la muestra, respectivamente. Cuánto mejor aproxime la función de regresión el conjunto de puntos, más cercanos a 1 serán los valores de estos índices ya que representan la fracción de variabilidad de \\(y\\) explicada por el modelo de regresión lineal. En la última fila, aparecen el valor del estadístico \\(F\\), los grados de libertad, 1 y n-k-1, y el p-valor, en este orden, del siguiente contraste ANOVA: \\[ \\left\\{\\begin{array}{ll} H_0:&amp;\\beta_1=\\cdots=\\beta_k=0\\\\ H_1:&amp; \\hbox{existe al menos un } \\beta_i\\neq 0 \\end{array}\\right. \\] En la regresión lineal simple, este contraste es equivalente al contraste para \\(\\beta_1\\) dado en la tabla Coefficients. En este ejemplo concreto, hay que destacar que el valor de \\(R^2\\) es 0.6986, un valor relativamente bajo. También podemos inferir que \\(\\beta_1\\neq 0\\) ya que el p-valor correspondiente al contraste para la variable pesonotif es pequeño. Observad que si \\(\\beta_1\\) fuera 0, los valores de la variable peso no dependerían de los de la variable pesonotif y el modelo carecería de sentido. Por lo tanto, hemos obtenido evidencia de que el modelo lineal puede ser válido. La información proporcionada por summary(lm( )) se puede extraer individualmente ya que se trata de una list. Veamos algunos ejemplos. El coeficiente de determinación se obtiene con el sufijo $r.squared: summary(lm(peso~pesonotif,data=datospeso))$r.squared ## [1] 0.6986308 El coeficiente de determinación ajustado se obtiene con el sufijo $adj.r.squared: summary(lm(peso~pesonotif,data=datospeso))$adj.r.squared ## [1] 0.6969658 La tabla de coeficientes se obtiene con el sufijo $coefficients: summary(lm(peso~pesonotif,data=datospeso))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.3362605 3.03690979 1.757135 8.058558e-02 ## pesonotif 0.9278428 0.04529609 20.483950 5.102807e-49 Las estimaciones de los coeficientes forman la primera columna de la tabla anterior, y por lo tanto se obtienen con el sufijo $coefficients[,1]: summary(lm(peso~pesonotif,data=datospeso))$coefficients[,1] ## (Intercept) pesonotif ## 5.3362605 0.9278428 Los residuos se obtienen con el sufijo $residuals: residuos=summary(lm(peso~pesonotif,data=datospeso))$residuals head(residuos) ## 1 2 3 4 5 6 ## 0.2198430 5.3437561 -2.4397724 -2.2852573 -1.0789864 0.1476858 Los residuos también se obtienen a partir del resultado de lm con el mismo sufijo $residuals: recta_regresion=lm(peso~pesonotif,data=datospeso) Residuos=recta_regresion$residuals head(Residuos) ## 1 2 3 4 5 6 ## 0.2198430 5.3437561 -2.4397724 -2.2852573 -1.0789864 0.1476858 Los valores \\(\\widehat{y}_i\\) de la variable dependiente predichos por el modelo sobre los sujetos de la muestra se obtienen añadiendo al resultado de lm el sufijo $fitted.values: estimados=recta_regresion$fitted.values head(estimados) ## 1 2 3 4 5 6 ## 76.78016 52.65624 55.43977 70.28526 60.07899 75.85231 Recordemos que en este caso, el valor del coeficiente de determinación \\(R^2\\) no ha sido muy alto. Sin embargo, hemos observado que visualmente la gran mayoría de observaciones se ajustan a la recta de regresión salvo la observación anómala 12. A continuación, calcularemos la recta de regresión sin tener en cuenta esta observación mediante el uso del parámetro subset de la función lm. summary(lm(peso~pesonotif,data=datospeso,subset=-12)) ## ## Call: ## lm(formula = peso ~ pesonotif, data = datospeso, subset = -12) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.5296 -1.1010 -0.1322 1.1287 6.3891 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.73380 0.81479 3.355 0.000967 *** ## pesonotif 0.95837 0.01214 78.926 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.254 on 180 degrees of freedom ## Multiple R-squared: 0.9719,\tAdjusted R-squared: 0.9718 ## F-statistic: 6229 on 1 and 180 DF, p-value: &lt; 2.2e-16 plot(datospeso$pesonotif, datospeso$peso, pch=20, xlab=&quot;peso notificado&quot;, ylab=&quot;peso medido&quot;) abline(lm(peso~pesonotif,data=datospeso), col=&quot;red&quot;) abline(lm(peso~pesonotif,data=datospeso,subset=-12), col=&quot;blue&quot;,lty=2) Figura 10.2: Gráfico de los pares de observaciones (pesonotif,peso) junto con las rectas de regresión teniendo en cuenta el valor anómalo (roja continua) y sin tenerlo en cuenta (azul discontinua). Excluyendo la observación 12 del cálculo de la recta de regresión hemos obtenido un coeficiente de determinación mucho mayor que el inicial. Por otro lado, en el gráfico vemos como la observación anómala “atrae” la recta de regresión disminuyendo su pendiente. Hemos utilizado la función lm para calcular la nueva recta de regresión. Otra posibilidad hubiera sido usar la función update, que permite recalcular la recta de regresión a partir de una recta de regresión anterior. Su sintaxis es la siguiente: update(x, formula., subset=...) donde x es un modelo de regresión lineal, es decir, la salida de una función lm. formula. es un parámetro opcional que indica un cambio en la formula especificada para obtener el nuevo modelo: si no cambiamos la fórmula, no hay que añadirlo. Es muy útil en regresión lineal múltiple para eliminar una de las variables consideradas. En este caso entraríamos como formula. la expresión .~.-X, donce X es la variable que queremos eliminar. Esta construcción indica que se ha de utilizar la misma formula que en el modelo x pero ahora sin tener en cuenta la variable independiente X. subset también es un parámetro opcional y tiene el mismo significado que en la función lm. Naturalmente, se tiene que especificar al menos uno de estos dos parámetros opcionales, o del contrario no estaremos modificando el modelo lineal x. La salida de la función update es similar a la de lm. Así, con la instrucción siguiente, se obtiene el coeficiente de determinación anterior: summary(update(recta_regresion,subset=-12))$r.squared ## [1] 0.9719157 Vamos a calcular a continuación los valores de \\(R^2\\) y \\(R^2_{adj}\\) de los modelos considerados en el Ejemplo 10.2. summary(lm(obama~grad_univ,data=USA))$r.squared ## [1] 0.5119194 summary(lm(obama~grad_univ,data=USA))$adj.r.squared ## [1] 0.5019585 summary(lm(obama~grad_univ+veteranos,data=USA))$r.squared ## [1] 0.5154579 summary(lm(obama~grad_univ+veteranos,data=USA))$adj.r.squared ## [1] 0.4952686 Sobre el uso de los valores \\(R^2\\) y \\(R^2_{adj}\\) hay varias consideraciones a realizar. En primer lugar, hay que tener en cuenta que el valor de \\(R^2\\) se mantendrá o crecerá si añadimos nuevas variables independientes al modelo. Esto es debido a que se plantea un modelo más general que incluye el anterior y por lo tanto, siempre se explicará como mínimo el mismo porcentaje de variabilidad de \\(y\\). Así se puede observar en este ejemplo, en el que el modelo obama~grad_univ+veteranos obtiene un valor mayor de \\(R^2\\) que obama~grad_univ. Teniendo en cuenta este hecho, y con el objetivo de conseguir un equilibrio entre la complejidad del modelo y el ajuste a los datos, se recomienda usar los valores de \\(R^2_{adj}\\) para comparar modelos de regresión lineal múltiple. Este coeficiente penaliza la adición de una nueva variable a no ser que esta adición suponga una mejora sustancial del ajuste del modelo a los datos. Así, como el modelo obama~grad_univ tiene un valor de \\(R^2_{adj}\\) mayor, se considera que es un modelo mejor al que contempla también la variable veteranos. Veamos finalmente qué ocurre si consideramos todas las variables sociales, demográficas y económicas para explicar el porcentaje de voto a Obama en un estado. Para simplificar nuestra tarea, definimos el data frame USA2 que contiene solo estas variables, eliminando de la tabla USA original las variables que dan el estado, la región y el número de diputados. USA2=USA[,-c(1,2,21)] summary(lm(obama~.,data=USA2))$adj.r.squared ## [1] 0.8278411 El modelo mejora de forma muy significativa. Hemos usado la fórmula obama~. para considerar el resto de variables de USA2 como variables independientes. 10.2 Intervalos de confianza en el modelo de regresión lineal Nuestro siguiente objetivo es calcular intervalos de confianza para los coeficientes \\(\\beta_i\\) del modelo lineal así como para el valor esperado \\(\\mu_{Y|{x_1 \\ldots x_k}}\\) y el valor estimado \\(y_0\\) de \\(Y\\) sobre un sujeto en el que \\((X_1,\\ldots,X_k)=(x_1,\\ldots,x_k)\\). La función de R que calcula intervalos de confianza para los coeficientes \\(\\beta_i\\) es confint. Su sintaxis básica es confint(objeto, parm, level=...) donde: objeto es el resultado de una función lm. parm permite indicar para qué parámetros se tienen que calcular los intervalos de confianza. Tanto se puede igualar a un vector de números (empezando a contar por el término independiente) como a un vector con los nombres de las variables independientes correspondientes a dichos parámetros. Por defecto se calculan los intervalos de confianza para todos los parámetros. level es el nivel de confianza. Por defecto, se calculan intervalos de confianza al 95%. Veamos una aplicación de la función confint. Vamos a calcular los intervalos de confianza del 95% para los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) del Ejemplo 10.1. Recordad que hemos llamado recta_regresion al resultado correspondiente de la función lm. confint(recta_regresion) ## 2.5 % 97.5 % ## (Intercept) -0.6560394 11.328560 ## pesonotif 0.8384665 1.017219 La salida de la función es una matriz cuyas filas son los extremos inferior y superior de los intervalos de confianza de los coeficientes deseados. Las filas se indican con el nombre de la variable dependiente correspondiente o con (Intercept) en el caso del término independiente, y las columnas se indican mediante el nivel del cuantil utilizado para el cálculo del intervalo. Así, el intervalo de confianza para \\(\\beta_0\\) es (-0.656, 11.329) y para \\(\\beta_1\\), (0.838, 1.017). Si solo queremos calcular el intervalo de confianza para \\(\\beta_1\\) podemos usar cualquiera de las tres instrucciones siguientes (aunque la tercera en realidad calculará todos los intervalos de confianza y solo nos mostrará el de \\(\\beta_1\\)): confint(recta_regresion, parm=2) ## 2.5 % 97.5 % ## pesonotif 0.8384665 1.017219 confint(recta_regresion, parm=&quot;pesonotif&quot;) ## 2.5 % 97.5 % ## pesonotif 0.8384665 1.017219 confint(recta_regresion)[2,] ## 2.5 % 97.5 % ## 0.8384665 1.0172191 Una de las utilidades básicas de los intervalos de confianza es poder contrastar si la variable correspondiente aporta información al modelo o no. Este hecho, además de venir determinado por el p-valor obtenido en la tabla de coeficientes en la salida de lm, también se puede decidir comprobando si 0 pertenece al intervalo de confianza para el coeficiente. En caso afirmativo, no se puede descartar que la \\(\\beta_i\\) correspondiente sea 0 y en consecuencia, podría ocurrir que la variable \\(X_i\\) no influyera en el modelo. En este ejemplo concreto, 0 no pertenece al intervalo de confianza del 95% para \\(\\beta_1\\), y por lo tanto podemos concluir (con un nivel de significación del 5%) que el peso de un individuo está relacionado con el peso que afirma que tiene. Fijaos en que 0 sí que pertenece al intervalo de confianza del 95% para \\(\\beta_0\\), por lo que con este nivel de significación no podemos rechazar que \\(\\beta_0=0\\), pero esto no afecta la conclusión anterior. Si hacemos lo mismo para la regresión lineal del voto de Obama en función de las variables grad_univ y veteranos, obtenemos: recta_reg=lm(obama~grad_univ+veteranos,data=USA) confint(recta_reg) ## 2.5 % 97.5 % ## (Intercept) -9.6169060 37.080465 ## grad_univ 0.9650792 1.855656 ## veteranos -2.6293525 1.433104 summary(recta_reg) ## ## Call: ## lm(formula = obama ~ grad_univ + veteranos, data = USA) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.0415 -5.0620 0.7336 6.1104 20.2152 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.7318 11.6126 1.182 0.243 ## grad_univ 1.4104 0.2215 6.368 6.85e-08 *** ## veteranos -0.5981 1.0102 -0.592 0.557 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.391 on 48 degrees of freedom ## Multiple R-squared: 0.5155,\tAdjusted R-squared: 0.4953 ## F-statistic: 25.53 on 2 and 48 DF, p-value: 2.805e-08 El intervalo de confianza para el coeficiente \\(\\beta_2\\), el correspondiente a la variable veteranos, contiene el 0 y por lo tanto, no se puede descartar que esta variable de hecho no influya en el modelo. Observad que el p-valor de esta variable en la tabla de coeficientes es muy grande, por lo que podemos llegar a esta conclusión a partir de este p-valor. Esto es coherente con la disminución del valor de \\(R^2_{adj}\\) cuando añadíamos la variable veteranos al modelo. El ajuste mejora pero no compensa el aumento de complejidad y de hecho, no está claro que esta variable influya en la variable de respuesta. Llegados a este punto, vamos a introducir la función predict que sirve para calcular intervalos de confianza para las estimaciones de la variable dependiente. La sintaxis de esta función es la siguiente: predict(objeto, newdata, interval=..., level=...) donde: objeto es una salida de la función lm. newdata es un data frame del que la función predict toma los valores de las variables independientes de los individuos para los que queremos predecir el valor de la variable dependiente. Por lo tanto, el data frame que entremos en este parámetro tiene que tener como columnas las variables independientes del modelo, y predict calculará un intervalo de confianza para cada una de sus filas. interval es un parámetro con tres posibles valores: \"none\", con el que simplemente se calcula el valor de la variable dependiente que predice la función lineal de regresión para cada individuo descrito en newdata. \"confidence\", con el que se calcula el intervalo de confianza para el valor esperado de la variable dependiente para cada individuo descrito en newdata. \"prediction\", con el que se calcula el intervalo de confianza para el valor de la variable dependiente predicho por la función de regresión para los individuos cada individuo descrito en newdata. level, como siempre, indica el nivel de confianza y por defecto vale 0.95. Volvamos a la recta de regresión encontrada en el Ejemplo 10.1. Vamos a calcular algunos intervalos de confianza para estimaciones de pesos a partir de valores notificados. Supongamos que tenemos dos individuos de nuestra población de interés, un dice que pesa 70 kg y el otro que pesa 100 kg. Entramos estos datos en un data frame de única variable pesonotif: individuos=data.frame(pesonotif=c(70,100)) Entonces: Estimamos que pesan round(predict(recta_regresion,individuos,interval=&quot;none&quot;),1) ## 1 2 ## 70.3 98.1 Los intervalos de confianza del 95% para lo que esperamos que pesen son: round(predict(recta_regresion,individuos,interval=&quot;confidence&quot;),1) ## fit lwr upr ## 1 70.3 69.0 71.6 ## 2 98.1 94.8 101.4 Los intervalos de confianza del 95% para lo que predecimos que pesan son: round(predict(recta_regresion,individuos,interval=&quot;prediction&quot;),1) ## fit lwr upr ## 1 70.3 53.6 86.9 ## 2 98.1 81.2 115.1 Como podéis ver, la salida de la función predict es una matriz cuyas filas corresponden a los individuos representados en el data frame que le entramos, en el mismo orden que las filas de ese data frame (o un vector, si el data frame tiene una sola fila) y tres columnas: fit: los valores predichos por la función de regresión. lwr y upr: los extremos inferior (lower) y superior (upper), respectivamente, de los intervalos de confianza que se han pedido. Así, por ejemplo, tenemos que si una persona dice que pesa 100 kg, entonces nuestra recta de regresión predice que pesará 98.1 kg con un intervalo de confianza para esta predicción que va de 81.2 a 115.1 kg. Además, tenemos una confianza del 95% en que, de media, los individuos que dicen que pesan 100 kg en realidad pesan entre 94.8 y 101.4 kg. Observad que los intervalos de confianza obtenidos con interval=\"prediction\" son más anchos que los obtenidos con interval=\"confidence\". Naturalmente, tenemos una mayor incertidumbre a la hora de predecir el peso de un individuo concreto que dice que pesa X que al predecir el peso medio de todos los individuos que dicen que pesan X, lo que se traduce en las anchuras de los intervalos de confianza. Veamos una aplicación real de estos intervalos de confianza. Vamos a suponer que el porcentaje de voto demócrata en las elecciones presidenciales del 2016 iba a seguir el mismo modelo que en las elecciones del 2012 y que podemos estimar el porcentaje de voto demócrata a partir de las variables socio-económicas y demográficas de los estados. Disponemos de los valores de las variables independientes para el estado de Virginia correspondientes al año 2015 (últimos valores publicados con anterioridad a las elecciones del 2016), y vamos a calcular los intervalos de confianza para el porcentaje de voto demócrata y para su valor esperado en el estado de Virginia: virginia2015=virginia2015=data.frame(densidad=81.4,veteranos=9.4,mujeres=50.8, grad_instituto=88.5,grad_univ=36.7,afro=19.7,asia=6.3,hispanos=8.9, blancos=63.1,evangelicos=30,protestantes=16,relig_afro=12,catolicos=12, mormones=2, jubilados=13.8,paro=4.3,salario=66155) predict(lm(obama~.,data=USA2),virginia2015,interval=&quot;prediction&quot;) ## fit lwr upr ## 1 49.87625 37.00499 62.74752 predict(lm(obama~.,data=USA2),virginia2015,interval=&quot;confidence&quot;) ## fit lwr upr ## 1 49.87625 41.73612 58.01638 El modelo predice que en 2016 el partido demócrata iba a obtener un 49.88% de los votos, con un intervalo de confianza del 95% del 37% al 62.75%. Además, un intervalo de confianza del 95% para el valor esperado de este porcentaje de votos (es decir, la media de los porcentajes de votos que obendría el partido demócrata en estados con los mismos datos socio-económicos y demográficos que Virginia en 2015 si repitiésemos muchísimas veces las elecciones) va del 41.74% al 58.02%. El porcentaje de voto demócrata en Virginia en las elecciones del 2016 fue del 49.73%, así que la predicción del modelo en este caso es bastante acertada. 10.3 Selección del modelo en base al ajuste de los datos En los modelos de regresión lineal múltiple, es decir, en aquellas situaciones en las que existen varias variables independientes candidatas a intervenir en el modelo de regresión lineal, surge la pregunta de qué modelo se debe seleccionar en base al ajuste de los datos. No existe una respuesta definitiva a esta cuestión debido a la complejidad del tema. Aunque un modelo sea efectivo a la hora de ajustar los datos y por lo tanto, tenga un valor grande de \\(R^2\\), puede ocurrir que algunas de las variables independientes seleccionadas no sean en realidad relevantes en el modelo. Son las conocidas como variables redundantes. Estas variables dificultan la interpretación del modelo y es conveniente eliminarlas. Sin embargo, surge la duda de determinar de forma objetiva cuál es el mejor modelo, en el sentido de cuál es el modelo más sencillo que explique la mayor cantidad de varianza posible. Así, llegamos a la cuestión de cómo comparar dos modelos de regresión múltiple con un número distinto de variables. Tenemos disponibles tres métodos para este fin: Comparación de los \\(R^2_{adj}\\): Ya hemos hablado de este método en la Sección 10.1. Recordemos que en base a este criterio, el modelo con un mayor \\(R^2_{adj}\\) es el óptimo. Las medidas AIC (Akaike’s Information Criterion) y BIC (Bayesian Information Criterion): La medida AIC, definida como \\[ AIC=n\\ln(SS_e/n)+2k, \\] donde \\(SS_e\\) indica la suma de los cuadrados de los errores, cuantifica cuánta información de la variable dependiente se pierde con el modelo y el número de variables utilizado. En cambio, la medida BIC, definida como \\[ BIC=n\\ln(SS_e/n)+k\\ln(n), \\] también tiene en cuenta el tamaño de la muestra al penalizar el número de variables independientes. Cuánto menor sea el valor de AIC o BIC, mejor se considera el modelo. Las medidas AIC y BIC se calculan con la función de R extractAIC(x,k)[2] donde x es un modelo lineal obtenido mediante la función lm y k es el peso que afecta al número de variables del modelo. El valor de k por defecto es 2 y corresponde al criterio AIC. Si se indica k=log(n), donde n es el número de observaciones, extractAIC calcula la medida BIC. Los tres criterios no son equivalentes y por lo tanto, pueden darse resultados contradictorios entre ellos. En ese caso, conviene escoger uno de los criterios y explicar la preferencia por él, o realizar más análisis para determinar qué modelo es mejor. Estudiemos a continuación los siguientes modelos de regresión lineal múltiple a partir de los datos del Ejemplo 10.2: modelo1=lm(obama~.,data=USA2) modelo2=update(modelo1,.~.-jubilados) modelo3=update(modelo1,.~.-paro) El modelo 1 considera todas las variables socio-económicas y demográficas de los estados norteamericanos, mientras que los modelos 2 y 3 no tienen en cuenta, respectivamente, las variables jubilados y paro. Veamos los valores de \\(R^2_{adj}\\), AIC y BIC para decidir cuál de estos tres modelos es el mejor. Organizaremos estos valores en un data frame para facilitar su comparación y usaremos la función kable del paquete knitr para que nos muestre el data frame en el documento final como una tabla bien formateada. R2.adj=c(summary(modelo1)$adj.r.squared, summary(modelo2)$adj.r.squared, summary(modelo3)$adj.r.squared) AIC=c(extractAIC(modelo1)[2], extractAIC(modelo2)[2], extractAIC(modelo3)[2]) BIC=c(extractAIC(modelo1,k=log(dim(USA2)[1]))[2], extractAIC(modelo2,k=log(dim(USA2)[1]))[2], extractAIC(modelo3,k=log(dim(USA2)[1]))[2]) Medidas=data.frame(R2.adj,AIC,BIC,row.names=c(&quot;modelo1&quot;,&quot;modelo2&quot;,&quot;modelo3&quot;)) names(Medidas)=c(&quot;Coef. Det. ajustado&quot;,&quot;AIC&quot;,&quot;BIC&quot;) knitr::kable(Medidas) Coef. Det. ajustado AIC BIC modelo1 0.8278411 175.9132 210.6861 modelo2 0.8328691 173.9240 206.7651 modelo3 0.7722827 189.6997 222.5408 En este caso, los tres indicadores coinciden en que el modelo que no tiene en cuenta la variable jubilados es el mejor de los tres: es el que tiene mayor valor de \\(R^2_{adj}\\) y menor valor de las medidas AIC y BIC. En un problema en el que se dispongan de k variables independientes, existen \\(2^{k}-1\\) modelos de regresión lineal múltiple que deberían ser evaluados para encontrar cuál es la mejor combinación de variables. El proceso manual es tedioso y el número de casos es demasiado elevado. Para ayudarnos en esta tarea, en R disponemos de la función step(x,direction=...,scope=...,k=..., trace=...) que, a partir de un modelo lineal dado x, va probando diferentes modelos añadiendo o eliminando variables independientes hasta encontrar un modelo óptimo (entre todos los modelos que se puedan obtener de esta manera). Sus otros argumentos son: direction indica la metodología que ha de utilizar R para generar los nuevos modelos a evaluar en la siguiente iteración. Tiene 3 valores posibles: \"backward\" indica que en cada iteración se han de evaluar y comparar el modelo obtenido en la iteración anterior y todos los modelos obtenidos a partir de él eliminando una de sus variables independientes; \"forward\" indica que en cada iteración se han de evaluar y comparar el modelo obtenido en la iteración anterior y todos los modelos obtenidos a partir de él añadiéndole una nueva variable independiente; \"both\" indica que en cada iteración se han de evaluar y comparar el modelo obtenido en la iteración anterior y todos los modelos obtenidos a partir de él añadiéndole una nueva variable independiente o eliminando una de sus variables independientes. scope define el rango de modelos que se tienen que considerar. Se introduce con el formato list(lower=formula1,upper=formula2) donde formula1 y formula2 son los modelos que constituyen los extremos del rango a considerar en el sentido de que los modelos que se tendrán en cuenta han de contener las variables independientes de la formula1 y sus variables independientes han de aparecer a la derecha de la tilde en la formula2. k tiene el mismo significado que en la función extractAIC e indica si la evaluación de los modelos se realiza en base al AIC (entrando k=2, el valor por defecto) o al BIC (entrando k=log(n) con n el el número de observaciones). trace es un parámetro lógico. Su valor por defecto es TRUE y va mostrando en la consola la información de cada iteración. Igualado a FALSE solo da el modelo final. Esta función realiza un proceso iterativo de eliminación o adición de variables consiguiendo en cada paso del algoritmo un modelo con un valor de AIC (o BIC) menor al modelo obtenido en el paso anterior. El proceso se para cuando no encuentra ningún modelo mejor. Ejecutemos esta función con el modelo que considera todas las variables socio-económicas y demográficas, usando el AIC para valorar los modelos y con direction=\"backward\", es decir, eliminando variables una a una. Omitimos la información en consola de algunas iteraciones intermedias (indicado en la salida con [...]) para ahorrar espacio vertical. step(modelo1,direction=&quot;backward&quot;) Start: AIC=175.91 obama ~ densidad + veteranos + mujeres + grad_instituto + grad_univ + afro + asia + hispanos + blancos + evangelicos + protestantes + relig_afro + catolicos + mormones + jubilados + paro + salario Df Sum of Sq RSS AIC - jubilados 1 0.168 792.69 173.92 - afro 1 1.209 793.73 173.99 - catolicos 1 9.637 802.16 174.53 - relig_afro 1 12.544 805.07 174.71 - veteranos 1 12.555 805.08 174.72 - blancos 1 13.715 806.24 174.79 - salario 1 14.878 807.40 174.86 - protestantes 1 24.473 817.00 175.46 - hispanos 1 25.006 817.53 175.50 &lt;none&gt; 792.52 175.91 - grad_instituto 1 32.482 825.01 175.96 - evangelicos 1 43.177 835.70 176.62 - mujeres 1 57.840 850.36 177.51 - grad_univ 1 73.797 866.32 178.45 - densidad 1 84.717 877.24 179.09 - mormones 1 85.509 878.03 179.14 - asia 1 173.269 965.79 184.00 - paro 1 287.526 1080.05 189.70 Step: AIC=173.92 obama ~ densidad + veteranos + mujeres + grad_instituto + grad_univ + afro + asia + hispanos + blancos + evangelicos + protestantes + relig_afro + catolicos + mormones + paro + salario Df Sum of Sq RSS AIC - afro 1 1.349 794.04 172.01 - catolicos 1 9.740 802.43 172.55 - blancos 1 13.567 806.26 172.79 - veteranos 1 13.870 806.56 172.81 - relig_afro 1 13.991 806.68 172.82 - salario 1 14.765 807.46 172.87 - protestantes 1 24.346 817.04 173.47 - hispanos 1 24.992 817.68 173.51 &lt;none&gt; 792.69 173.92 - grad_instituto 1 33.638 826.33 174.04 - evangelicos 1 43.398 836.09 174.64 - densidad 1 84.719 877.41 177.10 - mormones 1 86.199 878.89 177.19 - grad_univ 1 96.666 889.36 177.79 - mujeres 1 114.224 906.92 178.79 - asia 1 180.199 972.89 182.37 - paro 1 300.135 1092.83 188.30 Step: AIC=172.01 obama ~ densidad + veteranos + mujeres + grad_instituto + grad_univ + asia + hispanos + blancos + evangelicos + protestantes + relig_afro + catolicos + mormones + paro + salario Df Sum of Sq RSS AIC - catolicos 1 10.218 804.26 170.66 - salario 1 14.423 808.46 170.93 - veteranos 1 15.412 809.45 170.99 - protestantes 1 22.997 817.04 171.47 - blancos 1 23.551 817.59 171.50 - relig_afro 1 24.048 818.09 171.53 &lt;none&gt; 794.04 172.01 - grad_instituto 1 34.693 828.73 172.19 - hispanos 1 39.340 833.38 172.48 - evangelicos 1 42.987 837.03 172.70 - mormones 1 87.616 881.66 175.35 - densidad 1 92.800 886.84 175.65 - grad_univ 1 95.935 889.98 175.83 - mujeres 1 113.018 907.06 176.80 - asia 1 239.541 1033.58 183.46 - paro 1 309.656 1103.70 186.80 [...] Step: AIC=165.22 obama ~ densidad + mujeres + grad_instituto + grad_univ + asia + hispanos + evangelicos + mormones + paro Df Sum of Sq RSS AIC &lt;none&gt; 879.45 165.22 - hispanos 1 37.14 916.59 165.33 - grad_univ 1 70.03 949.47 167.13 - evangelicos 1 84.02 963.47 167.87 - densidad 1 103.52 982.97 168.90 - grad_instituto 1 119.32 998.77 169.71 - mujeres 1 285.19 1164.63 177.54 - paro 1 305.27 1184.71 178.42 - mormones 1 492.00 1371.45 185.88 - asia 1 775.31 1654.76 195.46 Call: lm(formula = obama ~ densidad + mujeres + grad_instituto + grad_univ + asia + hispanos + evangelicos + mormones + paro, data = USA2) Coefficients: (Intercept) densidad mujeres grad_instituto grad_univ -3.263e+02 1.555e-03 5.087e+00 1.056e+00 4.207e-01 asia hispanos evangelicos mormones paro 7.629e-01 1.379e-01 -1.596e-01 -3.967e-01 1.854e+00 Al inicio del algoritmo, tenemos en Start el modelo inicial, entrado a la función step, y su valor de AIC. A continuación, en cada paso se disponen en una tabla las variables y el valor de AIC que obtendría el modelo si se eliminara la variable en cuestión. Las variables aparecen en la tabla ordenadas en orden ascendente del AIC que se obtiene al eliminarlas, y el modelo actual se indica por medio de &lt;none&gt; (no se elimina ninguna variable). Así, en la primera iteración se eliminará la variable jubilados ya que su eliminación proporciona un modelo con un valor mínimo AIC=173.92 que es menor que el valor del modelo inicial. Luego, al principio de cada iteración sucesiva se indica la formula correspondiente al modelo en ese momento. En la segunda iteración, ya se ha eliminado la variable jubilados, se calculan los AIC y como la variable afro da un valor mínimo de AIC, será la variable que se eliminará. Y así sucesivamente. El algoritmo finaliza cuando si se elimina cualquiera de las variables restantes, aumenta el AIC empeorando el modelo: corresponde a la situación en la que la variable &lt;none&gt; aparece en la primera fila de la tabla de valores AIC. En este ejemplo, se eliminan jubilados, afro y catolicos, veteranos, protestantes, salario, religion_afro y blancos resultando un modelo de regresión con nueve variables independientes y con un valor de AIC=165.22. La última parte de la salida de la función nos da las variables y los coeficientes de la función de regresión correspondiente. El vector de coeficientes de la función lineal resultante se puede obtener, usando trace=FALSE, con el sufijo $coefficients y el modelo con el sufijo $call. step(modelo1,direction=&quot;backward&quot;,trace=FALSE)$coefficients ## (Intercept) densidad mujeres grad_instituto grad_univ ## -3.262501e+02 1.555251e-03 5.086534e+00 1.056493e+00 4.206862e-01 ## asia hispanos evangelicos mormones paro ## 7.628660e-01 1.379246e-01 -1.595779e-01 -3.966677e-01 1.854289e+00 step(modelo1,direction=&quot;backward&quot;,trace=FALSE)$call ## lm(formula = obama ~ densidad + mujeres + grad_instituto + grad_univ + ## asia + hispanos + evangelicos + mormones + paro, data = USA2) Ejecutemos a continuación la función step pero ahora con direction=\"forward\" desde un modelo sin variables independientes (que se indica con ~1) y un rango que permita llegar al modelo completo. Para ahorrar espacio vertical, vamos a omitir de la salida la información de las iteraciones que realiza R. modelo_vacio= lm(obama~1,data=USA2) step(modelo_vacio,direction=&quot;forward&quot;,scope=list(lower=modelo_vacio, upper=modelo1),trace=FALSE) ## ## Call: ## lm(formula = obama ~ grad_univ + paro + jubilados + asia + mormones + ## densidad + evangelicos + mujeres + protestantes, data = USA2) ## ## Coefficients: ## (Intercept) grad_univ paro jubilados asia ## -1.365e+02 8.494e-01 2.043e+00 7.263e-01 7.332e-01 ## mormones densidad evangelicos mujeres protestantes ## -2.833e-01 1.134e-03 -1.956e-01 2.678e+00 1.972e-01 En esta aplicación de step se vuelve a obtener un modelo lineal con 9 variables independientes, aunque diferente al obtenido en la aplicación anterior ya que las variables protestantes y jubilados son ahora consideradas en lugar de grad_instituto e hispanos. Evidentemente la función step no considera todos los modelos posibles. Para considerar todos los modelos posibles con un número máximo de variables independientes, se puede utilizar la función regsubsets del paquete leaps. La sintaxis de la función es la siguiente: regsubsets(x, nbest=..., nvmax=...) donde: x es un modelo lineal obtenido mediante la función lm. nbest es la cantidad de modelos que queremos que nos muestre para cada número de variables independientes considerado, por defecto un modelo para cada número de variables. nvmax es el número máximo de variables independientes que queremos que se consideren. La función evalúa todos los modelos lineales con k variables aleatorias, para k entre 1 y nvmax, en base a su valor de BIC. Para cada valor de k, se muestran los nbest modelos con un menor valor de BIC. Para una visualización sencilla de los resultados, se puede utilizar la función plot. A modo de ejemplo, vamos a determinar cuál és el mejor modelo lineal con hasta 9 variables independientes, que es la cantidad de variables que se han obtenido con las dos ejecuciones de step, para los datos del Ejemplo 10.2. library(leaps) plot(regsubsets(obama~.,data=USA2,nbest=1,nvmax=9)) Figura 10.3: Variables independientes involucradas en el mejor modelo según BIC para una cantidad fija de variables independientes entre 1 y 9. Los modelos se ordenan por filas en base al valor BIC. En el gráfico resultante, se han coloreado, por filas, las variables que intervienen en los distintos modelos. Las filas están ordenadas según el valor BIC del modelo que representan. Así, el mejor modelo con una única variable independiente corresponde a la fila inferior y es el que solo usa la variable grad_univ. Para k=9, resulta que el mejor modelo corresponde al obtenido con la función step con direction=\"backward\" (fila 5 desde la parte inferior). Sin embargo, el mejor modelo según esta función es el correspondiente a la fila superior y que considera las 8 variables mujeres, grad_univ, asia, evangelicos, protestantes, mormones y paro. 10.4 Diagnósticos de regresión Para acabar esta lección, vamos a tratar la verificación de los requisitos que dotan de significación al modelo de regresión lineal. Como sabéis, la estimación y la inferencia a partir de un modelo de regresión lineal tienen sentido solo cuando se satisfacen una serie de varias hipótesis. Estas hipótesis tienen que ser comprobadas utilizando los llamados diagnósticos de regresión. Los problemas potenciales que puede sufrir un modelo de regresión lineal se clasifican en tres categorías: Relativos a los residuos: Los errores del modelo han de seguir una distribución normal con media 0 y varianza \\(\\sigma^2\\) constante (en el sentido de que no dependa del valor de las variables independientes) y ser incorrelados. Relativos al modelo: Los puntos han de ajustarse a la estructura lineal considerada. Relativos a las observaciones anómalas: Puede que algunas de las observaciones no se ajusten al modelo, comprometiendo su validez general. El primer paso para llevar a cabo los diagnósticos de regresión es utilizar la función plot con entrada el modelo lineal generado por la función lm. Esta instrucción generará cuatro gráficos: Residuos vs valores predichos (Residuals vs Fitted): Puede ser utilizado para comprobar la hipótesis de linealidad del modelo. El gráfico representa los puntos \\((\\hat{y}_i,e_i)\\), donde recordemos que \\(e_i\\) e \\(\\hat{y}_i\\) son, respectivamente, el valor estimado de la variable dependiente en el sujeto i-ésimo de la muestra y el error cometido en esta estimación, añadiendo una regresión local (o regresión móvil) de estos puntos, representada por una curva de color rojo, que permite comprobar si existe algún tipo de patrón o tendencia en los mismos. Si la curva se acerca a la recta horizontal \\(y=0\\) es indicativo de que los errores son muy pequeños y que por lo tanto se puede asumir la linealidad del modelo. Q-Q-plot (Normal Q-Q): Como ya explicamos en la Sección ??, este gráfico sirve para examinar la normalidad de los residuos. Recordemos que se puede aceptar la normalidad si los Q-Q-puntos están próximos a la recta cuartil-cuartil, representada en este gráfico por una línea discontinua. Escala-Localización (Scale-Location): El tercer gráfico sirve para comprobar la homocedasticidad del modelo, es decir, si la varianza de los errores es constante y no depende del valor de las variables independientes. Si se cumple la condición de homocedasticidad, se dice que el modelo es homocedástico, mientras que si no la cumple, se dice que es heterocedástico. En el gráfico se representan los puntos \\((\\hat{y}_i,\\sqrt{e_i^*})\\) donde \\(e_i^*\\) son los llamados residuos estandarizados. No entraremos en detalles de cómo se calculan los residuos estandarizados pero, de forma simplificada, son el cociente entre \\(e_i\\) y una estimación de su desviación típica. Este gráfico incluye también, como en el caso del primero, la regresión móvil de los puntos y podremos aceptar que se satisface la homocedasticidad si esta curva es horizontal y los puntos se distribuyen de forma homogénea a su alrededor. Residuos vs Apalancamiento (Residuals vs Leverage): El cuarto gráfico se utiliza para identificar observaciones influyentes, es decir, los valores extremos que podrían influir de forma significativa en el modelo lineal cuando son incluidos o excluidos del modelo. Estas observaciones suelen ser outliers (valores atípicos, anómalos) y leverage points (puntos de apalancamiento). Expliquemos el significado de estos términos. Outlier: Es una observación que tiene un valor anómalo de la variable dependiente condicionado a los valores de las variables independientes, y por lo tanto un residuo muy grande. Leverage point: Sin entrar en detalles, el leverage es una medida de la anomalía de los valores de las variables independientes, y un leverage point es un punto con alto valor de leverage. Los leverage points son puntos que están lejos del rango mayoritario de los valores de las variables independientes. El gráfico representa los puntos \\((e_i^*,h_i)\\), donde los \\(e_i^*\\) son los residuos estandarizados de los que ya hemos hablado y los \\(h_i\\) indican el grado de leverage de cada observación. Esto permite reconocer los outliers y los leverage points ya que los primeros se identifican como aquellas observaciones tales que \\(|e_i^*|&gt;3\\), mientras que los segundos se definen como aquellos puntos para los que \\(h_i&gt;2(k+1)/n\\) donde k es el número de variables independientes y n el de observaciones. Para identificar las observaciones influyentes también se utiliza la conocida como distancia de Cook. No vamos a definirla, pero existe el consenso de que si la distancia de Cook de un punto es mayor que 1, la observación correspondiente es influyente, mientras que si está entre 0.5 y 1 podría serlo y debe ser investigada . En este gráfico también se incluyen unas curvas discontinuas que marcan las regiones de los puntos con distancia de Cook entre 0 y 0.5, entre 0.5 y 1 y mayores que 1. En todos los gráficos anteriores, se identifican por defecto las tres observaciones más extremas según el criterio que evalúa cada gráfico. Estas observaciones podrían ser problemáticas y deberían ser evaluadas de forma individual para comprobar si la observación tiene algún tipo de característica diferencial o si simplemente es un error de entrada de datos. Realicemos el diagnóstico de regresión de los Ejemplos 10.1 y 10.2. Empecemos por el primero. Vamos a tomar los gráficos que produce por defecto R. Si queréis modificarlos, por ejemplo traduciendo los textos a otro idioma, consultad la Ayuda de la función plot.lm. par(mfrow=c(2,2)) plot(lm(peso~pesonotif,data=datospeso)) Figura 10.4: Gráficos del diagnóstico de regresión del modelo lineal del Ejemplo 10.1. Como ya se podía intuir, la observación 12, que era muy optimista con respecto al peso real (o un olvido de la cifra de las centenas), distorsiona claramente el análisis destacando en cada gráfico y es la única observación que podría ser influyente según el cuarto gráfico. Veamos qué ocurre si eliminamos esta observación que claramente constituye un caso muy especial que no responde a la lógica. par(mfrow=c(2,2)) plot(lm(peso~pesonotif,subset=-12,data=datospeso)) Figura 10.5: Gráficos del diagnóstico de regresión del modelo lineal del Ejemplo 10.1 sin la observación 12. Para este modelo los gráficos son mucho más interpretables. El primer gráfico muestra que la linealidad del modelo es razonable, aunque es cierto que existen pequeñas desviaciones respecto a una línea horizontal. El segundo gráfico claramente demuestra que los residuos no siguen una distribución normal. Comprobemos esta hipótesis aplicando el test de Shapiro-Wilk a los residuos del modelo: shapiro.test(lm(peso~pesonotif,subset=-12,data=datospeso)$residuals) ## ## Shapiro-Wilk normality test ## ## data: lm(peso ~ pesonotif, subset = -12, data = datospeso)$residuals ## W = 0.98104, p-value = 0.01416 Podemos rechazar con un nivel de significación del 5% que los residuos siguen una distribución normal. Por otra parte, el tercer gráfico nos indica que el modelo presenta heterocedasticidad al haber regiones considerables sin puntos y con una curva roja claramente no horizontal. Finalmente, no hay ninguna observación influyente en base al cuarto gráfico. En conclusión, parece ser que fallan dos hipótesis imprescindibles para realizar cualquier inferencia o estimación con este modelo lineal: la normalidad y la homocedasticidad de los residuos. A continuación, analicemos los gráficos generados en el diagnóstico de regresión del modelo obtenido por la función regsubsets en la Sección 10.3 para los datos del Ejemplo 10.2. par(mfrow=c(2,2)) modelo_obama=lm(obama~mujeres+grad_univ+asia+evangelicos+protestantes+mormones+paro, data=USA2) plot(modelo_obama) Figura 10.6: Gráficos del diagnóstico de regresión del modelo lineal obtenido con la función regsubsets a partir de los datos del Ejemplo 10.2. El primer gráfico no aporta pruebas muy evidentes de que el modelo no es lineal. Cuando puedan surgir dudas, para los modelos de regresión lineal múltiple, se puede utilizar la función crPlots del paquete car. Esta función, que se aplica a un modelo lineal, representa los gráficos de residuos parciales útiles para detectar la no linealidad en un modelo de regresión. Se definen los residuos parciales \\(e_{ij}\\) para una variable independiente \\(x_j\\) como \\[ e_{ij}=e_i+b_jx_{ij}. \\] Los residuos parciales se dibujan contra los valores de \\(x_j\\) y se calcula su recta de regresión lineal simple, representada en azul en el gráfico. Además, se realiza una recta de regresión no paramétrica suave (las variables independientes no están predeterminadas y se construyen con los datos), representada en color morado. Si estas curvas divergen considerablemente en alguna de las variables independientes, no se cumple la linealidad del modelo. En nuestro caso coinciden en general y podemos suponer que el modelo sí es lineal: crPlots(modelo_obama) Figura 10.7: Gráficos de residuos parciales del modelo lineal obtenido con la función regsubsets a partir de los datos del Ejemplo 10.2. El Q-Q-plot parece indicar normalidad de los residuos. Lo comprobamos con el test de Shapiro-Wilk: shapiro.test(modelo_obama$residuals) ## ## Shapiro-Wilk normality test ## ## data: modelo_obama$residuals ## W = 0.96359, p-value = 0.1187 Sin embargo, parece que nuevamente el modelo presenta heterocedasticidad. También se aprecia la existencia de una observación influyente, la observación 12. Esta observación corresponde al estado de Hawaii, que además de ser el único estado no continental de Estados Unidos, tiene un porcentaje de población de origen asiático mucho más elevado que el resto de estados. 10.5 Guía rápida lm(fórmula) realiza la regresión lineal de la variable dependiente en la parte izquierda de la fórmula respecto de las variables independientes indicadas a la derecha de la misma. El parámetro opcional data sirve para indicar el data frame del que se extraen las variables de la fórmula y el parámetro opcional subset sirve para indicar el subconjunto de filas del data frame que se ha de usar como muestra. Su salida es una list y sus dos componentes más interesantes son: coefficients: los coeficientes de la función de regresión lineal. residuals: los errores o residuos. fitted.values: los valores estimados de la variable dependiente en los puntos de la muestra. summary(lm(...)) muestra la información calculada con la función lm. Las componentes más interesantes de su salida son las siguientes: r.squared: el coeficiente de determinación \\(R^2\\). adj.r.squared: el coeficiente de determinación ajustado \\(R^2_{adj}\\). coefficients[,1]: los coeficientes de la función de regresión lineal. coefficients[,4]: los p-valores de los contrastes bilaterales de nulidad para dichos coeficientes. sigma: la estimación de la desviación típica común de los residuos. p.value: el p-valor del ANOVA asociado a la regresión lineal realizada. update permite recalcular una función de regresión lineal modificando la fórmula que describe su modelo (con el parámetro formula.) o reduciendo la muestra usada (con el parámetro subset). confint calcula los intervalos de confianza de los coeficientes de una función de regresión lineal. Se aplica a una salida de la función lm y admite además los parámetros parm, para especificar de qué coeficientes se piden los intervalos de confianza (por defecto, de todos) y level para indicar el nivel de confianza (por defecto, del 95%). predict calcula intervalos de confianza de predicciones usando una función de regresión lineal. Se aplica a una salida de la función lm y a los parámetros siguientes: newdata: un data frame cuyas filas dan los valores de las variables independientes de los puntos sobre los que queremos calcular los intervalos de confianza. ìnterval: indica de qué queremos calcular los intervalos de confianza. Admite 3 posibles valores: none, con el que no calcula intervalos de confianza, solo predicciones de valores; confidence, con el que calcula intervalos de confianza de valores esperados; y prediction, con el que calcula intervalos de confianza de valores predichos. level, como siempre, indica el nivel de confianza y su valor por defecto es 95%. extractAIC(...)[2] calcula la medida AIC (por defecto) o BIC (entrando el parámetro k=log(n)) de un modelo de regresión lineal. step prueba de manera iterativa una serie de modelos de regresión lineal obtenidos añadiendo o quitando variables independientes al que se le entra y da el mejor modelo de los que considera (por defecto, el de menor valor de AIC). Sus parámetros principales son: direction: indica la metodología que ha de utilizar R para generar los nuevos modelos a evaluar en la siguiente iteración. Sus valores posibles son: \"backward\", que indica que en cada iteración se evalúan y comparan el modelo obtenido en la iteración anterior y todos los modelos obtenidos a partir de él eliminando una de sus variables independientes; \"forward\", que indica que en cada iteración se evalúan y comparan el modelo obtenido en la iteración anterior y todos los modelos obtenidos a partir de él añadiendole una nueva variable independiente; y \"both\", que indica que en cada iteración se prueban la unión de todos lo modelos que usarían las dos opciones anteriores. scope: define el rango de modelos que se van a considerar. k: tiene el mismo significado que en la función extractAIC. trace: igualado a FALSE, no da en consola información de las iteraciones, solo el modelo obtenido al final. regsubsets del paquete leaps prueba de manera sistemática todos los modelos de regresión lineal con un número máximo de variables independientes y da para cada uno número de variables considerado los mejores modelos según el criterio BIC. Se aplica a una salida de la función lm y a los dos parámetros siguientes: nvmax: indica el número máximo de variables independientes que queremos que se consideren. nbest: indica la cantidad de modelos que ha de mostrar para cada número de variables independientes considerado, por defecto 1. plot(regsubsets(...)) del paquete leaps representa gráficamente de manera adecuada el resultado de una aplicación de la función regsubsets. plot(lm(...)) produce los gráficos Residuals vs Fitted, Normal Q-Q, Scale-Location y Residuals vs Leverage que permiten realizar el diagnóstico del modelo lineal calculado con la función lm. crPlots(lm(...)) genera el gráfico de residuos parciales del modelo lineal calculado con la función lm. kable del paquete knitr produce tablas y data frames bien formateados al compilar el fichero R Markdown. na.omit aplicado a un data frame elimina las filas que contienen algún valor NA. 10.6 Ejercicios Test (1) El data frame BostonHousing del paquete mlBench contiene información de propiedades inmobiliarias de la ciudad de Boston en el año 1970. Cada propiedad está caracterizada por 13 variables que indican las características de la misma, así como por la variable medv que representa una estimación del valor de la propiedad en miles de dólares. Realizad una regresión lineal múltiple de la variable medv en función del resto de variables excepto las variables chas y rad. Tenéis que dar el coeficiente de la variable crim en la ecuación lineal encontrada redondeado a 4 cifras decimales sin ceros innecesarios a la derecha y decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%, que el coeficiente de esta variable sea 0. Dad el coeficiente y la conclusión en este orden, separados por un único espacio en blanco. (2) El data frame BostonHousing del paquete mlBench contiene información de propiedades inmobiliarias de la ciudad de Boston en el año 1970. Cada propiedad está caracterizada por 13 variables que indican las características de la misma, así como por la variable medv que representa una estimación del valor de la propiedad en miles de dólares. Sin tener en cuenta las variables chas y rad, encontrad el mejor modelo lineal con exactamente 3 variables independientes en base al valor de la medida BIC y calculad el valor de \\(R^2_{adj}\\) del mismo. Además, tenéis que decir si la variable ptratio se usa (con un SI) o no (con un NO) en el modelo. Dad el valor de \\(R^2_{adj}\\) redondeado a 4 cifras decimales sin ceros innecesarios a la derecha y la respuesta a la pregunta planteada en este orden, separados por un único espacio en blanco. (3) El data frame abalone, disponible en el paquete AppliedPredictiveModeling, contiene información de 4177 abulones (también conocidos como orejas de mar). Dispone de la variable Type de tipo factor que diferencia entre especímenes machos (M), hembras (F) y jóvenes (I) y el resto son variables numéricas. Teniendo en cuenta sólo los especímenes hembra, encontrad el modelo lineal que explica la variable Rings en función del resto de variables numéricas y determinad si los residuos son normales (con un SI) o no (con un NO) con un nivel de significación del 5% utilizando el test de Shapiro-Wilk. Determinad también las observaciones que pueden influyentes en el modelo, entendidas como aquéllas con distancia de Cook mayor que 0.5. Dad el p-valor del contraste de normalidad redondeado a 4 cifras decimales sin ceros innecesarios a la derecha, la conclusión del contraste y las observaciones influyentes en orden creciente, separados por un único espacio en blanco. En el caso que no haya observaciones influyentes, contestad NO. (4) El data frame abalone, disponible en el paquete AppliedPredictiveModeling, contiene información de 4177 abulones (también conocidos como orejas de mar). Dispone de la variable Type de tipo factor que diferencia entre especímenes machos (M), hembras (F) y jóvenes (I). Encontrad el mejor modelo lineal que explica la variable Rings mediante la función step partiendo del modelo completo con todas las variables numéricas con parámetro direction=\"backward\". A partir de este modelo, dad en este orden los intervalos de confianza para el valor estimado y para el valor esperado de Rings de una oreja de mar con las siguientes características: LongestShell=0.44, Diameter=0.45, Height=0.15, WholeWeight=0.8, ShuckedWeight=0.36, VisceraWeight=0.2, ShellWeight=0.4. Dad en primer lugar el extremo izquierdo y a continuación el extremo derecho del intervalo para el valor predicho y a continuación, de forma análoga para el valor esperado, todos los valores sucesivos separados por un único espacio en blanco y redondeados a tres decimales sin ceros innecesarios a la derecha. Respuestas al test (1) -0.0701 SI Nosotros lo hemos resuelto mediante library(mlbench) data(BostonHousing) #Cargamos la tabla de datos str(BostonHousing) #Consultamos la tabla de datos ## &#39;data.frame&#39;:\t506 obs. of 14 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : num 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ b : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... boston=BostonHousing[,-c(4,9)] #Eliminamos las variables chas y rad summary(lm(medv~.,data=boston)) ## ## Call: ## lm(formula = medv ~ ., data = boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.3315 -2.8771 -0.6792 1.6858 27.4744 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.970e+01 5.051e+00 5.879 7.59e-09 *** ## crim -7.010e-02 3.269e-02 -2.144 0.032482 * ## zn 3.989e-02 1.409e-02 2.831 0.004835 ** ## indus -4.198e-02 6.080e-02 -0.691 0.490195 ## nox -1.458e+01 3.899e+00 -3.740 0.000206 *** ## rm 4.188e+00 4.255e-01 9.843 &lt; 2e-16 *** ## age -1.868e-03 1.359e-02 -0.137 0.890696 ## dis -1.503e+00 2.059e-01 -7.301 1.15e-12 *** ## tax 8.334e-04 2.386e-03 0.349 0.727038 ## ptratio -8.738e-01 1.323e-01 -6.607 1.02e-10 *** ## b 8.843e-03 2.763e-03 3.200 0.001461 ** ## lstat -5.267e-01 5.224e-02 -10.083 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.899 on 494 degrees of freedom ## Multiple R-squared: 0.7225,\tAdjusted R-squared: 0.7163 ## F-statistic: 116.9 on 11 and 494 DF, p-value: &lt; 2.2e-16 (2) 0.6767 SI Nosotros lo hemos resuelto mediante plot(regsubsets(medv~.,data=boston,nbest=1,nvmax=3)) La fila con tres variables independientes es la superior y vemos que involucra las variables rm, ptratio y lstat. Ahora realizamos la regresión lineal correspondiente y calculamos el valor de \\(R^2_{adj}\\): round(summary(lm(medv~rm+ptratio+lstat,data=boston))$adj.r.squared,4) ## [1] 0.6767 (3) 0 NO 2052 Nosotros lo hemos resuelto mediante library(AppliedPredictiveModeling) data(abalone) #Cargamos la tabla de datos str(abalone) #Consultamos la tabla de datos ## &#39;data.frame&#39;:\t4177 obs. of 9 variables: ## $ Type : Factor w/ 3 levels &quot;F&quot;,&quot;I&quot;,&quot;M&quot;: 3 3 1 3 2 2 1 1 3 1 ... ## $ LongestShell : num 0.455 0.35 0.53 0.44 0.33 0.425 0.53 0.545 0.475 0.55 ... ## $ Diameter : num 0.365 0.265 0.42 0.365 0.255 0.3 0.415 0.425 0.37 0.44 ... ## $ Height : num 0.095 0.09 0.135 0.125 0.08 0.095 0.15 0.125 0.125 0.15 ... ## $ WholeWeight : num 0.514 0.226 0.677 0.516 0.205 ... ## $ ShuckedWeight: num 0.2245 0.0995 0.2565 0.2155 0.0895 ... ## $ VisceraWeight: num 0.101 0.0485 0.1415 0.114 0.0395 ... ## $ ShellWeight : num 0.15 0.07 0.21 0.155 0.055 0.12 0.33 0.26 0.165 0.32 ... ## $ Rings : int 15 7 9 10 7 8 20 16 9 19 ... abaloneF=abalone[abalone$Type==&quot;F&quot;,-1] # Nos quedamos con las hembras y eliminamos el factor Type modelo=lm(Rings~.,data=abaloneF) #Calculamos la regresión lineal round(shapiro.test(modelo$residuals)$p.value,4) ## [1] 0 par(mfrow=c(2,2)) plot(modelo) par(mfrow=c(1,1)) Vemos la única observación influyente es la 2052. (4) 7.096 15.83 11.053 11.873 Nosotros lo hemos resuelto mediante abalone2=abalone[,-1] #Eliminamos el factor Type modelo_completo=lm(Rings~.,data=abalone2) #Calculamos la regresión lineal step(modelo_completo,direction=&quot;backward&quot;,trace=FALSE) #Realizamos el step pedido ## ## Call: ## lm(formula = Rings ~ Diameter + Height + WholeWeight + ShuckedWeight + ## VisceraWeight + ShellWeight, data = abalone2) ## ## Coefficients: ## (Intercept) Diameter Height WholeWeight ShuckedWeight ## 2.896 11.634 11.790 9.256 -20.271 ## VisceraWeight ShellWeight ## -9.931 8.606 modelo_step=update(modelo_completo,.~.-LongestShell) #Modificamos la regresión lineal datos=data.frame(LongestShell=0.44,Diameter=0.45,Height=0.15,WholeWeight=0.8, ShuckedWeight=0.36,VisceraWeight=0.2,ShellWeight=0.4) #Nuevo individuo round(predict(modelo_step,datos,interval=&quot;prediction&quot;),3) #Intervalo de confianza del valor predicho fit lwr upr 11.463 7.096 15.83 round(predict(modelo_step,datos,interval=&quot;confidence&quot;),3) #Intervalo de confianza del valor esperado fit lwr upr 11.463 11.053 11.873 Véase C. Davis. “Body image and weight preocuppation: A comparison between exercising and non-exercising women.” Appetite 15 (1990), pp. 13-21.↩︎ "],
["chap-clustering.html", "Lección 11 Clustering básico 11.1 Método de k-medias o k-means 11.2 Elección del número de clusters 11.3 Métodos jerárquicos aglomerativos 11.4 Guía rápida 11.5 Ejercicios", " Lección 11 Clustering básico En esta lección explicamos cómo usar R para clasificar objetos (o individuos, observaciones, etc.). En la práctica, supondremos que estos objetos están representados por medio de las filas de una tabla de datos de variables cuantitativas. A partir de estas descripciones como vectores numéricos, calcularemos de alguna manera la diferencia o semejanza entre cada par de objetos y las usaremos para clasificarlos, de manera que cada clase agrupe objetos parecidos. De esta manera, reducimos el problema de describir muchos objetos al de describir unas pocas clases. En este contexto, a las clases resultantes se las llama clusters y a la clasificación, clustering. Veremos dos tipos de procedimientos de clustering: el método de k-medias, o k-means, donde hay que especificar a priori el número de clusters que queremos formar, y los métodos jerárquicos aglomerativos, que producen un árbol que indica el orden en el que se van agrupando los objetos de manera jerárquica, empezando por los más cercanos. 11.1 Método de k-medias o k-means El método de k-medias produce una partición de una serie de objetos, representados mediante puntos \\(x_1,\\ldots,x_n\\) de un espacio \\(\\mathbb{R}^p\\), en un número prefijado k de clusters. En su versión básica, que es de la que toma su nombre genérico, se usa la distancia euclídea para comparar estos puntos y se identifica cada cluster con su centro: el punto medio (mean) de sus elementos. El objetivo es conseguir una clasificación óptima en el sentido siguiente. Dado un clustering de un conjunto de puntos, llamaremos \\(SSC\\) a la suma de los cuadrados de las distancias de los puntos a los centros de los clusters a los que han sido asignados. En símbolos, si los clusters resultantes son \\(C_1,\\ldots,C_k\\), de centros \\(c_1,\\ldots,c_k\\) respectivamente, y, para cada \\(i=1,\\ldots,k\\), el cluster \\(C_i\\) está formado por los puntos \\(C_i=\\{x_{i,1},\\ldots,x_{i,n_i}\\}\\), entonces \\[ SSC=\\sum_{i=1}^k\\sum_{l=1}^{n_i} \\|x_{i,l}-c_i\\|^2 \\] donde \\(\\|x-y\\|^2\\) indica la distancia euclídea al cuadrado entre los vectores \\(x\\) e \\(y\\) (recordad que si \\(x=(x_1,\\ldots,x_p)\\) e \\(y=(y_1,\\ldots,y_p)\\), su distancia euclídea al cuadrado es \\(\\|x-y\\|^2=\\sum\\limits_{i=1}^p (x_i-y_i)^2\\)). Entonces, el objetivo es conseguir un clustering con \\(SSC\\) mínima. Ya que estamos, denotaremos por \\(SSC_i\\) el sumando de \\(SSC\\) correspondiente al cluster \\(C_i\\); es decir, \\[ SSC_i=\\sum_{l=1}^{n_i} \\|x_{i,l}-c_i\\|^2. \\] No hay ningún algoritmo que resuelva este problema de manera eficiente, por lo que se han propuesto varios métodos heurísticos que calculan rápidamente clusterings aproximadamente óptimos. A continuación explicamos las líneas básicas de algunos de ellos. Algoritmo de Lloyd: Para empezar, se escogen k centros, se calculan las distancias euclídeas de cada punto a cada centro, y se asigna a cada centro el cluster formado por los puntos que están más cerca de él que de los otros centros. A continuación, en pasos sucesivos se itera el bucle (1)–(3) siguiente hasta que en una iteración los clusters no se modifican: Se sustituye cada centro por el punto medio de los puntos que forman su cluster. Se calculan las distancias euclídeas de cada punto a cada centro. Se asigna a cada centro el cluster formado por los puntos que están más cerca de él que de los otros centros. El clustering resultante está formado entonces por los últimos clusters construidos. Algoritmo de Hartigan-Wong: Este algoritmo empieza igual que el de Lloyd: se escogen k centros, se calculan las distancias euclídeas de cada punto a cada centro, y se asigna a cada centro el cluster de puntos que están más cerca de él que de los otros centros. A continuación, en pasos sucesivos se itera el bucle (1)–(5) siguiente hasta que en una iteración del mismo los clusters no cambian: Se sustituye cada centro por el punto medio de los puntos asignados a su cluster. Se calculan las distancias euclídeas de cada punto a cada centro. Se asigna (temporalmente) a cada centro el cluster formado por los puntos que están más cerca de él que de los otros centros. Si en esta asignación algún punto ha cambiado de cluster, digamos que el punto \\(x_i\\) se ha incorporado al cluster \\(C_j\\) de centro \\(c_j\\), entonces: Se calcula el valor \\(SSE_j\\) que se obtiene multiplicando \\(SSC_j\\) por \\(n_j/(n_j-1)\\) (donde \\(n_j\\) indica el número de elementos del cluster \\(C_j\\)). Se calcula, para todo otro cluster \\(C_k\\), el correspondiente valor \\(SSE_{i,k}\\) como si \\(x_i\\) hubiera ido a parar a \\(C_k\\). Si algún \\(SSE_{i,k}\\) resulta menor que \\(SSE_j\\), \\(x_i\\) se asigna definitivamente al cluster \\(C_k\\) que da valor mínimo de \\(SSE_{i,k}\\). Una vez realizado el procedimiento anterior para todos los puntos que han cambiado de cluster, estos se asignan a sus clusters definitivos y se da el bucle por completado. Como en el algoritmo de Lloyd, el clustering resultante está formado por los últimos clusters construidos. Algoritmo de McQueen: Es el mismo método que el de Lloyd salvo por el hecho de que no se recalculan todos los clusters y sus centros de golpe, sino elemento a elemento. Es decir, se empieza igual que en los dos algoritmos anteriores: se escogen k centros, se calculan las distancias euclídeas de cada punto a cada centro, se asigna a cada centro el cluster de puntos que están más cerca de él que de los otros centros, y se sustituye cada centro por el punto medio de los puntos asignados a su cluster. A partir de aquí, en pasos sucesivos se itera el bucle siguiente (recordemos que los puntos a clasificar son \\(x_1,\\ldots,x_n\\), y los supondremos ordenados por su fila en la tabla de datos): Para cada \\(i=1,\\ldots,n\\), se mira si el punto \\(x_i\\) está más cerca del centro de otro cluster que del centro del cluster al que está asignado. Si no lo está, se mantiene en su cluster y se pasa al punto siguiente, \\(x_{i+1}\\). Si se llega al final de la lista de puntos y todos se mantienen en sus clusters, el algoritmo se para. Si \\(x_i\\) está más cerca de otro centro, se traslada al cluster definido por este centro, se recalculan los centros de los dos clusters afectados (el que ha abandonado \\(x_i\\) y aquél al que se ha incorporado), y se reinicia el bucle, empezando de nuevo con \\(x_1\\). El clustering resultante está formado por los clusters existentes en el momento de parar. Ninguno de estos algoritmos garantiza un clustering óptimo, en el que la \\(SSC\\) resultante sea mínima. Lo que se suele hacer entonces es repetir varias veces el algoritmo con distintos conjuntos iniciales de k centros, y cruzar los dedos para que la \\(SSC\\) más pequeña que se obtenga en alguna de estas repeticiones sea efectivamente mínima. En todo caso, se ha demostrado que el algoritmo de Hartigan-Wong es, en general, más rápido y también más eficiente, en el sentido de que con mayor probabilidad da un clustering óptimo.9 El método de k-medias está implementado en R en la función kmeans. Su sintaxis básica es la siguiente: kmeans(x, centers=..., iter.max=..., algorithm =...) donde: x es la matriz o el data frame cuyas filas representan los objetos; en ambos casos, todas las variables han de ser numéricas. centers sirve para especificar los centros iniciales, y se puede usar de dos maneras: igualado a un número k, R escoge aleatoriamente los k centros iniciales, mientras que igualado a una matriz de k filas y el mismo número de columnas que x, R toma las filas de esta matriz como centros de partida. iter.max permite especificar el número máximo de iteraciones a realizar; su valor por defecto es 10. Al llegar a este número máximo de iteraciones, si el algoritmo aún no ha acabado porque los clusters aún no hayan estabilizado, se para y da como resultado los clusters que se han obtenido en la última iteración. algorithm indica el algoritmo a usar. Puede tomar como valor cualquiera de los que hemos explicado, y se ha de entrar entrecomillado. El método por defecto, que usa si no especificamos ninguno, es el de Hartigan-Wong. Otros parámetros se pueden consultar en la Ayuda de la función. Para ilustrar el funcionamiento de esta función, usaremos la tabla de datos saving del paquete faraway, que nos da 5 indicadores económicos de 50 países en el período 1960–1970. Dichos indicadores son: sr: la tasa de ahorro de cada país. pop15: su porcentaje de población menor de 15 años. pop75: su porcentaje de población mayor de 75 años. dpi: su renta per cápita en dólares. ddpi: su tasa de crecimiento, como porcentaje de su renta per cápita. Echemos un vistazo a la tabla de datos: library(faraway) str(savings) ## &#39;data.frame&#39;:\t50 obs. of 5 variables: ## $ sr : num 11.43 12.07 13.17 5.75 12.88 ... ## $ pop15: num 29.4 23.3 23.8 41.9 42.2 ... ## $ pop75: num 2.87 4.41 4.43 1.67 0.83 2.85 1.34 0.67 1.06 1.14 ... ## $ dpi : num 2330 1508 2108 189 728 ... ## $ ddpi : num 2.87 3.93 3.82 0.22 4.56 2.43 2.67 6.51 3.08 2.8 ... head(savings) ## sr pop15 pop75 dpi ddpi ## Australia 11.43 29.35 2.87 2329.68 2.87 ## Austria 12.07 23.32 4.41 1507.99 3.93 ## Belgium 13.17 23.80 4.43 2108.47 3.82 ## Bolivia 5.75 41.89 1.67 189.13 0.22 ## Brazil 12.88 42.19 0.83 728.47 4.56 ## Canada 8.79 31.72 2.85 2982.88 2.43 En este ejemplo, para poder representar gráficamente los resultados obtenidos, sólo usaremos los indicadores tasa de ahorro (sr) y renta per cápita (dpi) de los países. savings2=savings[,c(1,4)] Vamos a clasificar los 50 países en 4 clusters. Este número lo hemos elegido por ahora de manera arbitraria, en la próxima sección ya explicaremos cómo se puede hallar el número más adecuado de clusters en el que clasificar una determinada tabla de datos. Vamos a usar el método por defecto de R, y le dejaremos generar al azar los centros iniciales, pero fijaremos la semilla de aleatoriedad con la función set.seed para que el resultado sea reproducible. set.seed(100) estudio.paises=kmeans(savings2, centers=4) estudio.paises ## K-means clustering with 4 clusters of sizes 20, 9, 11, 10 ## ## Cluster means: ## sr dpi ## 1 7.830500 260.3405 ## 2 11.806667 1654.1378 ## 3 9.933636 741.0073 ## 4 11.141000 2709.2790 ## ## Clustering vector: ## Australia Austria Belgium Bolivia Brazil ## 4 2 2 1 3 ## Canada Chile China Colombia Costa Rica ## 4 3 1 1 1 ## Denmark Ecuador Finland France Germany ## 4 1 2 4 4 ## Greece Guatamala Honduras Iceland India ## 3 1 1 2 1 ## Ireland Italy Japan Korea Luxembourg ## 3 2 2 1 4 ## Malta Norway Netherlands New Zealand Nicaragua ## 3 4 2 2 1 ## Panama Paraguay Peru Philippines Portugal ## 3 1 1 1 3 ## South Africa South Rhodesia Spain Sweden Switzerland ## 3 1 3 4 4 ## Turkey Tunisia United Kingdom United States Venezuela ## 1 1 2 4 3 ## Zambia Jamaica Uruguay Libya Malaysia ## 1 1 3 1 1 ## ## Within cluster sum of squares by cluster: ## [1] 187921.7 577293.5 272602.3 2894953.3 ## (between_SS / total_SS = 91.8 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; Veamos en detalle el contenido de estudio.paises. En primer lugar, R nos dice que los clusters están formados por 20, 9, 11 y 10 países, respectivamente. Seguidamente nos da la matriz Cluster means, cuyas filas son las coordenadas de los centros de los clusters, en el orden correspondiente. A continuación, el Clustering vector nos indica a qué cluster pertenece cada país: en este caso, Australia está en el cluster 4, Austria, en el 2, y así sucesivamente. Luego nos da las “sumas de cuadrados de los clusters”, Within cluster sum of squares by cluster; es decir, el vector de las \\(SSC_i\\). A modo de ejemplo, comprobemos que efectivamente la primera entrada del vector Within cluster sum of squares by cluster es \\(SSC_1\\). Vamos a guardar en un data frame savings.1 la subtabla de datos de los países que pertenecen al primer cluster, llamaremos centro.1 al vector de coordenadas del centro del primer cluster, y finalmente sumaremos los cuadrados de las distancias euclídeas de las filas de savings.1 a centro.1. Para ello, vamos a usar que el resultado de kmeans es una list que contiene, por un lado, la componente cluster formada por el Clustering vector de las asignaciones de objetos a clusters (y por lo tanto, podemos especificar las filas que corresponden a países que pertenecen al primer cluster con estudio.paises$cluster==1) y, por otro lado, la componente centers formada por la matriz Cluster means de centros (y por lo tanto las coordenadas del centro del primer cluster se obtienen con estudio.paises$centers[1,]). savings.1=savings2[estudio.paises$cluster==1, ] #Miembros del primer cluster centro.1=estudio.paises$centers[1,] #Centro del primer cluster dist.euclid2=function(x,y){sum((x-y)^2)} #Distancia euclídea al cuadrado distancia_a_centro.1=function(x){dist.euclid2(x, centro.1)} #Distancia de un punto al centro del primer cluster sum(apply(savings.1, MARGIN=1, FUN=distancia_a_centro.1)) #SSC_1 ## [1] 187921.7 Coincide con la primera entrada del vector Within cluster sum of squares by cluster. Como el resultado de kmeans contiene la componente withinss formada por el vector de las \\(SSC_1\\), el valor de \\(SSC_1\\) se puede obtener directamente con estudio.paises$withinss[1] ## [1] 187921.7 Debajo del vector de las \\(SSC_i\\), R nos da el porcentaje between_SS / total_SS, en este caso, un 91.8%. Aquí, total_SS representa la suma \\(SST\\) de los cuadrados de las distancias de los puntos al centro \\(M\\) del conjunto de todos los puntos, y between_SS indica la suma \\(SSB\\) de los cuadrados de las distancias de los centros de los clusters a \\(M\\), contada cada una de ellas tantas veces como elementos tiene cluster. Es decir, si los puntos de partida son \\(x_1,\\ldots,x_n\\) y cada cluster \\(C_i\\) tiene centro \\(c_i\\) y está formado por \\(n_i\\) puntos, entonces \\[ SST=\\sum_{i=1}^n \\|x_i-M\\|^2,\\qquad SSB=\\sum_{i=1}^k n_i\\|c_i-M\\|^2. \\] Los valores de \\(SST\\), \\(SSC\\) y \\(SSB\\) son las componentes totss, tos.withinss y betwenss, respectivamente, del resultado de kmeans. Resulta que se tiene la identidad de sumas de cuadrados \\[ SST=SSC+SSB \\] que representa que la variabilidad total de los datos es igual a la suma de las variabilidades dentro de los clusters (la suma \\(SSC\\)) más la variabilidad de los centros de los clusters (\\(SSB\\)). Por lo tanto, el porcentaje between_SS / total_SS indica la fracción de la variabilidad total que explica la variabilidad de los centros de los clusters. Como, dada una tabla de datos, el valor de \\(SST\\) es fijo, una mayor fracción \\(SSB/SST\\) es equivalente a una menor \\(SSC\\), y por lo tanto a un mejor clustering. Comprobemos todas estas afirmaciones en nuestro ejemplo. Vamos a empezar calculando a mano \\(SST\\) y comprobando que coincide con estudio.paises$totss. Para ello, calculamos el punto medio de todo el conjunto de datos y a continuación la suma de las distancias al cuadrado de todos los puntos a este centro global. Usaremos la función dist.euclid2 que hemos definido hace un rato para calcular las distancias euclídeas al cuadrado. centro.global=apply(savings2, MARGIN=2, FUN=mean) #El punto medio global, M centro.global ## sr dpi ## 9.671 1106.758 dist_a_centro=function(x){dist.euclid2(x,centro.global)} SST=sum(apply(savings2, MARGIN=1, FUN=dist_a_centro)) SST ## [1] 48110220 estudio.paises$totss #La SST contenida en el resultado de la función kmeans ## [1] 48110220 El valor de \\(SSC\\) ha de ser la suma de las \\(SSC_i\\), que ya hemos visto que forman el vector estudio.paises$withinss. SSC=sum(estudio.paises$withinss) SSC ## [1] 3932771 estudio.paises$tot.withinss #La SSC contenida en el resultado de la función kmeans ## [1] 3932771 Finalmente, vamos a calcular la \\(SSB\\). Recordemos que la matriz de los centros de los clusters es el objeto estudio.paises$centers y se tiene que los tamaños de los clusters forman el vector estudio.paises$size. Centros=estudio.paises$centers #La matriz de centros Distancias_centros_centroglobal=apply(Centros,MARGIN=1,FUN=dist_a_centro) #Distancias de los centros a M SSB=sum(Distancias_centros_centroglobal*estudio.paises$size) SSB ## [1] 44177450 estudio.paises$betweenss #La SSB contenida en el resultado de la función kmeans ## [1] 44177450 Comprobemos finalmente la identidad \\(SST=SSC+SSB\\): SST ## [1] 48110220 SSB+SSC ## [1] 48110220 y comprobemos que el cociente \\(SSB/SST\\) es el 91.8% que nos ha dado R: SSB/SST ## [1] 0.918255 Así pues, el resultado de una aplicación de kmeans es una list. R nos da sus componentes al final del resultado de kmeans, bajo Available components:. A modo de resumen, vamos a recordar las componentes más interesantes para nuestros propósitos: size: vector de tamaños de los clusters. cluster: vector de enteros indicando a qué cluster pertenece cada fila de la tabla de datos. centers: matriz de filas los vectores de coordenadas de los centros de los clusters. totss: la \\(SST\\). withinss: el vector de las \\(SSC_i\\). tot.withinss: la \\(SSC\\), es decir, la suma del vector anterior. betweenss: la \\(SSB\\). Antes de continuar, tenemos de hacer un inciso. Cuando usamos un algoritmo de k-means partiendo de una configuración aleatoria de los centros iniciales, los resultados no tienen por qué ser los mismos cada vez que se ejecuta el algoritmo, debido a que distintas configuraciones iniciales de centros pueden desembocar en clusterings diferentes. Por este motivo, en el ejemplo anterior hemos usado set.seed(100) para que fuera reproducible. Con otra semilla de aleatoriedad, podría haber dado un resultado diferente, o no. Por ejemplo, con set.seed(2000) obtenemos un clustering con porcentaje \\(SSB/SST\\) más alto, y por lo tanto mejor (lo confirmamos comprobando que su \\(SSC\\) es menor que la anterior). set.seed(2000) estudio.paises2=kmeans(savings2, centers=4) estudio.paises2 ## K-means clustering with 4 clusters of sizes 9, 3, 30, 8 ## ## Cluster means: ## sr dpi ## 1 11.603333 1546.5244 ## 2 7.736667 3428.0867 ## 3 8.484667 407.2647 ## 4 12.671250 2364.6250 ## ## Clustering vector: ## Australia Austria Belgium Bolivia Brazil ## 4 1 4 3 3 ## Canada Chile China Colombia Costa Rica ## 2 3 3 3 3 ## Denmark Ecuador Finland France Germany ## 4 3 1 4 4 ## Greece Guatamala Honduras Iceland India ## 3 3 3 1 3 ## Ireland Italy Japan Korea Luxembourg ## 1 1 1 3 4 ## Malta Norway Netherlands New Zealand Nicaragua ## 3 4 1 1 3 ## Panama Paraguay Peru Philippines Portugal ## 3 3 3 3 3 ## South Africa South Rhodesia Spain Sweden Switzerland ## 3 3 3 2 4 ## Turkey Tunisia United Kingdom United States Venezuela ## 3 3 1 2 3 ## Zambia Jamaica Uruguay Libya Malaysia ## 3 3 3 3 3 ## ## Within cluster sum of squares by cluster: ## [1] 531037.1 543998.3 1580679.3 211532.6 ## (between_SS / total_SS = 94.0 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; estudio.paises2$tot.withinss #Nueva SSC ## [1] 2867247 estudio.paises$tot.withinss #Anterior SSC ## [1] 3932771 La moraleja es que conviene ejecutar unas cuantas veces la función kmeans con centros iniciales aleatorios y quedarnos con el mejor clustering que obtengamos. En nuestro caso, en el bloque de código siguiente realizamos 10000 ejecuciones aleatorias y no encontramos ningún clustering con \\(SSC\\) inferior a estudio.paises2$tot.withinss, por lo que consideraremos estudio.paises2 como una clasificación óptima de nuestros datos en 4 clases. fun.SSC=function(x){ set.seed(x) kmeans(savings2, centers=4)$tot.withinss } X=sample(10^8,10000) #10000 semillas aleatorias min(sapply(X,FUN=fun.SSC)) #Ejecutamos 10000 kmeans aleatorios y calculamos su mínimo SSC ## [1] 2867247 Para visualizar gráficamente un clustering de datos bidimensionales, lo más sencillo es producir el gráfico de dispersión de los puntos coloreándolos según los clusters. plot(savings2, col=estudio.paises2$cluster, xlab=&quot;Tasa de ahorro&quot;, ylab=&quot;Renta per cápita&quot;,pch=20) text(savings2, rownames(savings2), col=estudio.paises2$cluster, cex=0.6, pos=1) #Añadimos los nombres de los países a los puntos Obtenemos la Figura 11.1. Observamos que los países “cercanos” están coloreados con el mismo color. Así, por ejemplo, los países más desarrollados están coloreados de color rojo y están en la parte superior del gráfico, ya que tienen la renta per cápita más alta, mientras que los países menos desarrollados están coloreados de color verde y están en la parte inferior del gráfico al tener la renta per cápita más baja. Figura 11.1: Representación gráfica de las variables “Tasa de ahorro” y “Renta per cápita” agrupadas según el clustering estudio.paises2. Existen paquetes que aportan funciones para dibujar clusterings más informativos. Por ejemplo, la función clusplot del paquete cluster enmarca los clusters con elipses. Su sintaxis básica es clusplot(x, vector, parámetros) donde x es la matriz o la tabla de datos numéricos. El vector es la componente cluster del resultado de kmeans. Algunos otros parámetros útiles: shade: un parámetro lógico que igualado a TRUE sombrea las elipses según su densidad (número de puntos dividido entre área de la elipse): más densamente cuanto más densas. color: un parámetro lógico que igualado a TRUE colorea las elipses según su densidad, por defecto de azul a granate por orden creciente de intensidad. labels, que puede tomar (entre otros) los valores siguientes: 0 (no añade ninguna etiqueta), 2 (etiqueta los puntos y las elipses), 3 (etiqueta solo los puntos), 4 (etiqueta solo las elipses). col.clus: los colores de las elipses. col.p: los colores de los puntos. col.txt: los colores de las etiquetas. lines, permite añadir líneas uniendo los clusters y puede tomar los valores siguientes: 0 (no añade ninguna línea), 1 (las líneas unen los centros) y 2 (las líneas unen las fronteras de las elipses). Estas líneas nos permiten hacernos una idea de las distancias entre los clusters. cex y cex.text permiten modificar el tamaño de los puntos y de las etiquetas. Esta función dispone de muchos más parámetros, que pueden consultarse en la Ayuda de clusplot.default. A modo de ejemplo, el código library(cluster) clustering=estudio.paises2$cluster clusplot(savings2, clustering, shade=TRUE, lines=0, labels=3, color=TRUE, col.p=&quot;black&quot;, main=&quot;&quot;, sub=&quot;&quot;, cex.txt=0.75) produce la Figura 11.2 (con main=\"\" y sub=\"\" hemos eliminado el título y subtítulo por defecto). Observaréis que los puntos de clusters diferentes se han representado por medio de símbolos diferentes, y que los ejes de coordenadas no se corresponden con las variables originales. En realidad, clusplot ha realizado un Análisis de Componentes Principales (ACP) de los datos, que para dos variables simplemente significa que ha centrado los datos y ha tomado como eje de abscisas la variable con mayor varianza. En este caso, tras centrar los datos, ha tomado como eje de abscisas (la Componente Principal 1) la “Renta per cápita” y como eje de ordenadas (la Componente Principal 2) la “Tasa de ahorro”; si no os gustan las etiquetas de los ejes por defecto, podéis modificarlas con los parámetros adecuados. Estudiaremos con detalle el Análisis de Componentes Principales en la siguiente lección. Figura 11.2: Representación gráfica del clustering “estudio.paises2” usando la función “clusplot”. 11.2 Elección del número de clusters No se conoce ninguna manera de saber a priori el número correcto de clusters que tenemos que usar para clasificar un conjunto de datos. En cambio, sí que se dispone de algunos métodos más o menos aceptados para decidir a posteriori, una vez clasificados los datos en diversos números de clusters, cuál es el más adecuado. El procedimiento general es el siguiente: Tomamos una secuencia de valores consecutivos de k, usualmente de 2 hasta un tercio del número total de puntos (para que el número medio de puntos por cluster no sea inferior a 3). Para cada k, ejecutamos varias veces la función kmeans con diferentes configuraciones iniciales de k centros, y nos quedamos con un clustering que tenga el menor valor de \\(SSC\\) de los obtenidos para esta k. Llamaremos \\(SSC(k)\\) al valor mínimo de las \\(SSC\\) obtenidas para k clusters. A partir del vector de los \\(SSC(k)\\), medimos “algo” y a partir de esta medición decidimos la k. Por ejemplo, una regla bastante extendida, y claramente arbitraria, es la regla del 90%: “se toma el valor de k mas pequeño para el que \\((SST-SSC(k))/SST\\geq 0.9\\)” (observad que \\(SST-SSC(k)\\) es la \\(SSB\\) de un clustering que tenga \\(SSC=SSC(k)\\)). Vamos a organizar la información necesaria para aplicar los métodos que vamos a explicar en esta sección a nuestra tabla de datos savings2. Como la tabla contiene información sobre 50 países, probaremos los valores para k de 2 a 17. Para cada valor de k, calcularemos 500 clusterings partiendo de k centros escogidos aleatoriamente con valores de set.seed tomados de un vector aleatorio de 500 entradas escogidas de manera equiprobable (con sample) entre 1 y 50000. De esta manera, los resultados serán bastante aleatorios, pero como hemos fijado el vector de semillas en un bloque de código anterior, serán reproducibles. Entonces, para cada k guardaremos la semilla del primer clustering que dé el valor mínimo de \\(SSC\\) entre las 500 repeticiones (para poder volver a calcularlo si es necesario) y su valor de \\(SSC\\). Finalmente, organizaremos la información en una matriz Resultados de 3 columnas: una para los valores de k, una para los primeros valores de las semillas de aleatoriedad que han dado, para cada k, la \\(SSC\\) mínima, y una tercera columna con los correspondientes valores mínimos \\(SSC(k)\\). x=sample(50000,500,rep=FALSE) #Las 500 semillas aleatorias que usaremos Resultados=c() for(k in 2:17){ SSCs=c() for (i in x){ set.seed(i) SSCs=cbind(SSCs,c(i,kmeans(savings2,k)$tot.withinss)) } SSC.min=SSCs[,which.min(SSCs[2,])] Resultados=rbind(Resultados,c(k,SSC.min))} dimnames(Resultados)=list(NULL, c(&quot;k&quot;, &quot;semilla&quot;, &quot;SSC&quot;)) Resultados ## k semilla SSC ## [1,] 2 48465 10550652.77 ## [2,] 3 48465 5290876.37 ## [3,] 4 763 2867247.28 ## [4,] 5 48465 1561126.26 ## [5,] 6 29454 1067250.81 ## [6,] 7 24564 672060.28 ## [7,] 8 24564 506455.05 ## [8,] 9 49964 385292.98 ## [9,] 10 9125 302132.14 ## [10,] 11 45374 224846.04 ## [11,] 12 430 176821.57 ## [12,] 13 7115 130373.92 ## [13,] 14 23066 108975.59 ## [14,] 15 7115 97564.19 ## [15,] 16 37528 80886.18 ## [16,] 17 37528 75940.10 Si ahora, por ejemplo, quisiéramos aplicar la regla del 90%, calcularíamos para cada k el valor de \\((SST-SS(k))/SST\\) y tomaríamos el primer k para el que este cociente fuera mayor de 0.9 SST #Recordemos el valor de SST ## [1] 48110220 SSB=SST-Resultados[,3] Resultados[min(which(SSB/SST&gt;0.9)),1] ## k ## 4 Con la regla del 90% hubiéramos escogido \\(k=4\\), que es justamente lo que hemos hecho en la sección anterior. Un método muy popular para elegir el valor de k es el método del “codo”, que consiste en lo siguiente. Si representamos en un gráfico los puntos \\((k,SS_C(k))\\), la línea quebrada que se obtiene uniéndolos es más o menos cóncava. Si podemos detectar un valor de k a partir del cual \\(SS_C(k)\\) disminuya más lentamente que antes de él, ese k será el número recomendable de clusters a usar; si, en cambio, no existe ningún k con esta propiedad, es señal de que no existe ninguna clasificación natural de los objetos bajo estudio. Esta regla se llama el método del “codo” porque si superpusiésemos un brazo en la representación gráfica de los puntos \\((k,SS_C(k))\\), el punto correspondiente al k elegido sería dónde colocaríamos el codo. Apliquemos este método a nuestra tabla de datos savings2. El código siguiente produce la Figura 11.3, que muestra que el número k adecuado de clusters parece ser 5: plot(Resultados[,c(1,3)], xlab=&quot;k&quot;, ylab=&quot;SSC(k)&quot;, type=&quot;b&quot;, col=&quot;red&quot;) Figura 11.3: Aplicación del método del codo para hallar el número óptimo de clusters en la tabla de datos “savings2”. El último test del que queremos hablar para determinar el número adecuado de clusters es el llamado test F, el cual, aunque es un método muy usado, en nuestra opinión no tiene una justificación teórica suficiente. En este test, para cada k, se calcula el estadístico siguiente: \\[ F_k=\\frac{SS_C(k)-SS_C(k+1)}{SS_C(k+1)/(n-k-1)}, \\] donde, recordemos, \\(n\\) es el número de filas de nuestra tabla de datos. Se toma entonces como p-valor del contraste, para cada k, la probabilidad de que una variable con distribución F de Fisher con \\(p\\) y \\(p(n-k-1)\\) grados de libertad (donde \\(p\\) es el número de variables de nuestra tabla de datos) tome un valor mayor que \\(F_k\\): \\[ \\mbox{p-valor}_k=P(F_{p,p(n-k-1)} &gt; F_k). \\] Una vez hallado este p-valor para cada k, escogemos como k adecuado aquel cuyo p-valor sea el más pequeño. Vamos a aplicar este test F a nuestra tabla de datos. Recordemos que ya hemos calculado los valores \\(SSC(k)\\), en la tercera columna de la matriz Resultados. k=2:16 n=dim(savings2)[1] p=dim(savings2)[2] c(n,p) ## [1] 50 2 SSC=Resultados[ ,3] F_k=(SSC[-16]-SSC[-1])/(SSC[-1]/(n-k-1)) p.valores=1-pf(F_k,p,p*(n-k-1)) F.test=cbind(k,F_k,p.valores) F.test ## k F_k p.valores ## [1,] 2 46.723732 8.215650e-15 ## [2,] 3 38.882917 5.768719e-13 ## [3,] 4 37.649386 1.314504e-12 ## [4,] 5 20.361212 5.400730e-08 ## [5,] 6 25.285221 2.307249e-09 ## [6,] 7 13.733538 6.911503e-06 ## [7,] 8 12.893163 1.352875e-05 ## [8,] 9 11.009864 5.973559e-05 ## [9,] 10 13.405429 9.904776e-06 ## [10,] 11 10.320742 1.083179e-04 ## [11,] 12 13.181803 1.268388e-05 ## [12,] 13 7.068920 1.573935e-03 ## [13,] 14 4.093705 2.082776e-02 ## [14,] 15 7.010500 1.705498e-03 ## [15,] 16 2.149334 1.246509e-01 El p-valor más pequeño se obtiene para \\(k=2\\), por lo que éste sería el número adecuado de clusters según este test. Para terminar esta sección, vamos a dibujar los clusters que se obtienen para \\(k=2\\) y \\(k=5\\): #k=2 set.seed(48465) kmeans2=kmeans(savings2,centers=2) plot(savings2, col=kmeans2$cluster, xlab=&quot;Tasa de ahorro&quot;, ylab=&quot;Renta per cápita&quot;,pch=20) text(savings2, rownames(savings2), cex=0.6, pos=1, col=kmeans2$cluster) Figura 11.4: Clustering óptimo de la tabla de datos “savings2” con 2 clusters. #k=5 set.seed(48465) kmeans5=kmeans(savings2,centers=5) plot(savings2, col=kmeans5$cluster, xlab=&quot;Tasa de ahorro&quot;, ylab=&quot;Renta per cápita&quot;,pch=20) text(savings2, rownames(savings2), cex=0.6, pos=1, col=kmeans5$cluster) Figura 11.5: Clustering óptimo de la tabla de datos “savings2” con 5 clusters. 11.3 Métodos jerárquicos aglomerativos Dada una tabla de datos numéricos, un método de clustering jerárquico aglomerativo usa una matriz \\(D\\) de distancias entre las filas de la tabla de datos para ir agrupando secuencialmente los objetos que representan estas filas. El procedimiento básico es el siguiente: Partimos de \\(n\\) objetos y de la matriz \\(D\\) de distancias entre ellos, de orden \\(n\\times n\\). Consideramos que, inicialmente, cada objeto forma un cluster de un solo elemento. Hallamos los dos clusters \\(C_1\\) y \\(C_2\\) que están a distancia mínima. Unimos estos clusters \\(C_1\\) y \\(C_2\\) en un nuevo cluster \\(C_1+C_2\\). Eliminamos \\(C_1\\) y \\(C_2\\) de la lista de clusters. Recalculamos la distancia de \\(C_1+C_2\\) a los demás clusters, con lo que obtendremos una matriz de distancias de un orden inferior a la que teníamos; la manera de recalcular estas distancias es la que da lugar a algoritmos diferentes. Repetimos los pasos (3)–(6) hasta que sólo quede un único cluster. La matriz de distancias \\(D\\) entre los objetos se puede calcular con la función dist, cuya sintaxis básica es dist(x, method=...) donde: x es nuestra tabla de datos (una matriz o un data frame de variables cuantitativas). method sirve para indicar la distancia que queremos usar, cuyo nombre se ha de entrar entrecomillado. La distancia por defecto es la euclídea que hemos venido usando hasta ahora. Otros posibles valores son (en lo que sigue, \\(x=(x_1,\\ldots,x_m)\\) e \\(y=(y_1,\\ldots,y_m)\\) son dos vectores de \\(\\mathbb{R}^m\\)): La distancia de Manhattan, \"manhattan\", que entre \\(x\\) e \\(y\\) vale \\(\\sum\\limits_{i=1}^m |x_i-y_i|\\). La distancia del máximo, \"maximum\", que entre \\(x\\) e \\(y\\) vale \\(\\max_{i=1,\\ldots,m} |x_i-y_i|\\). La distancia de Canberra, \"canberra\", que entre \\(x\\) e \\(y\\) vale \\[ \\sum_{i=1}^m \\frac{|x_i-y_i|}{|x_i|+|y_i|}. \\] La distancia de Minkowski, \"minkowski\", que depende de un parámetro \\(p&gt;0\\) (que se ha de especificar en la función dist con p igual a su valor), y que entre \\(x\\) e \\(y\\) vale \\[ \\sqrt[p]{\\sum_{i=1}^m |x_i -y_i|^p} \\] Observad que cuando \\(p=1\\) se obtiene la distancia de Manhattan y cuando \\(p=2\\) se obtiene la distancia euclídea usual. La distancia binaria, \"binary\", que sirve básicamente para comparar vectores binarios (si los vectores no son binarios, R los entiende como binarios sustituyendo cada entrada diferente de 0 por 1). La distancia binaria entre \\(x\\) e \\(y\\) binarios es el número de posiciones en las que estos vectores tienen entradas diferentes, dividido por el número de posiciones en las que alguno de los dos vectores tiene un 1. Una vez calculada la matriz de distancias, los diferentes métodos de clustering jerárquico aglomerativos están implementados en R en la función hclust, cuya sintaxis básica es la siguiente: hclust(d, method=...) donde: d es la matriz de distancias entre nuestros objetos calculada con la función dist. method sirve para especificar cómo se define la distancia de la unión de dos clusters al resto de los clusters en el paso (6) del algoritmo general. El nombre del método se ha de entrar entrecomillado. La función hclust dispone de muchos métodos en este sentido, los más populares son los siguientes: El método de enlace completo, \"complete\", que es el método usado por hclust por defecto: dados tres clusters \\(C_1,C_2,C\\), la distancia de \\(C_1+C_2\\) a \\(C\\) es \\[ d(C_1+C_2,C)=\\max\\{d(C,C_1),d(C,C_2)\\} \\] donde (en este método y en los que siguen) las distancias entre clusters \\(d(C,C_1)\\) y \\(d(C,C_2)\\) se conocen de pasos anteriores: o bien vienen dadas por la matriz de distancias \\(D\\), si son clusters individuales, o bien se han calculado por este mismo método al formarse por la unión de clusters más pequeños. El método de enlace simple, \"single\": dados tres clusters \\(C_1,C_2,C\\), la distancia de \\(C_1+C_2\\) a \\(C\\) es \\[ d(C_1+C_2,C)=\\min\\{d(C,C_1),d(C,C_2)\\}. \\] El método de enlace promedio, \"average\", más conocido en filogenética como UPGMA (Unweighted Pair Group Method Using Arithmetic averages): dados tres clusters \\(C_1,C_2,C\\), la distancia de \\(C_1+C_2\\) a \\(C\\) es \\[ d(C_1+C_2,C)=\\frac{|C_1|}{|C_1|+|C_2|}d(C,C_1)+\\frac{|C_2|}{|C_1|+|C_2|}d(C,C_2), \\] donde \\(|\\ |\\) denota el cardinal de cada cluster. El método de Ward clásico, \"ward.D\": dados tres clusters \\(C_1,C_2,C\\), la distancia de \\(C_1+C_2\\) a \\(C\\) es \\[ \\begin{array}{rl} d(C_1+C_2,C)= &amp; \\displaystyle \\frac{|C|+|C_1|}{|C|+|C_1|+|C_2|}d(C,C_1)+\\frac{|C|+|C_2|}{|C|+|C_1|+|C_2|}d(C,C_2)\\\\[2ex] &amp; \\displaystyle -\\frac{|C|}{ (|C|+|C_1|+|C_2|)^2}d(C_1,C_2). \\end{array} \\] Para ilustrar cómo usar los métodos aglomerativos con R usaremos la famosa tabla de datos iris que reúne las longitudes y amplitudes de los sépalos y pétalos de \\(150\\) flores de tres especies de iris diferentes: Setosa, Versicolor y Virginica. Vamos a escoger al azar (pero fijando la semilla de aleatoriedad, para que sea reproducible) 5 flores de cada especie, calcularemos las distancias euclídeas entre los vectores numéricos de sus longitudes y amplitudes de sépalos y pétalos, y usaremos estas distancias para calcular algunos clusterings jerárquicos aglomerativos de estas flores, con diversos métodos. set.seed(100) iris.setosa=iris[iris$Species==&quot;setosa&quot;,] #Subtabla de flores setosa iris.versicolor=iris[iris$Species==&quot;versicolor&quot;,] #Subtabla de flores versicolor iris.virginica=iris[iris$Species==&quot;virginica&quot;,] #Subtabla de flores virginica flores.setosa=iris.setosa[sample(1:50,5),] #Muestra de 5 flores setosa flores.versicolor=iris.versicolor[sample(1:50,5),] #Muestra de 5 flores versicolor flores.virginica=iris.virginica[sample(1:50,5),] #Muestra de 5 flores virginica flores=rbind(flores.setosa,flores.versicolor,flores.virginica) #Tabla de datos con las 15 flores En el data frame flores, las filas han heredado sus números de la tabla iris original, como podemos comprobar: head(flores) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 10 4.9 3.1 1.5 0.1 setosa ## 38 4.9 3.6 1.4 0.1 setosa ## 48 4.6 3.2 1.4 0.2 setosa ## 25 4.8 3.4 1.9 0.2 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 94 5.0 2.3 3.3 1.0 versicolor Renumeremos las filas del 1 al 15: rownames(flores)=1:15 head(flores) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 4.9 3.1 1.5 0.1 setosa ## 2 4.9 3.6 1.4 0.1 setosa ## 3 4.6 3.2 1.4 0.2 setosa ## 4 4.8 3.4 1.9 0.2 setosa ## 5 4.3 3.0 1.1 0.1 setosa ## 6 5.0 2.3 3.3 1.0 versicolor Vamos a guardar en una matriz llamada distancias.iris las distancias euclídeas entre los vectores de longitudes y amplitudes de pares de miembros de la tabla flores. distancias.iris=dist(flores[ ,1:4]) Calculamos finalmente el clustering jerárquico de estas 15 flores usando el método del enlace completo, y lo llamamos estudio.flores para poder referirnos a él en lo sucesivo: estudio.flores=hclust(distancias.iris) Para poder comprender un clustering jerárquico, lo mejor es dibujarlo en forma de árbol binario o, en este contexto, de dendrograma, con los objetos que clasificamos en las hojas. Para ello le aplicamos simplemente la función plot. Al usar esta función para representar gráficamente un clustering jerárquico, hay dos parámetros que hay que tener en cuenta (aparte de los usuales): hang, que controla la situación de las hojas del dendrograma respecto del margen inferior. labels, que permite poner nombres a los objetos; por defecto, se identifican en la representación gráfica por medio de sus números de fila en la matriz o el data frame que contiene los datos. Demos una ojeada al clustering estudio.flores; usaremos primero el valor de hang por defecto (que es hang=0.1) y luego hang=-1 para que veáis la diferencia. Pondremos además nombres a las flores para identificar su especie, etiquetas adecuadas en los ejes, y con main=\"\"y sub=\"\" eliminaremos el título y el subtítulo que añade R por defecto. nombres.flores=c(paste(&quot;Set&quot;,1:5,sep=&quot;&quot;), paste(&quot;Vers&quot;,1:5,sep=&quot;&quot;), paste(&quot;Vir&quot;,1:5,sep=&quot;&quot;)) nombres.flores ## [1] &quot;Set1&quot; &quot;Set2&quot; &quot;Set3&quot; &quot;Set4&quot; &quot;Set5&quot; &quot;Vers1&quot; &quot;Vers2&quot; &quot;Vers3&quot; ## [9] &quot;Vers4&quot; &quot;Vers5&quot; &quot;Vir1&quot; &quot;Vir2&quot; &quot;Vir3&quot; &quot;Vir4&quot; &quot;Vir5&quot; plot(estudio.flores, labels=nombres.flores, xlab=&quot;flores&quot;, ylab=&quot;distancias&quot;, main=&quot;&quot;, sub=&quot;&quot;) Figura 11.6: Dendrograma para 15 flores de iris producido con el método de enlace completo y dibujado con hang=0.1. plot(estudio.flores, labels=nombres.flores, xlab=&quot;flores&quot;, ylab=&quot;distancias&quot;, main=&quot;&quot;, sub=&quot;&quot;, hang=-1) Figura 11.7: Dendrograma para 15 flores de iris producido con el método de enlace completo y dibujado con hang=-1. Vemos cómo el clustering jerárquico separa, en nuestra muestra, las flores setosa del resto, mientras que las versicolor y las virginica aparecen algo mezcladas. ¿Cuál sería el resultado de usar otros métodos para calcular las distancias entre clusters? Veamos los resultados con los métodos de enlace simple, promedio y Ward. plot(hclust(distancias.iris,method=&quot;single&quot;), hang=-1, labels=nombres.flores, xlab=&quot;flores&quot;, ylab=&quot;distancias&quot;, main=&quot;&quot;, sub=&quot;&quot;) Figura 11.8: Dendrograma para 15 flores de iris producido con el método de enlace simple. plot(hclust(distancias.iris,method=&quot;average&quot;), hang=-1, main=&quot;&quot;, sub=&quot;&quot;) Figura 11.9: Dendrograma para 15 flores de iris producido con el método de enlace promedio. plot(hclust(distancias.iris,method=&quot;ward.D&quot;), hang=-1, labels=nombres.flores, xlab=&quot;flores&quot;, ylab=&quot;distancias&quot;, main=&quot;&quot;, sub=&quot;&quot;) Figura 11.10: Dendrograma para 15 flores de iris producido con el método de Ward. Observamos que cada método ha dado lugar a una jerarquía de clusters diferente. Volviendo a la función hclust, veamos la estructura del clustering distancias.iris: str(estudio.flores) ## List of 7 ## $ merge : int [1:14, 1:2] -1 -7 -2 -4 -8 -15 -10 -14 -5 -13 ... ## $ height : num [1:14] 0.346 0.361 0.51 0.574 0.64 ... ## $ order : int [1:15] 11 14 15 7 12 5 4 2 1 3 ... ## $ labels : chr [1:15] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ method : chr &quot;complete&quot; ## $ call : language hclust(d = distancias.iris) ## $ dist.method: chr &quot;euclidean&quot; ## - attr(*, &quot;class&quot;)= chr &quot;hclust&quot; Como vemos, un clustering jerárquico producido con hclust es una list. Sus dos componentes más interesantes para nosotros son las siguientes. Por un lado, la componente merge es una matriz de dos columnas que indica el orden en el que se han ido agrupando los objetos de dos en dos. En esta matriz, los objetos originales se representan con números negativos, y los nuevos clusters con números positivos que indican el paso en el que se han creado. En nuestro ejemplo: estudio.flores$merge ## [,1] [,2] ## [1,] -1 -3 ## [2,] -7 -12 ## [3,] -2 1 ## [4,] -4 3 ## [5,] -8 -9 ## [6,] -15 2 ## [7,] -10 5 ## [8,] -14 6 ## [9,] -5 4 ## [10,] -13 7 ## [11,] -6 10 ## [12,] -11 8 ## [13,] 9 11 ## [14,] 12 13 Este resultado nos dice que: en el primer paso del algoritmo, se han agrupado las flores 1 y 3; en el segundo paso del algoritmo, se han agrupado las flores 7 y 12; en el tercer paso del algoritmo, se han agrupado el cluster formado en el primer paso (flores 1 y 3) y la flor 2; en el cuarto paso, se han agrupado el cluster formado en el tercer paso (flores 1, 2 y 3) y la flor 4; y así sucesivamente. Es conveniente que dediquéis un rato a comparar esta matriz de emparejamientos con los dendrogramas que representan el clustering, como por ejemplo el de la Figura 11.7. La otra componente que nos interesa es height, un vector que contiene las distancias a las que se han ido agrupando los pares de clusters, representadas como alturas en el eje de ordenadas en el dendrograma. Por ejemplo: round(estudio.flores$height,4) ## [1] 0.3464 0.3606 0.5099 0.5745 0.6403 0.7000 0.7810 0.7937 1.0296 1.3928 ## [11] 1.5067 2.3707 3.8730 6.7186 indica que las dos flores agrupadas en el primer paso estaban a distancia 0.3464, las dos flores agrupadas en el segundo paso estaban a distancia 0.3606, el cluster y la flor agrupados en el tercer paso estaban a distancia 0.5099, etc. De nuevo, es conveniente comparar esta lista de alturas con el dendrograma de la Figura 11.7: las líneas horizontales que forman los clusters están a las alturas en los que se han unido sus componentes. Un clustering jerárquico puede usarse para definir un clustering ordinario, es decir, una clasificación de los objetos bajo estudio. Esto se puede hacer de dos maneras: indicando cuántos clusters deseamos, o indicando a qué altura queremos cortar el dendrograma, de manera que clusters que se unan a una distancia mayor que dicha altura queden separados. Por ejemplo, en el dendrograma representado en la Figura 11.7, si cortamos a altura 3 obtenemos 3 clusters: de izquierda a derecha, uno formado por las cinco setosa, otro formado por cuatro virginica y una versicolor, y un tercero formado por la virginica y las cuatro versicolor restantes. Lo podéis comprobar imaginando una recta horizontal en el dendrograma a altura 3 y visualizando qué grupos forma. Con R disponemos de dos funciones básicas para obtener agrupamientos a partir de un clustering jerárquico. La primera es la función cutree. Su sintaxis básica es cutree(hclust, k=..., h=...) donde hclust es el resultado de una función homónima, y se ha de especificar o bien el parámetro k que indica el número de clusters deseado o bien el parámetro h que indica la altura a la que queremos cortar. Veamos dos ejemplos. Para cortar a altura 3, usamos: cutree(estudio.flores,h=3) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## 1 1 1 1 1 2 3 2 2 2 3 3 2 3 3 Y para clasificar en 4 clusters, usamos: cutree(estudio.flores,k=4) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## 1 1 1 1 1 2 3 2 2 2 4 3 2 3 3 Como vemos, el resultado de cutree es un vector similar a la componente cluster de un kmeans. Otra posibilidad es usar la función rect.hclust, que sobre el dendrograma dibujado en la instrucción inmediatamente anterior resalta los grupos enmarcándolos en rectángulos. La sintaxis es similar: se aplica al resultado de un hclust y al número k de grupos o a la altura h. Admite además un parámetro border que permite especificar los colores de los rectángulos (por defecto, todos rojos). Veamos el resultado de su aplicación por defecto en los mismos casos que hemos aplicado cutree: plot(estudio.flores, labels=nombres.flores, xlab=&quot;flores&quot;, ylab=&quot;distancias&quot;, main=&quot;&quot;, sub=&quot;&quot;, hang=-1) rect.hclust(estudio.flores,h=3) Figura 11.11: Clustering de 15 flores de iris a partir de su dendrograma calculado con el método de enlace completo, cortándolo a altura 3. plot(estudio.flores, labels=nombres.flores, xlab=&quot;flores&quot;, ylab=&quot;distancias&quot;, main=&quot;&quot;, sub=&quot;&quot;, hang=-1) rect.hclust(estudio.flores,k=4) Figura 11.12: Clustering de 15 flores de iris a partir de su dendrograma calculado con el método de enlace completo, usando 4 grupos. 11.4 Guía rápida kmeans aplica el algoritmo de k-means a una tabla de datos. Sus parámetros principales son: centers, que sirve para especificar o bien el número de clusters o bien las coordenadas de los centros iniciales; iter.max, que permite especificar el número máximo de iteraciones a realizar; algorithm, que indica el algoritmo específico a usar. El resultado es una list cuyas componentes principales son: cluster: vector que especifica a qué cluster pertenece cada individuo. centers: matriz de los centros de los clusters. totss: valor de \\(SST\\). withinss: vector \\((SSC_1,\\ldots,SSC_k)\\) . tot.withinss: valor de \\(SSC\\). betweenss: valor de \\(SSB\\). size: vector de los tamaños de los clusters. clusplot del paquete cluster, permite representar gráficamente el resultado de un kmeans, enmarcando los clusters con elipses. Se aplica a la tabla de datos original y al componente cluster del kmeans, y dispone de muchos parámetros entre los que destacan, aparte de los usuales para plot: shade: igualado a TRUE, sombrea las elipses según su densidad. color: igualado a TRUE, colorea las elipses según su densidad. labels: permite indicar qué queremos etiquetar en el gráfico. col.clus: permite modificar los colores de las elipses. col.p: permite modificar los colores de los puntos. col.txt: permite modificar los colores de las etiquetas. lines: permite añadir líneas uniendo los clusters. cex: permite modificar el tamaño de los puntos. cex.text: permite modificar el tamaño de las etiquetas. dist calcula la matriz de distancias entre las filas de una tabla de datos. Su parámetro principal es method, con el que se especifica la distancia concreta. hclust, aplicada a una matriz de distancias calculada con dist, produce un clustering jerárquico aglomerativo de los objetos representados en la tabla de datos original. Su parámetro principal es method, que permite especificar el algoritmo concreto que se desea usar. El resultado es una list, cuyas componentes principales son: merge: una matriz que indica el orden en el que se han realizado los agrupamientos. height: un vector que indica las distancias a las que se han realizado los agrupamientos. plot, aplicado al resultado de hclust, dibuja su dendrograma. Los parámetros específicos más importantes para esta aplicación de plot son: hang: controla la situación de las hojas del dendrograma respecto del margen inferior. labels: permite modificar las etiquetas de los objetos representados por las hojas del dendrograma. cutree permite obtener un clustering ordinario a partir del resultado de un hclust, bien sea especificando el número de clusters con el parámetro k, bien sea especificando la altura a la que deseamos cortar el clustering jerárquico con h. rect.hclust resalta, enmarcados en rectángulos sobre el dendrograma dibujado con plot en la instrucción inmediatamente anterior, los clusters que se producen al cortar el resultado de un hclust en k grupos o a la altura h. 11.5 Ejercicios Test (1) El data frame kanga del paquete faraway contiene diferentes medidas de los cráneos de 148 ejemplares de canguros de 3 especies. Considerad solo las medidas asociadas a su mandíbula (columnas 17 a 19), y como hay valores NA en estas variables, usad solo las filas que no contengan ningún NA. Vamos a llamar kanga.mand a la tabla resultante. Usad esta tabla de datos para realizar un clustering (de los canguros que queden en ella) con el método de k-means de Hartigan-Wong en \\(k=3\\) clases, usando como centros iniciales los puntos (1000,100,100), (1200,120,120) y (1500,150,150). Cuántos elementos tiene el cluster más grande que se obtiene de esta manera? (2) Seguimos con la tabla kanga.mand. Realizad un clustering de sus canguros con el método de k-means de Hartigan-Wong en \\(k=3\\) clases usando 3 puntos iniciales escogidos al azar pero fijando la semilla de aleatoriedad en 314. Qué vale la \\(SSC\\) del cluster que contiene el quinto ejemplar de la tabla de datos (según el orden de las filas)? Dad el resultado redondeado a un entero. (3) Seguimos con la tabla kanga.mand de las dos preguntas anteriores. Cuál de los dos clusterings calculados en las preguntas anteriores es mejor, el de la pregunta (1) o el de la pregunta (2)? La respuesta ha de ser 1 si es el de la pregunta (1), 2 si es el de la pregunta (2), o 0 si ninguno es mejor que el otro. (4) Seguimos con la tabla kanga.mand de las preguntas anteriores. Realizad un clustering jerárquico de sus canguros usando la distancia euclídea y el método de enlace promedio. Si lo cortáis en 3 clases, cuántos elementos tiene el cluster más grande que obtenéis? (5) Seguimos con la tabla kanga.mand de las preguntas anteriores. Realizad un clustering jerárquico de sus canguros usando la distancia euclídea y el método de Ward clásico. Si lo cortáis a altura 1000, cuántos clusters se forman? Respuestas al test (1) 62 Nosotros lo hemos calculado con library(faraway) kanga.mand=kanga[,17:19] kanga.mand=na.omit(kanga.mand) Centros=rbind(c(1000,100,100),c(1200,120,120),c(1500,150,150)) KM1=kmeans(kanga.mand,centers=Centros) max(KM1$size) ## [1] 62 (2) 160802 Nosotros lo hemos calculado con set.seed(314) KM2=kmeans(kanga.mand,centers=3) cluster.5=KM2$cluster[5] round(KM2$withinss[cluster.5]) ## [1] 160802 (3) 0 Nosotros lo hemos resuelto calculando los dos \\(SSC\\) y viendo que son iguales: KM1$totss ## [1] 2938803 KM2$totss ## [1] 2938803 (4) 76 Nosotros lo hemos calculado con D=dist(kanga.mand) HC1=hclust(D,method=&quot;average&quot;) table(cutree(HC1,k=3)) ## ## 1 2 3 ## 76 5 55 (5) 5 Nosotros lo hemos calculado con HC2=hclust(D,method=&quot;ward.D&quot;) table(cutree(HC2,h=1000)) ## ## 1 2 3 4 5 ## 26 50 5 35 20 Véase, por ejemplo, N. Slonim, E. Aharoni, K. Crammer, “Hartigan’s K-Means Versus Lloyd’s K-Means—Is It Time for a Change?”. Proceedings of the XXIII International Joint Conference on Artificial Intelligence (2013), pp. 1677-1684. Accesible (mayo 2019) en http://www.ijcai.org/Proceedings/13/Papers/249.pdf.↩︎ "],
["extras-de-r-markdown.html", "Lección 12 Extras de R Markdown 12.1 Parámetros de los chunks de R 12.2 Los chunks en modo línea 12.3 Figuras 12.4 Tablas 12.5 Fórmulas matemáticas", " Lección 12 Extras de R Markdown Para la mayor parte de las necesidades de este curso en lo que se refiere a composición de ficheros R Markdown, el documento Markdown Quick Reference que encontraréis en el menú Help de RStudio es más que suficiente. En esta lección, vamos a ampliar diversos puntos: Cómo controlar el comportamiento de los bloques de código o chunks al compilar el fichero R Markdown, y su aspecto en el documento final. Cómo controlar el aspecto de los gráficos producidos en los chunks. Cómo incluir de manera sencilla tablas bien formateadas producidas con R. Cómo escribir fórmulas matemáticas bien formateadas. 12.1 Parámetros de los chunks de R Los bloques de código de R, o chunks, se indican dentro de un documento R Markdown de la manera siguiente: ```{r} # Y aquí el código de R ``` Para crear un bloque de código, podemos usar el menú Insert de la barra superior de la ventana de ficheros. Si no queréis usar ese menú, tened en cuenta que cada signo ` se produce con un acento grave seguido de un espacio en blanco. La parte entre llaves de la línea que abre el chunk puede contener una etiqueta (un nombre que sirva para identificar el chunk), separada de la r inicial por un espacio en blanco, y diversos parámetros. Estos parámetros se separan de la r (o de la etiqueta, si la hay) por una coma y entre ellos también con comas, y permiten determinar el comportamiento del bloque al compilar el documento pulsando el botón Knit. Los parámetros más útiles son: echo: permite indicar si se ha de mostrar el código fuente de R en el documento final; su valor por defecto es TRUE, y si lo igualáis a FALSE, no lo mostrará, dando sólo el resultado. eval: permite indicar si queremos que se evalúe el código; su valor por defecto es TRUE, y si lo igualáis a FALSE, no lo evaluará. results: permite indicar cómo queremos ver el resultado de la ejecución del código en el documento final. Sus posibles valores son (y fijaos que hay que entrarlos entre comillas, porque son palabras): \"hide\", hace que no se muestre el resultado en el documento final. \"markup\", que es su valor por defecto, muestra los resultados en el documento final línea a línea, encabezados con dos marcas de comentario ##, y literales, sin que el programa que abre el documento final los interprete como código. \"asis\", devuelve los resultados en el documento final línea a línea y el programa con el que se abre el documento final los interpreta como texto y los formatea adecuadamente. \"hold\", muestra todos los resultados de golpe al final del bloque de código. Observad que no es lo mismo especificar que no se evalúe el código (eval=FALSE) que hacer que se evalúe, pero no mostrar el resultado (results=\"hide\"): en el segundo caso, el resultado del cálculo no aparecerá en el documento final, pero se podrá usar en bloques de código posteriores. message: permite indicar si se han de mostrar en el documento final los mensajes que R produce al ejecutar el código; su valor por defecto es TRUE, y si lo igualáis a FALSE no los muestra. warning: permite indicar si se han de mostrar en el documento final los mensajes de advertencia que a veces producen algunas funciones al ejecutarse; como antes, su valor por defecto es TRUE, y si lo igualáis a FALSE no los muestra. include: permite indicar si queremos “incluir” el chunk en el documento final: su valor por defecto es, naturalmente, TRUE, pero igualado a FALSE no deja ninguna traza del chunk en el documento final (aunque sí que lo ejecuta, así que más adelante podemos usar sus resultados). Para cada parámetro, el valor por defecto no tiene por qué especificarse. Veamos a continuación algunos ejemplos: El bloque (con los valores por defecto para echo y results, así que no importaría especificarlos) ```{r,echo=TRUE,results=&quot;markup&quot;} x=1:10 x sqrt(x) ``` produce, en el documento final: x=1:10 x ## [1] 1 2 3 4 5 6 7 8 9 10 sqrt(x) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 ## [8] 2.828427 3.000000 3.162278 El bloque ```{r,echo=TRUE,results=&quot;asis&quot;} x=1:10 x sqrt(x) ``` produce, en el documento final: x=1:10 x [1] 1 2 3 4 5 6 7 8 9 10 sqrt(x) [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 [8] 2.828427 3.000000 3.162278 El bloque ```{r,echo=TRUE,results=&quot;hold&quot;} x=1:10 x sqrt(x) ``` produce, en el documento final: x=1:10 x sqrt(x) ## [1] 1 2 3 4 5 6 7 8 9 10 ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 ## [8] 2.828427 3.000000 3.162278 El bloque ```{r,echo=FALSE} x=1:10 x sqrt(x) ``` no aparece en el documento final, y sólo se muestra el resultado: ## [1] 1 2 3 4 5 6 7 8 9 10 ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 ## [8] 2.828427 3.000000 3.162278 El bloque ```{r,echo=TRUE,message=TRUE} library(magic) magic(5) ``` produce, en el documento final: library(magic) ## Loading required package: abind magic(5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 9 2 25 18 11 ## [2,] 3 21 19 12 10 ## [3,] 22 20 13 6 4 ## [4,] 16 14 7 5 23 ## [5,] 15 8 1 24 17 El bloque ```{r,echo=TRUE,message=FALSE} library(magic) magic(5) ``` produce, en el documento final: library(magic) magic(5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 9 2 25 18 11 ## [2,] 3 21 19 12 10 ## [3,] 22 20 13 6 4 ## [4,] 16 14 7 5 23 ## [5,] 15 8 1 24 17 Si queréis cambiar el valor de algún parámetro en todos los chunks de un documento, por ejemplo para esconderlos todos (con echo=FALSE) o para modificar las dimensiones de la figuras como explicaremos más adelante, lo más práctico es incluir al principio del fichero R Markdown un chunk de opciones globales, indicado con la etiqueta global_options. En este chunk las opcions globales se han de incluir en el argumento de la función opts_chunk$set(...) del paquete knitr. También es una buena idea cargar en este chunk todos los paquetes que se van a usar en el documento. Y como seguramente no querréis que se note su existencia en el documento final, usad la opción include=FALSE. Así, por ejemplo, incluyendo al principio del fichero R Markdown el chunk ```{r global_options, include=FALSE} library(knitr) opts_chunk$set( echo=FALSE, message=FALSE, warning=FALSE ) ``` especificamos que, por defecto, en el documento final no aparezcan ni los bloques de código ni los mensajes que produce R al compilarlos. Podéis encontrar la lista completa de parámetros que se pueden usar en chunks en la guía de knitr. 12.2 Los chunks en modo línea Los bloques de código que hemos explicado en la sección anterior sirven para generar resultados en línea aparte. Si queremos introducir dentro de un párrafo un trozo de código de R que se ejecute al compilar el documento y muestre el resultado en el documento final, hay que hacerlo con `r código`. Por ejemplo, si en el documento R Markdown escribimos El cubo de dos es `r 2^3`, o lo que es lo mismo, 2^3^=`r 2^3`. produce, al pulsar Knit, la salida El cubo de dos es 8, o lo que es lo mismo, 23=8. Veamos otro ejemplo más práctico. Supongamos que nos dan una muestra y queremos calcular su media, su varianza, su desviación típica y su tamaño muestral. Entonces, podemos cargar los datos y efectuar los cálculos en un bloque de código (que, si queremos, podemos ocultar completamente en el documento final con include=FALSE) y a continuación en un párrafo ir llamando los resultados mediante pequeños chunks Por ejemplo, podríamos usar el bloque ```{r,include=FALSE} muestra=c(1,2,3,NA,2.8,3.1,4.9) media=mean(muestra,na.rm=TRUE) n=length(na.omit(muestra)) varianza=round(var(muestra,na.rm=TRUE)*(n-1)/n,3) desv.tipica=round(sqrt(varianza),3) ``` y a continuación escribir el párrafo La muestra es de tamaño `r n`, su media es `r media`, su varianza es `r varianza` y su desviación típica es `r desv.tipica`. Entonces, en el documento final, el bloque con los cálculos permanecería oculto, y este párrafo produciría La muestra es de tamaño 6, su media es 2.8, su varianza es 1.403 y su desviación típica es 1.184. 12.3 Figuras En un fichero R Markdown, podemos controlar el tamaño, la posición etc. de los gráficos en el documento final (html, pdf o Word) por medio de opciones en el encabezamiento de los chunks que los producen. Veamos algunas de las opciones básicas más útiles: fig.height y fig.width sirven para especificar la altura y la anchura, respectivamente, del gráfico “real”. Sus posibles valores son números enteros y se sobreentiende que están en pulgadas (no pueden usarse otras unidades); por defecto, ambos valen 7. out.height y out.width sirven para especificar la altura y la anchura, respectivamente, del gráfico en el documento final. Estas opciones son útiles si se quieren indicar las dimensiones del gráfico en tamaños relativos respecto de las medidas de la caja de texto. Así, por ejemplo, al compilar con Knit PDF o Knit HTML, si se especifican fig.height y fig.width (o se dejan con sus valores por defecto) y se incluye la opción out.width=\"60%\" (entrecomillado), el gráfico aparecerá en el documento escalado de manera que su ancho sea un 60% del ancho del texto; cuando se compila con Knit Word, out.width=\"60%\" no tiene ningún efecto. Como norma general, os recomendamos que uséis fig.height y fig.width para especificar las dimensiones del gráfico y lo incluyáis tal cual en el documento. Solo os recomendamos usar out.height o out.width cuando de trate de gráficos que se producen de manera diferente según sus dimensiones, como pueden ser las nubes de palabras. Para incorporar una nube de palabras a tamaño reducido en un documento R Markdown, lo mejor es dejar que R la produzca con su tamaño por defecto y luego reducirla con out.width; si reducís sus fig.height y fig.width, puede que algunas palabras no quepan en el gráfico o que queden muy apelmazadas. fig.asp permite fijar la proporción altura/anchura de un gráfico si solo se especifica su fig.width. fig.show sirve para controlar cómo se incluyen en el documento final los gráficos producidos en el chunk cuando hay más de uno. Su valor por defecto es \"as.is\", que los va incluyendo a medida que se generan. Con fig.show=\"hold\" se dibujan todos los gráficos de golpe al final del chunk. dev sirve para especificar el tipo de fichero gráfico que se genera. Solo hay un caso en que nos parece útil usar esta opción: si pensáis publicar vuestro documento html como una página web, os conviene usar dev=\"svg\" y el gráfico se creará en formato “Scalable Vector Graphics”, con lo que se escalará de manera correcta y sin perder calidad al aumentar o disminuir el ancho de la página web en el navegador. Para más opciones, podéis consultar la Guía de Referencia de R Markdown. Por otro lado, dentro de un chunk podemos usar las funciones par y layout, que también aparecen en algunas lecciones del curso. Estas funciones sirven para modificar el aspecto de los gráficos generados con R, y en particular pueden usarse para especificar el modo como se agrupan los gráficos producidos en un mismo chunk. Su efecto es el mismo al usarlas en la consola, salvo que allí los cambios introducidos con estas funciones son permanentes hasta que volvamos a cambiarlos, mientras que en un documento R Markdown solo afectan a su chunk. Con la función par(mfrow=c(x,y)) organizaremos los gráficos producidos en el chunk formando una matriz de \\(x\\) filas e \\(y\\) columnas, llenando sus entradas por filas. A modo de ejemplo, si un chunk produce 3 gráficos y queremos mostrarlos juntos, con la opción fig.show=\"hold\" aparecerán al final del chunk, uno debajo del otro. En cambio, si no especificamos la opción fig.show y al principio del chunk incluimos par(mfrow=c(1,3)), se dibujarán uno al lado del otro formando un único gráfico (y entonces conviene ajustar con fig.height y fig.width o fig.asp sus dimensiones: por ejemplo, seguramente no querremos que este gráfico compuesto sea cuadrado). Esta misma construcción se puede usar en la consola, pero entonces hay que tener en cuenta que a partir de la instrucción par(mfrow=c(x,y)) todos los gráficos se agruparán de esta manera. Para evitarlo, entrad par(mfrow=c(1,1)) cuando queráis volver al modo por defecto de los gráficos uno a uno. Veamos un ejemplo: par(mfrow=c(2,2)) plot(iris[iris$Species==&quot;setosa&quot;,1:2], main=&quot;Sépalos de setosa&quot;, xlab=&quot;Longitud&quot;, ylab=&quot;Anchura&quot;, pch=20) plot(iris[iris$Species==&quot;versicolor&quot;,1:2], main=&quot;Sépalos de versicolor&quot;, xlab=&quot;Longitud&quot;, ylab=&quot;Anchura&quot;, pch=20, col=&quot;red&quot;) plot(iris[iris$Species==&quot;virginica&quot;,1:2],main=&quot;Sépalos de virginica&quot;, xlab=&quot;Longitud&quot;, ylab=&quot;Anchura&quot;, pch=20, col=&quot;blue&quot;) plot(iris[ 1:2],main=&quot;Sépalos de iris&quot;,xlab=&quot;Longitud&quot;, ylab=&quot;Anchura&quot;, pch=20, col=&quot;green&quot;) par(mfrow=c(1,1)) produce la Figura 12.1 y restaura el formato original. Figura 12.1: Cuatro gráficos en uno organizados con la instrucción par(mfrow=c(2,2)) En general, con la función par se pueden modificar “hasta nueva orden” muchos aspectos de los gráficos. Si lo hacéis, es conveniente que antes de modificarlos guardéis su valor actual con, por ejemplo, par.anterior=par(). A continuación, ya podéis modificar los parámetros de par que queráis; si en algún momento queréis volver al estilo original, bastará que entréis par(par.anterior). Pasemos a la función layout. Su sintaxis básica es layout(M, widths=..., heights=...) donde M es una matriz que indica la composición en forma de cuadrícula de los gráficos que queremos agrupar en una misma figura. Sus entradas han de ser 0,1,2,…,n donde n el número de gráficos que queremos organizar (el 0 puede faltar, pero no podemos saltarnos ninguna entre 1 y n). Entonces, si la entrada (i,j) de la matriz M es el número k&gt;0, el k-ésimo gráfico producido en el chunk se situará en la posición (i,j) de la cuadrícula; si la entrada (i,j) de la matriz es 0, la posición (i,j) de la cuadrícula quedará vacía. Si un valor se repite en diferentes entradas de M, el gráfico correspondiente se reparte entre estas entradas. El parámetro widths sirve para especificar las anchuras relativas de las diferentes columnas (por defecto, todas iguales): para ello, se iguala al vector de estas anchuras. El parámetro heights sirve para especificar, de manera similar, las alturas relativas de las diferentes filas; por defecto, todas son iguales. Así, el código M=matrix(c(1,1,1,2,3,4),nrow=2,byrow=TRUE) layout(M) plot(iris[ 1:2], main=&quot;Sépalos de iris&quot;, xlab=&quot;Longitud&quot;, ylab=&quot;Anchura&quot;, pch=20, col=&quot;green&quot;) plot(iris[iris$Species==&quot;setosa&quot;,1:2], main=&quot;Sépalos de setosa&quot;, xlab=&quot;Longitud&quot;, ylab=&quot;Anchura&quot;, pch=20) plot(iris[iris$Species==&quot;versicolor&quot;,1:2],main=&quot;Sépalos de versicolor&quot;, xlab=&quot;Longitud&quot;, ylab=&quot;Anchura&quot;, pch=20, col=&quot;red&quot;) plot(iris[iris$Species==&quot;virginica&quot;,1:2],main=&quot;Sépalos de virginica&quot;, xlab=&quot;Longitud&quot;, ylab=&quot;Anchura&quot;, pch=20, col=&quot;blue&quot;) layout(1) produce la Figura 12.2, que no es ninguna maravilla, pero sirve para observar cómo la matriz M organiza los gráficos: el primer plot ocupa toda la primera fila, y los otros tres, las tres posiciones de la segunda fila ordenados de izquierda a derecha. Como antes, el efecto de layout es permanente, por lo que si queréis volver al modo “gráficos de uno en uno” lo más práctico es que entréis al terminar vuestro gráfico compuesto la instrucción layout(1), como hemos hecho en el código anterior. Figura 12.2: Tres gráficos en uno organizados con la instrucción layout(matrix(c(1,1,1,2,3,4),nrow=2,byrow=TRUE)) 12.4 Tablas La manera más sencilla de incluir tablas producidas con R en un fichero R Markdown es usando el paquete printr. Este paquete hace que las matrices, tablas de contingencia y dataframes se presenten como tablas de manera adecuada, que además dependerá del formato de salida elegido. Veamos algunos ejemplos de chunks y cómo se ven sus resultados. Para empezar, veamos como muestra los dataframes en el documento final. El chunk library(printr) head(iris) produce: En este documento html, la tabla siguiente: Figura 12.3: Una tabla en formato html producida con printr En un documento pdf, la tabla siguiente: Figura 12.4: Una tabla en formato pdf producida con printr En un documento Word, la tabla siguiente: Figura 12.5: Una tabla en formato Word producida con printr A partir de ahora, y en lo que queda de sección, sólo mostraremos los resultados en formato html. Veamos como se ven los diferentes tipos de tabla de contingencia en el documento html final: Sexo=c(&quot;M&quot;,&quot;H&quot;,&quot;H&quot;,&quot;H&quot;,&quot;H&quot;,&quot;M&quot;,&quot;M&quot;,&quot;H&quot;,&quot;M&quot;,&quot;H&quot;,&quot;H&quot;,&quot;M&quot;,&quot;M&quot;,&quot;H&quot;, &quot;H&quot;,&quot;M&quot;,&quot;H&quot;,&quot;M&quot;,&quot;H&quot;,&quot;H&quot;,&quot;H&quot;,&quot;M&quot;,&quot;H&quot;,&quot;M&quot;,&quot;H&quot;,&quot;H&quot;,&quot;H&quot;,&quot;H&quot;,&quot;H&quot;,&quot;M&quot;) Respuesta=c(&quot;No&quot;,&quot;Si&quot;,&quot;Si&quot;,&quot;Si&quot;,&quot;No&quot;,&quot;No&quot;,&quot;Si&quot;,&quot;No&quot;,&quot;No&quot;,&quot;No&quot;, &quot;No&quot;,&quot;No&quot;,&quot;Si&quot;,&quot;No&quot;,&quot;No&quot;,&quot;Si&quot;,&quot;Si&quot;,&quot;Si&quot;,&quot;Si&quot;,&quot;Si&quot;,&quot;Si&quot;,&quot;No&quot;,&quot;Si&quot;, &quot;No&quot;,&quot;Si&quot;,&quot;Si&quot;,&quot;Si&quot;,&quot;No&quot;,&quot;Si&quot;,&quot;Si&quot;) Edad=c(&quot;40-60&quot;,&quot;40-60&quot;,&quot;&gt;60&quot;,&quot;&gt;60&quot;,&quot;40-60&quot;,&quot;20-40&quot;,&quot;20-40&quot;, &quot;40-60&quot;,&quot;20-40&quot;,&quot;&lt;20&quot;,&quot;&gt;60&quot;,&quot;&lt;20&quot;,&quot;40-60&quot;,&quot;40-60&quot;,&quot;&lt;20&quot;, &quot;20-40&quot;,&quot;40-60&quot;,&quot;&lt;20&quot;,&quot;&gt;60&quot;,&quot;40-60&quot;,&quot;40-60&quot;,&quot;&lt;20&quot;,&quot;20-40&quot;, &quot;20-40&quot;,&quot;&gt;60&quot;,&quot;20-40&quot;,&quot;40-60&quot;,&quot;20-40&quot;,&quot;40-60&quot;,&quot;&lt;20&quot;) table(Edad) &lt;20 &gt;60 20-40 40-60 6 5 8 11 table(Respuesta,Edad) Respuesta/Edad &lt;20 &gt;60 20-40 40-60 No 4 1 4 4 Si 2 4 4 7 table(Respuesta,Sexo,Edad) Respuesta Sexo Edad Freq No H &lt;20 2 &gt;60 1 20-40 1 40-60 3 M &lt;20 2 &gt;60 0 20-40 3 40-60 1 Si H &lt;20 0 &gt;60 4 20-40 2 40-60 6 M &lt;20 2 &gt;60 0 20-40 2 40-60 1 Veamos finalmente como se ven las matrices en el documento html final. M=matrix(c(2,8,5,5,9,7,3,6,6,5,2,7,3,7,7,8,9,6,9,3,2, 3,3,10,8,10,2,10,8,7,3,3,2,6,3,2,1,9,8,5),nrow=8) M 2 6 9 8 2 8 5 6 10 6 5 2 9 2 3 5 7 3 10 2 9 3 2 8 1 7 7 3 7 9 3 7 3 3 8 6 8 10 3 5 dimnames(M)=list(NULL, paste(&quot;Col&quot;,1:5,sep=&quot;.&quot;)) M Col.1 Col.2 Col.3 Col.4 Col.5 2 6 9 8 2 8 5 6 10 6 5 2 9 2 3 5 7 3 10 2 9 3 2 8 1 7 7 3 7 9 3 7 3 3 8 6 8 10 3 5 Si en algún momento queréis volver a la presentación usual de matrices, dataframes y tablas de contingencia, basta que descarguéis el paquete printr entrando en un chunk la instrucción detach(package:printr, unload=TRUE) y luego ya lo volveréis a cargar si lo necesitáis de nuevo. Así, ahora que ya hemos descargado el paquete, la matriz M anterior se ve: M Col.1 Col.2 Col.3 Col.4 Col.5 2 6 9 8 2 8 5 6 10 6 5 2 9 2 3 5 7 3 10 2 9 3 2 8 1 7 7 3 7 9 3 7 3 3 8 6 8 10 3 5 12.5 Fórmulas matemáticas La manera de incluir fórmulas matemáticas en R Markdown se basa en la sintaxis del sistema de composición de textos científicos LaTeX. Esta misma sintaxis, con pequeñas modificaciones, se usa para escribir fórmulas matemáticas bien formateadas en otros contextos: en los foros de Moodle, en las entradas y comentarios de blogs en Blogger o Wordpress, en la Wikipedia, etc. Incluir fórmulas en un texto R Markdown no tiene ningún misterio. Solo hay que introducir el código que representa la fórmula de una de las dos formas siguientes: Para las fórmulas o ecuaciones dentro del mismo párrafo, se escribe el código entre dos dólares: $código$. Para las fórmulas o ecuaciones que queramos que aparezcan centradas en una línea aparte, se escribe el código entre dos dobles dólares: $$código$$. Al componer una fórmula a partir del código, RStudio ignora los espacios en blanco que hayamos escrito en ella, y añade los espacios en blanco a partir del significado lógico de sus elementos. Por ejemplo (y dejamos algunos espacios en blanco innecesarios para que veáis que no tienen ningún efecto en el resultado), el código siguiente: Las raíces de la ecuación $x^2= 2$ son $x=\\sqrt{ 2}$ y $x=-\\sqrt{2} $; en general, las raíces de $ax^2+b x+c=0$, con $a\\neq 0$, vienen dadas por la fórmula $$ x=\\frac{-b\\pm\\sqrt{b^2-4 a c}}{2a}. $$ produce el texto siguiente: Las raíces de la ecuación \\(x^2=2\\) son \\(x=\\sqrt{2}\\) y \\(x=-\\sqrt{2}\\); en general, las raíces de \\(ax^2+bx+c=0\\), con \\(a\\neq 0\\), vienen dadas por la fórmula \\[x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}.\\] Observad el código: Las potencias, y en general los superíndices, se indican con ^. La raíz cuadrada de algo se indica con \\sqrt{algo} (de square root). Una fracción se indica con \\frac{numerador}{denominador} (de fraction). Los signos \\(\\pm\\) y \\(\\neq\\) se indican con las marcas \\pm (de plus-minus) y \\neq (de not equal), respectivamente. Como podéis ver, las marcas de LaTeX que definen los diferentes elementos de las fórmulas matemáticas tienen nombres intuitivamente claros y (salvo las que corresponden a sígnos usuales como la suma o la resta) empiezan con el signo \\. A continuación damos algunas tablas con las marcas correspondientes a algunos de los signos matemáticos más usuales: Algunos operadores binarios \\[ \\begin{array}{cl|cl|cl|cl} \\text{Signo} &amp; \\text{Marca} &amp; \\text{Signo} &amp; \\text{Marca} &amp;\\text{Signo} &amp; \\text{Marca} &amp;\\text{Signo} &amp; \\text{Marca} \\\\ \\hline + &amp; \\texttt{+} &amp;- &amp; \\texttt{-} &amp; \\pm &amp; \\texttt{\\pm} &amp; \\times &amp; \\texttt{\\times} \\\\ \\cdot &amp; \\texttt{\\cdot} &amp; / &amp; \\texttt{/} &amp; \\cap &amp; \\texttt{\\cap} &amp; \\cup &amp; \\texttt{\\cup} &amp; \\end{array} \\] Algunos signos para relaciones \\[ \\begin{array}{cl|cl|cl} \\text{Signo} &amp; \\text{Marca} &amp; \\text{Signo} &amp; \\text{Marca} &amp;\\text{Signo} &amp; \\text{Marca} \\\\ \\hline =&amp; \\texttt{=} &amp; \\neq &amp; \\texttt{\\neq} &amp;&lt; &amp; \\texttt{&lt;} \\\\&gt; &amp; \\texttt{&gt;} &amp; \\leq &amp; \\texttt{\\leq} &amp; \\geq &amp; \\texttt{\\geq} \\\\ \\subseteq &amp; \\texttt{\\subseteq} &amp; \\subsetneq &amp; \\texttt{\\subsetneq} &amp; \\in &amp; \\texttt{\\in} \\\\ \\equiv &amp; \\texttt{\\equiv } &amp;\\sim &amp; \\texttt{\\sim} &amp; \\approx &amp; \\texttt{\\approx} \\end{array} \\] Algunos operadores \\[ \\begin{array}{cl|cl|cl} \\text{Signo} &amp; \\text{Marca} &amp; \\text{Signo} &amp; \\text{Marca} &amp;\\text{Signo} &amp; \\text{Marca} \\\\ \\hline \\sum&amp; \\texttt{\\sum} &amp;\\prod&amp; \\texttt{\\prod} &amp; \\bigcap&amp; \\texttt{ \\bigcap} \\\\ \\bigcup&amp; \\texttt{\\bigcup} &amp; \\int&amp; \\texttt{\\int} \\end{array} \\] Algunos delimitadores \\[ \\begin{array}{cl|cl|cl} \\text{Signos} &amp; \\text{Marcas} &amp; \\text{Signos} &amp; \\text{Marcas} &amp;\\text{Signos} &amp; \\text{Marcas} \\\\ \\hline (\\quad )&amp; \\texttt{( )} &amp; [\\quad ]&amp; \\texttt{[ ]} &amp; \\{\\quad \\} &amp; \\texttt{\\\\{ \\\\}} \\\\ \\lfloor\\quad\\rfloor &amp; \\texttt{\\lfloor \\rfloor} &amp;\\lceil\\quad \\rceil&amp; \\texttt{\\lceil \\rceil} \\end{array} \\] Algunas letras griegas: la marca es simplemente su nombre (en inglés) precedido del signo \\ \\[ \\begin{array}{cl|cl|cl|cl} \\text{Signo} &amp; \\text{Marca} &amp; \\text{Signo} &amp; \\text{Marca} &amp;\\text{Signo} &amp; \\text{Marca} &amp;\\text{Signo} &amp; \\text{Marca} \\\\ \\hline \\alpha&amp; \\texttt{\\alpha} &amp;\\beta&amp; \\texttt{\\beta} &amp;\\gamma&amp; \\texttt{\\gamma} &amp;\\delta&amp; \\texttt{\\delta} \\\\ \\epsilon&amp; \\texttt{\\epsilon} &amp; \\theta&amp; \\texttt{\\theta} &amp;\\gamma&amp; \\texttt{\\gamma} &amp; \\lambda&amp; \\texttt{\\lambda} \\\\ \\mu&amp; \\texttt{\\mu} &amp;\\nu&amp; \\texttt{\\nu} &amp;\\pi&amp; \\texttt{\\pi} &amp; \\rho&amp; \\texttt{\\rho}\\\\ \\sigma&amp; \\texttt{\\sigma} &amp;\\tau &amp; \\texttt{\\tau } &amp;\\varphi&amp; \\texttt{\\varphi} &amp;\\chi&amp; \\texttt{\\chi} \\\\ \\omega&amp; \\texttt{\\omega} &amp; \\Gamma&amp; \\texttt{\\Gamma} &amp; \\Sigma&amp; \\texttt{\\Sigma} &amp;\\Omega&amp; \\texttt{\\Omega} \\end{array} \\] Algunos acentos en matemáticas \\[ \\begin{array}{cl|cl|cl|cl} \\text{Signo} &amp; \\text{Marca} &amp; \\text{Signo} &amp; \\text{Marca} &amp;\\text{Signo} &amp; \\text{Marca} &amp;\\text{Signo} &amp; \\text{Marca} \\\\ \\hline \\hat{x}&amp; \\texttt{\\hat x} &amp;\\bar{x}&amp; \\texttt{\\bar x} &amp;\\dot{x}&amp; \\texttt{\\dot x} &amp; \\tilde{x}&amp; \\texttt{\\tilde x} \\end{array} \\] Algunos acentos “expandibles” \\[ \\begin{array}{cl|cl} \\text{Signo} &amp; \\text{Marca} &amp; \\text{Signo} &amp; \\text{Marca} \\\\ \\hline \\widetilde{xyz} &amp; \\texttt{\\widetilde{xyz}} &amp;\\widehat{xyz}&amp; \\texttt{\\widehat{xyz}} \\\\ \\overline{xyz}&amp; \\texttt{\\overline{xyz}} &amp; \\overbrace{xyz}&amp; \\texttt{\\overbrace{xyz}} \\end{array} \\] Algunas flechas \\[ \\begin{array}{cl|cl|cl} \\text{Signo} &amp; \\text{Marca} &amp; \\text{Signo} &amp; \\text{Marca} &amp;\\text{Signo} &amp; \\text{Marca} \\\\ \\hline \\leftarrow &amp; \\texttt{\\leftarrow} &amp; \\Leftarrow &amp; \\texttt{\\Leftarrow} &amp;\\rightarrow &amp; \\texttt{\\rightarrow} \\\\ \\Rightarrow &amp; \\texttt{\\Rightarrow} &amp; \\leftrightarrow &amp; \\texttt{\\leftrightarrow} &amp; \\Leftrightarrow &amp; \\texttt{\\Leftrightarrow} \\\\ \\mapsto &amp; \\texttt{\\mapsto} \\end{array} \\] Algunas funciones \\[ \\begin{array}{cl|cl|cl|cl} \\text{Signo} &amp; \\text{Marca} &amp; \\text{Signo} &amp; \\text{Marca} &amp;\\text{Signo} &amp; \\text{Marca} &amp;\\text{Signo} &amp; \\text{Marca} \\\\ \\hline \\sin &amp; \\texttt{\\sin} &amp; \\cos &amp; \\texttt{\\cos} &amp;\\tan &amp; \\texttt{\\tan} &amp; \\log &amp; \\texttt{\\log} \\\\ \\ln &amp; \\texttt{\\ln} &amp; \\max &amp; \\texttt{\\max} &amp; \\min &amp; \\texttt{\\min} &amp; \\lim &amp; \\texttt{\\lim}\\\\ \\end{array} \\] Otros signos útiles \\[ \\begin{array}{cl|cl|cl} \\text{Signo} &amp; \\text{Marca} &amp; \\text{Signo} &amp; \\text{Marca} &amp;\\text{Signo} &amp; \\text{Marca} \\\\ \\hline \\ldots &amp; \\texttt{\\ldots} &amp;\\cdots &amp; \\texttt{\\cdots} &amp; \\infty &amp; \\texttt{\\infty} \\\\ \\emptyset &amp; \\texttt{\\emptyset} &amp; \\$ &amp; \\texttt{\\$} \\% &amp; \\texttt{\\%} \\end{array} \\] Algunos puntos que hay que tener en cuenta en la composición de fórmulas: Los subíndices y superíndices se indican con los signos _ y ^, respectivamente. Si el subíndice o superíndice está formado por dos o más caracteres, hay que entrarlo entre llaves. Por ejemplo, $x_i$ produce \\(x_i\\) y $x^{25}$ produce \\(x^{25}\\), pero, cuidado, $x^25$ produce \\(x^25\\). Disponéis de diversos tipos de letra para usar en fórmulas matemáticas. Las dos más útiles son: Negrita, que se indica con \\mathbf{...}; por ejemplo, \\mathbf{X} y \\mathbf{a} producen \\(\\mathbf{X}\\) y \\(\\mathbf{a}\\), respectivamente. La llamada Negrita de pizarra, que se usa en las notaciones de algunos conjuntos de números y se indica con \\mathbb{...}; por ejemplo, \\mathbb{N} produce \\(\\mathbb{N}\\) (el conjunto de los números naturales) y \\mathbb{R} produce \\(\\mathbb{R}\\) (los números reales). \\sqrt produce raíces cuadradas o de orden superior: \\sqrt{xyz} produce \\(\\sqrt{xyz}\\) mientras que \\sqrt[n]{xyz} produce \\(\\sqrt[n]{xyz}\\). \\frac{numerador}{denominador} produce fracciones; su tamaño y composición depende de si la fórmula ha de aparecer en el interior de un párrafo o ha de aparecer en línea aparte. Por ejemplo, $\\frac{abc}{xyz}$ produce \\(\\frac{abc}{xyz}\\). Podemos especificar que los delimitadores se adapten a la altura de la expresión que envuelven, combinándolos con \\left y \\right. Comparad los tamaños de los paréntesis en $\\left(X\\right)$ y $\\left(\\frac{abc}{xyz}\\right)$, que producen \\(\\left(X\\right)\\) y \\(\\left(\\frac{abc}{xyz}\\right)\\), respectivamente. Podemos incluir chunks en modo línea dentro de fórmulas matemáticas: por ejemplo, $\\sqrt{2}=`r round(sqrt(2),4)`$ produce en el documento final \\(\\sqrt{2}=1.4142\\). Podemos incluir matrices, o, más en general, tablas, en las fórmulas matemáticas. Una tabla se define empezando con \\begin{array}{formato} y acabando con \\end{array}. El formato es una secuencia de letras l (de izquierda, left), r (de derecha, right) o c (de centrada): el número de letras indica el número de columnas, y cada letra indica el tipo de alineamiento de la columna correspondiente. Así, por ejemplo, \\begin{array}{rccl} define una tabla de cuatro columnas: la primera alineada a la derecha, la segunda y la tercera centradas, y la cuarta alineada a la izquierda. Entre el \\begin{array}{formato} y el \\end{array} se introducen por filas los valores de la tabla: los elementos de cada fila se separan con el signo &amp; y el cambio de fila se indica con \\\\. Para definir una matriz, hemos de envolver la tabla con los delimitadores \\left( y \\right). Por ejemplo, $$ \\left(\\begin{array}{ccc} a_{1,1} &amp; a_{1,2} &amp; a_{1,3}\\\\ a_{2,1} &amp; a_{2,2} &amp; a_{2,3} \\end{array}\\right) $$ produce la matriz (en línea aparte) \\[ \\left(\\begin{array}{ccc} a_{1,1} &amp; a_{1,2} &amp; a_{1,3}\\\\ a_{2,1} &amp; a_{2,2} &amp; a_{2,3} \\end{array}\\right) \\] De manera similar, para indicar el determinante de una matriz, hemos de usar los delimitadores \\left | y \\right |. Dos cuestiones a tener en cuenta: Si sólo queremos usar un delimitador a un lado de la tabla, tenemos que incluir al otro lado \\left. o \\right., según corresponda; las marcas \\left y \\right siempre han de ir en parejas. Por ejemplo, $$ \\left.\\begin{array}{l} 2x+3y=5\\\\ 6x-2y=8 \\end{array}\\right\\} $$ produce el sistema de ecuaciones \\[ \\left.\\begin{array}{l} 2x+3y=5\\\\ 6x-2y=8 \\end{array}\\right\\} \\] Si en una entrada de una tabla en modo matemático queremos introducir texto, lo tenemos que incluir en una caja de texto definida con la instrucción \\text{...}. Por ejemplo, $$ f(x)=\\left\\{ \\begin{array}{ll} 2x &amp; \\text{si }x\\leq 0 \\\\ 3x &amp; \\text{si }x\\geq 0 \\end{array} \\right. $$ produce \\[ f(x)=\\left\\{ \\begin{array}{ll} 2x &amp; \\mbox{si }x\\leq 0 \\\\ 3x &amp; \\mbox{si }x\\geq 0 \\end{array} \\right. \\] Observad que hemos dejado un espacio en blanco dentro de los \\text{si } para separar los “si” de las fórmulas que los siguen: el contenido de un \\text{...} se transforma en texto, y por lo tanto se tienen en cuenta los espacios en blanco igual que en el texto normal. Si necesitáis escribir expresiones matemáticas de manera regular en vuestros documentos R Markdown, os recomendamos que consultéis la sección de Matemáticas del Wikibook de LaTeX. "]
]
